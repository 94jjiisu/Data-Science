# 1. Tree Based Model

## 결정트리(Decision Tree)
* 결정 트리는 비용함수를 최소로 하는 특성과 그 값에 대한 Yes/No 질문의 대답으로 타겟 데이터를 분할하는 알고리즘임
* 데이터를 분할하는 형태가 나무의 가지가 뻗어나가는 모습과 비슷

* 트리의 구성 요소
  * 노드(node): 질문이나 정답을 담고 있는 결과
    * 뿌리(root)노드: 처음으로 분기되는 노드
    * 중간노드: 아직 분기가 끝까지 이루어지지 않은 노드
    * 말단, 리프노드(leaf): 더이상 분기하지 않는 노드
  * 엣지(edge): 노드를 연결하는 선

* 결정트리는 회귀, 분류 문제에 모두 적용 가능
  * 모두 비용함수를 최소화 하는 방향으로 데이터를 분할
  * 분류는 리프노드에 있는 타겟값들의 최빈값을 예측값으로 반환
  * 회귀는 리프노드에 있는 타겟값들의 평균을 예측값으로 반환


### 결정트리 비용함수
* 결정트리는 타겟 데이터의 불순도를 최소로 하는 지점을 찾아 데이터를 분할
  * 불순도(impurity): 여러 범주가 섞여 있는 정도
  * 불순도는 타겟 데이터를 분할하는 특성과 분할하는 지점을 찾아내는 기준이 됨
  * 지니불순도, 엔트로피가 주로 사용됨


#### 지니불순도
* 클래스가 혼합 되어 있을 때, 한 클래스의 비율이 높을수록 상대적으로 불순도가 낮음


#### 엔트로피
* 정보획득은 특정한 특성을 사용해 분할했을 때 엔트로피의 감소량을 뜻함
* 정보획득 = 분할 전 노드 불순도 - 분할 후 자식노드 들의 불순도

### 결정트리의 특징

1. 장점
   - 데이터 분할 과정이 직관적이고 시각화가 가능하여 이해하고 해석하기 용이함
   - 표준화, 더미변수 생성, 결측치 처리 등 전처리 과정이 많이 필요하지 않음
   - 트리모델은 특성 간 상호작용을 자연스럽게 포착할 수 있음
   - 다중 출력 문제를 풀 수 있음

2. 단점
   - 제약사항이 거의 없는 유연한 모델이기 때문에 과적합의 위험이 있음
   - 작은 데이터 변동으로도 다른 트리가 생성될 수 있기 때문에 불안정할 수 있음
   - 각 노드에 대해 국소적으로 최적의 의사 결정이 이루어지는 그리디 알고리즘을 기반으로 하기 때문에 전체적으로 최적인 의사결정 트리를 반환한다고 보장할 수 없음
   - 주어진 데이터를 분할하고 분할된 근사치를 예측값으로 반환하기 때문에 외삽이 어려움


## 파이프라인(Pipeline)

머신러닝 프로세스에서 파이프라인을 사용하는 이유

1. 편의성과 캡슐화
   - fit과 predict를 한 번 사용해도 전체 모델의 과정을 수행 가능
  
2. 하이퍼파라미터 선택
   - 한번에 모든 필요한 하이퍼파라미터를 서치할 수 있음
 
3. 안전성 
   - test data의 정보가 train data로 누수되는 것을 방지할 수 있음


* 트리 기반 모델에서는 원-핫 인코딩을 사용하지 않고 모두 Ordinal Encoding으로 변환해도 상관없음

### 결정트리 해석

결정트리는 시각화와 특성중요도를 통해 모델을 해석할 수 있음

1. 결정트리 시각화
   - graphviz

2. 특성중요도
   - 각 특성이 얼마나 먼저, 자주 분기에 사용되었는지에 따라 특성중요도를 계산

### 결정트리 과적합 해소

**트리의 복잡도를 줄이기 위한 하이퍼파라미터 조정**

* min_samples_split
* min_samples_leaf
* max_depth

* 결정트리 모델은 선형모델과 달리 
  
  1. 비선형, 비단조(non-monotonic), 특성상호작용(feature interactions) 특징을 가지고 있는 데이터 분석에 용이함
     - 단조: 데이터가 일관된 방향으로의 추세가 있는 경우
     - 특성상호작용: 특성들끼리 서로 상호작용을 하는 경우(변수들 간 상관관계) 

  2. 외삽이 어려움

## 랜덤포레스트(Random Forest)

* 결정트리의 과적합과 불안정성 단점을 보완
* 앙상블 기법 중 배깅(bagging)을 사용

### Bagging(Bootstrap Aggregating)

* 배깅은 훈련 데이터에서 무작위로 복원 추출한 샘플로 여러 기본 모델을 만들어 각 개별 모델의 예측 결과를 종합하여 최종 예측을하는 방법임
  
* 이런 방법은 예측을 하는 과정에서 랜덤성을 부여하고 랜덤하게 만들어진 기본 모델을 종합하면서 분산을 줄여 과적합을 피함

### 부트스트랩 샘플링

* 앙상블에 사용하는 기본 모델들은 부트스트래핑이라는 샘플링 과정으로 얻은 부트스트랩세트를 사용해 학습을 함
* 원본 데이터에서 샘플링을 할 때 복원추출을 한다는 의믜
* 같은 샘플이 반복될 수 있음

#### Out-of Bag Samples(oob sample)

* oob sample은 부트스트랩 과정에서 한 번도 추출되지 않은 샘플을 의미
* 데이터가 충분히 클 때 약 37% 의 샘플은 뽑히지 않음

#### Aggregation
- 부트스트랩세트로 만들어진 기본모델들을 합치는 과정
- 회귀: 평균으로 예측값 산출
- 분류: 다수결로 가장 많은 모델들이 선택한 범주로 예측

### Random Forest

- 결정트리를 기본모델로 사용하는 앙상블 기봅
- 결정트리들은 독립적으로 만들어지며 기본모델들을 합치는 과정에서 에러가 줄어듬
- 학습되는 트리들은 배깅을 통해 만들어짐
- 노드 불할할 때 특성 n개 중 일부분 k개의 특성을 랜덤하게 선택해서 분할 ($k = \log _{2}n$)

### RF 하이퍼파라미터

- max_depth
- min_samples_split
- min_samples_leaf
- n_estimators : 기본 모델의 수
- max_features : 분할에 사용되는 최대 특성의 수
- oob_score : oob를 이용한 검증 스코어 반환 여부


# 2. Boosting

## Bagging vs Boosting

- 앙상블 학습은 여러 기본 모델을 학습하고 모델들의 예측을 합하여 최종 예측을 내는 방식
- 일반화 성능을 제공함
- 약한 모델들을 많이 조합하는 것
- Bagging 과 Boosting이 있음

## Bagging

- 복원추출 - 기본 모델(weak learner) 학습 - 기본 모델(weak learner)들의 예측값을 평등하게 합침
- 기본 모델(weak learner)들을 병렬로 학습하고 평등하게 예측값을 합침
- 각 기본 모델(weak learner)들이 학습 시 상호 영향을 주고받지 않고 독립적, 병렬적으로 학습됨

>**Bagging은 모델의 분산을 줄여 과적합을 피하도록 해줌**

## Boosting

- Boosting은 모델들이 순차적으로 학습됨
- 지금까지 학습된 모델들이 잘 예측하지 못하는 부분에 집중해서 다음 모델을 학습시킴
- AdaBoost, GradientBoost 등이 있음

>**Boosting은 과정을 반복할수록 최종 모델의 복잡도를 상승시키며, 모델의 편향을 줄여 과소적합을 피하도록 해줌**

- weak learner들은 이전 모델의 오류를 고려하여 다음 모델이 순차적으로 학슴됨

### AdaBoost

- 각 단계별로 데이터셋으로부터 데이터를 샘플링하여 학습
- 다음 모델이 학습될 때, 이전 모델이 잘못 분류한 관측치가 샘플링될 확률을 높임
- 그래디언트 부스팅보다 이상치에 민감하고 성능이 떨어져 잘 사용하지 않음


### Gradient Boosting

- 회귀와 분류 모두 사용 가능
- 틀린 데이터에 집중하기 위해, 가중 샘플링을 하는 대신 잔차를 학습함
- 다음 모델이 이전 모델의 잔차를 학습
- 잔차가 큰 관측치를 더 학습하도록 하는 효과가 있으며, 이전 모델이 틀린 만큼을 직접 학습하여 이전 모델을 순차적으로 보완함

## XGBoost

- 2014년 공개된 라이브러리, 인기가 높음

### Gradient Boosting Decision Tree는 Tree-based 모델의 특성을 그대로 따름

- 특성 수치화 필요함(인코딩)
- 특성의 scaling, normalization 불필요
- 카디널리티 고려상 Ordinal encoding이 선호됨

### XGBoost 파라미터

- booster
  - weak learner 모델 설정 파라미터
  - gbtree: decision tree 모델 사용
  - dart: decision tree모델을 사용하되, dart 알고리즘을 사용해 모델을 정규화
    - 과적합 방지를 위한 이전 학습된 트리 중 몇개를 drop 시키는 기법
  - gblinear: 선형 모델 사용, 잘 사용하지 않음


- objective
  - 최소화하고자 하는 목적함수 설정
  - 설정 안하면 분류/회귀에 따라 기본값으로 설정
    - XGBClassifier: binary:logistic
    - XGBRegressor: reg:squarederror
  - reg:squarederror: MSE를 최소화
  - reg:logistic: 로지스틱 회귀 문제 해결
  - binary:logistic: 로지스틱 이진 분류

- eval_metric
  - 검증 데이터를 같이 넣어줄 경우, 검증 방법 설정 가능
  - 설정 안하면 objective 따라 기본값으로 설정
    - regression: rmse
    - classification: logloss


### XGBoost 하이퍼파라미터

- n_estimators: weak_learner 수 결정
- learning_rate
  - 단계별로 기본모델들을 얼마나 반영할지 결정
  - 0~1 범위
  - 값이 너무 크면 과적합
  - 값이 너무 작으면 학습이 느려짐
  - 일반적으로 0.05 ~ 0.3 정도
- max_depth
  - 각 기본모델 트리들의 최대 깊이 설정
  - 성능에 가장 큰 영향
  - -1 으로 설정시 깊이 제한 없음
  - 너무 크면 과적합과 메모리 사용량 증가
  - 일반적으로 5 ~ 12 사이
- min_child_weight
  - 리프노드에 포함되는 관측치의 수 결정
  - 커질수록 기본모델의 복잡도 감소
  - 과적합 발생 시 2배씩 늘려 성능 확인
- sumsample
  - 각 기본모델들을 학습할 때 과적합을 막고 일반화 성능을 올리기 위해 전체 데이터 중 일부를 샘플링하여 학습
  - 일반적으로 0.8
- colsample_bytree
  - 각 기본모델들을 학습할 때 과적합을 막고 일반화 성능 올리기 위해 전체 column 중 일부를 샘플링하여 학습
  - 일반적으로 0.8
  - 특성이 너무 많으면 작게 설정
- scale_pos_weight
  - class_weight 기능
  - sum(negative cases) / sum(positive cases) 값을 넣으면 balanced 옵션과 동일

### Early Stopping

- n_estimators 최적화를 위해 early_stopping 방법 사용 가능
- 일정 횟수에서 더 이상 성능이 향상되지 않으면 학습을 중단함
- 성증 결정 기준인 eval_set을 제공해줘야 함

# 3. Preprocessing

## Linear / Logistic Regression

1. 입력 특성들의 크기, 범위, 분포에 영향을 받음
   - 스케일링, 표준화 작업이 필요함

2. 결측치를 만드시 채워야 함
3. 비선형적 특성이나 특성 간 상호작용 등을 미리 처리해 줘야 함

## Tree Based Models

1. 입력 특성들의 크기, 범위, 분포에 영향을 받지 않음
   - 대소 관계에만 영향을 받음

2. 결측치를 반드시 채울 필요는 없음
3. 특성과 타겟 간의 비선형적 관계나 특성 간 상호작용이 자동으로 반영될 수 있음

## 결측치 처리 방법

결측치 발생 이유

- 특정 조건에서 자동으로 발생
- 미응답
- 실수
   
1. 결측치 그대로 두기
   - 결측치 자체가 정보가 될 수 있음

2. 단일 대표값으로 채우기
   - 결측치가 많지 않을 때 (~10% 내외)
   - SimpleImputer - mean, median, most_frequent, constant

3. 다른 특성들로부터 조건부로 채우기

## 수치형 변수 전처리

### 값 분포의 스케일만 변화시키는 변환

수치형 특성 값들의 분포 스케일만 변환

#### Min-Max Scaling

$$ x' = \frac{x - x_{min}}{x_{max} - x_{min}}$$ 

- 모든 값을 0과 1 사이로 스케일링
- 특성의 이상치가 정상치로부터 크게 벗어나지 않는다는 사실을 알 때, 또는 특성값들의 분포가 전체 범위에서 균등 분포를 보일 때 사용
  - 사람의 나이, 딥러닝 이미지 픽셀값 0~255
- 간단하지만 변환 후 평균이 0이 아니고 극단적인 이상치에 영향을 크게 받음

#### Standardization

$$ x' = \frac{x - \mu_{x}}{\sigma_{x}} $$ 

- 해당 특성의 평균을 0, 표준편차를 1로 조정
- 이상치에 덜 민감함
- 특성의 이상치 분포를 정확히 파악하기 어려울 때, 보통 처음에 사용
- 값들을 평행이동 및 크기 조정하여 평균과 표준편차만 0과 1로 조정해줄 뿐, 분포 자체를 정규분포의 모양으로 바꿔 주지는 않음

### 값 분포의 스케일과 형태를 변화시키는 전처리

분포 스케일뿐만 아니라, 분포의 형태까지 변화

#### Clipping

$$ x' = min(max(x, x_{min}), x_{max}) $$ 

- 특정 범위를 넘어서는 값들을 해당 경계값으로 변환시킴
- 비정상적인 이상치가 있을 때 시도
- 전체 분포에는 영향 없음

#### 로그 변환

$$ x' = log(1+x) $$ 

- 다수의 값들이 굉장히 제한된 범위 내에서만 존재하고 특정 값들이 굉장히 큰 형태의 분포에서 자주 적용됨
- 파레토 분포
- 모든 값들이 음이 아닐 때에만 적용할 수 있음
- 음의 값이 존재할 경우 적절한 값을 더해주어 양수로 먼저 변환해야 함

#### Bucketing

- 수치형 변수를 범주형처럼 다루는 방법
- 값들의 범위를 미리 지정하고, 어디에 속하는지 숫자로 표기
- 과세표준 등 분포에 사전 지식이 있는 경우

#### Rank 변환

- 값들을 해당 값들의 전체 데이터에서의 순위(혹은 percentile)로 변환
- 값들 간의 거리 정보들이 사라지고, 대소 관계만 남김
- 이상치에 민감하지 않고 어떠한 분포의 특성이든 균등 분포로 변환시켜줌


## 범주형 변수 전처리

**encoding**

#### One-hot encoding
- 각 범주형 특성 값들을 각 값들에 대해 해당/비해당 의 1과 0 값으로 인코딩함
- cardinality가 너무 클 경우에는 차원이 너무 커지기 때문에 적절하지 않음
- 트리 기반 모델에서는 특성 정보를 분산시키고 비효율성 때문에 사용 안함

#### Ordinal encoding
- 각 범주형 특성 값들을 단순히 서로 다른 정수값들로 인코딩함
- 순서형 변수
- 선형 회귀나 로지스틱에서는 양적 대소 관계처럼 보여서 사용 안함
- 트리기반 모델에서는 분기를 통해 양적 대소 관계가 점차 사라져서 효율적

#### Count encoding
- 각 카테고리의 등장 빈도로 범주형 값을 대체

#### Target encoding
- 각 범주형 특성값들을 해당 특성을 갖는 데이터의 타겟값의 평균으로 인코딩
- 해석이 쉽고 특성 차원을 늘리지 않음
- 특성값과 타겟값 간의 직접적인 관계를 모델링해 모델에게 중요한 정보를 제공 가능
- 과적합 문제에 취약함

# 4. Model Tunning

## Exhaustive Grid Search
- 범위 내 모든 하이퍼파라미터 조합으로 학습하고 가장 좋은 조합을 선택

## Randomized Search
- 범위 내에서 랜덤으로 몇 개의 조합을 선택하고 그 중 가장 좋은 조합을 선택
- 교차검증의 방법으로 탐색 진행
- 하이퍼파라미터 범위로 값을 지정할 수도 있고, 분포로 지정할 수도 있음
- 지정된 탐색 횟수만큼만 탐색

## Bayesian Search
- 검증하고자 하는 하이퍼파라미터의 범위 내에서, 이전에 탐색한 조합들의 성능을 기반으로 성능이 잘 나오는 조합들을 중심으로 확률적으로 탐색
- 랜덤서치보다 상대적으로 똑똑하고 효율적으로 탐색
- sklearn에 없음
- hyperopt 라이브러리에서 사용 가능: stochastic expressions

## 불필요한 특성 배제하기

### 직관에 기반한 특성 선택
- 사전 지식, 도메인 지식 활용


### 특성 중요도 기반 특성 선택
- feature_importance 등

### 통계량 기반 특성 선택
- 피어슨 상관계수
  - 연속형 특성과 연속형 타겟의 경우, 두 필드 간의 피어슨 상관계수를 통해 관련도를 추측 가능
  - 두 특성 간의 선형 관계성만 확인 가능

- 스피어만 상관계수
  - 두 특성의 단조성을 보여줌. 한 특성이 커질 때 다른 특성이 함께 커지는지를 수치화
  - 각 특성 값들을 그 값의 전체에서의 순위로 대치한 다음, 피어슨 상관계수를 구한 것과 동일

### sklearn.feature_selection.SelectKBest
- 문제의 유형에 따라 다양한 통계량을 기반으로 특성 선택을 도와주는 모듈
- 통계량 옵션: score_func
  - 분류
    - f_classif
    - mutual_info_classif
    - chi2
  - 회귀
    - f_regression
    - mutual_info_regression