{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN/EBTA5BnoBmuRjTxYTzh6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Post BERT (최근 NLP 연구 방향에 대해서)"],"metadata":{"id":"mikQvBrhe3Jz"}},{"cell_type":"markdown","source":["### 1) 더 큰 모델 만들기"],"metadata":{"id":"RJktwUaSe6vZ"}},{"cell_type":"markdown","source":["<img width=\"700\" alt=\"getting_bigger\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/02/TurningNGL_Model__1400x788.png\">"],"metadata":{"id":"ZkAY4e8Be7kW"}},{"cell_type":"markdown","source":["GPT(2018.6)와 BERT(2018.10) 이후로도 수많은 모델이 발표되어 왔습니다.<br/>\n","두 모델 이후로 발표되고 있는 모델의 주요 경향성 중 하나는 **모델 크기 키우기** 입니다.<br/>\n","위 그림에서 볼 수 있듯  GPT 와 BERT이후 발표되는 모델의 파라미터 수는 기하급수적으로 증가하고 있습니다.\n","\n","특히 2020년 6월에 발표된 GPT-3의 파라미터 개수는 약 1750억 개로 위 그림에 나와있는 T-NLG보다도 10배나 많습니다.<br/>\n","많은 모델이 크기를 키울수록 더 좋은 성능을 보여주고 있기 때문에 각 기업에서는 모델 크기를 계속해서 키우고 있는 상황입니다.<br/>\n","\n","하지만 크기가 커지면 사전 학습에 따른 비용이 많이 들고 그만큼 많은 학습 데이터를 확보해야 합니다.<br/>\n","이런 제약사항 때문에 초대형 모델은 학계보다는 구글, 페이스북 등과 같은 대기업에서만 이루어지고 있습니다.<br/>\n","큰 모델에 비해 그렇지 못한 모델이 좋은 성능을 보장하지 못하고 있기 때문에, 학계에서는 상대적으로 뒤처질 수밖에 없게됩니다.<br/>\n","이런 사태가 계속되면서 크기만 커지는 모델에 대한 우려의 시각도 있습니다."],"metadata":{"id":"-8-Ej6HJe8RV"}},{"cell_type":"markdown","source":["<img width=\"400\" alt=\"getting_bigger_gpt3\" src=\"https://miro.medium.com/max/1164/1*C-KNWQC_wXh-Q2wc6VPK1g.png\">"],"metadata":{"id":"37NPTd81e9go"}},{"cell_type":"markdown","source":["### 2) 더 좋은 학습 방법 적용하기"],"metadata":{"id":"IdeX-iaHe-cW"}},{"cell_type":"markdown","source":["여전히 더 좋은 학습 방법을 연구하고자 하는 움직임도 계속되고 있습니다.<br/>\n","특히 기존 GPT나 BERT의 단점을 보완하는 방향으로 많은 연구가 진행되고 있습니다.<br/>\n","\n","트랜스포머의 디코더 블록만을 사용한 GPT는 상대적으로 자연어 생성과 관련된 태스크에,<br/>\n","인코더 블록만을 사용한 BERT는 자연어 이해와 관련된 태스크에 특화되어 있습니다.<br/>\n","GPT와 같이 순차적으로 자연어를 생성하는 모델에는 AR(Auto-Regressive)한 방법이 적용되었다고 하고,<br/>\n","BERT와 같이 노이즈를 맞추어가는 방식으로 자연어를 이해하는 모델에는 AE(Auto-Encoder)한 방법이 적용되었다고 합니다.\n","\n","**두 모델이 사용했던 방법을 결합(AE+AR)**한 모델로 XLNet이나 BART가 있습니다.<br/>\n","두 모델 모두 자연어 이해와 생성 모두에서 좋은 성능을 보이며 특히 BART는 요약 태스크에서 좋은 성능을 보입니다.<br/>\n","\n","**다른 방향의 개선으로는 BERT의 Noising 방법을 어렵게** 만든 모델들이 있습니다.<br/>\n","대표적인 모델로 Masking 방법에 변화를 주는 SpanBERT, RoBERTa와 같은 모델이 있습니다."],"metadata":{"id":"1sy551oIe_DY"}},{"cell_type":"markdown","source":["아래는 BART 모델을 나타내는 그림입니다.\n","\n","<img width=\"500\" alt=\"bart\" src=\"https://miro.medium.com/max/1400/0*MeyyeTYxwtSZJPiL\">   \n","\n","위 그림처럼 AE 와 AR이 모두 적용되었습니다.<br/>\n","게다가 BART는 Masking 뿐만 아니라 Permutation, Infilling 등 다양한 Noising 방법이 적용되었다는 특징을 가지고 있습니다. \n","\n","<img width=\"500\" alt=\"bart_noising\" src=\"https://www.weak-learner.com/assets/img/blog/personal/bart_transformations.png\">"],"metadata":{"id":"Me7vi3KwfAAo"}},{"cell_type":"markdown","source":["### 3) 보다 가벼운 모델 만들기"],"metadata":{"id":"m0OK8b-jfBHY"}},{"cell_type":"markdown","source":["GPT와 BERT 기본 모델이라도 크기가 꽤 크다 보니 사이즈를 줄이되 성능은 보전하는 경량화로도 많은 연구가 진행되고 있습니다.<br/>\n","DistillBERT, ALBERT(A Light BERT) 나 ELECTRA가 이런 방향으로 연구된 대표적인 모델이라고 할 수 있습니다.\n","\n","세 모델 모두 각자만의 방법을 이용해서 BERT의 크기(=파라미터 수)를 많이 줄이고 성능은 어느정도 보존함으로써 모델 효율성을 높였습니다.\n","\n","아래는 ELECTRA의 모델 구조입니다.<br/>\n","다음 시간에 배울 GAN에 등장하는 방법론을 적용하여 BERT보다 적은 리소스를 활용하여 더 높은 성능을 기록하였습니다."],"metadata":{"id":"XQhIV9wbfCIf"}},{"cell_type":"markdown","source":["<img width=\"500\" alt=\"bart_noising\" src=\"https://1.bp.blogspot.com/-sHybc03nJRo/XmfLongdVYI/AAAAAAAAFbI/a0t5w_zOZ-UtxYaoQlVkmTRsyFJyFddtQCLcBGAsYHQ/s1600/image1.png\">"],"metadata":{"id":"iicExBuvfC70"}},{"cell_type":"markdown","source":["### 여러 방면에서의 다양한 시도"],"metadata":{"id":"WzaqRmFffDlm"}},{"cell_type":"markdown","source":["- **다양한 태스크를 수행할 수 있는 모델 (Meta Learning)**"],"metadata":{"id":"IlQvVMvrfEWH"}},{"cell_type":"markdown","source":["T5나 GPT-3와 같은 모델은 하나의 모델로 다양한 태스크를 수행할 수 있는 모델입니다.<br/>\n","특히 GPT-3는 Few-shot learning 방법론을 적용한 모델로 적당한 길이의 제시문만 주어주면 **Fine-tuning 없이도** 엄청나게 좋은 성능을 보여줍니다.\n","\n","N-shot learning 에 대해서는 아래 자료를 참고하시면 되겠습니다."],"metadata":{"id":"iGNDYR8kfFJ8"}},{"cell_type":"markdown","source":["1. ***파인튜닝(finetuning)*** : 다운스트림 태스크에 해당하는 데이터 전체를 사용합니다. 모델 전체를 다운스트림 데이터에 맞게 업데이트합니다.\n","\n","2. ***제로샷러닝(zero-shot learning)*** : 다운스트림 태스크 데이터를 전혀 사용하지 않습니다. 모델이 바로 다운스트림 태스크를 수행합니다.\n","\n","3. ***원샷러닝(one-shot learning)*** : 다운스트림 태스크 데이터를 한 건만 사용합니다. 모델 전체를 1건의 데이터에 맞게 업데이트합니다. 업테이트 없이 수행하는 원샷러닝도 있습니다. 모델이 1건의 데이터가 어떻게 수행되는지 참고한 뒤 바로 다운스트림 태스크를 수행합니다.\n","\n","4. ***퓨샷러닝(few-shot learning)*** : 다운스트림 태스크 데이터를 몇 건만 사용합니다. 모델 전체를 몇 건의 데이터에 맞게 업데이트합니다. 업데이트 없이 수행하는 퓨삿러닝도 있습니다. 모델이 몇 건의 데이터가 어떻게 수행되는지 참고한 뒤 바로 다운스트림 태스크를 수행합니다."],"metadata":{"id":"sN7DZWA2fF3L"}},{"cell_type":"markdown","source":["- **다국어(Multilingual) 모델**"],"metadata":{"id":"7UZepJSXfGky"}},{"cell_type":"markdown","source":["**다국어 모델** 역시 열심히 연구되고 있는 분야입니다.<br/>\n","일반적인 언어 모델의 경우 단일 말뭉치로만 사전 학습을 진행하는 경우가 많습니다.<br/>\n","그렇기 때문에 사전 학습된 언어 외에 다른 언어를 사용하면 성능이 급격히 저하되는 경우가 많습니다.\n","\n","예를 들어, 영어로 학습된 GPT-3 모델에 한국어를 집어넣으면 거의 이해하지 못하게 됩니다.<br/> \n","이러한 문제를 뛰어넘기 위해 다양한 언어를 넘나들며 사용할 수 있는 모델이 연구되고 있는데 이를 다국어 모델 이라고 합니다.\n","\n","대표적인 다국어 모델로는 mBART(multi-lingual BART), mT5(multi-lingual T5) 등이 있습니다."],"metadata":{"id":"EyLv2tSzfHK7"}},{"cell_type":"markdown","source":["- **자연어를 넘어(1) : 컴퓨터 비전(Computer Vision, CV)에서의 트랜스포머**"],"metadata":{"id":"_mQKfiJRfIC7"}},{"cell_type":"markdown","source":["트랜스포머는 원래 자연어처리 중 번역 태스크에 적용하기 위해서 나온 모델이었습니다.<br/>\n","하지만 최근에는 컴퓨터 비전 태스크인 이미지 처리에서도 트랜스포머를 적용하고자 하는 움직임이 나타나고 있습니다.<br/>\n","\n","[ViT(Vision in Transformer)](https://arxiv.org/abs/2010.11929) 논문에서는 컴퓨터 비전 분야에서 SOTA인 CNN 계열 모델보다 트랜스포머가 더 좋은 성능을 나타냈다고 말하고 있습니다.<br/>\n","(CNN 모델이 무엇인지 당장은 몰라도 좋습니다. Sprint 3에서 자세히 배우게 될 것입니다.)\n","\n","아직 컴퓨터 비전 분야에서는 CNN에 비해 개발 속도가 더딘 편이지만,<br/>\n","트랜스포머를 사용하여 비전과 자연어 모두 정복하기 위한 다양한 시도가 지금도 진행되고 있습니다."],"metadata":{"id":"Gt2wTjIjfIzy"}},{"cell_type":"markdown","source":["- **자연어를 넘어(2) : 멀티 모달(Multi-modal) 모델**"],"metadata":{"id":"BGYPIar1fJkG"}},{"cell_type":"markdown","source":["지난 1월에는 GPT를 발표했던 OpenAI에서 DALL-E 와 CLIP 이라는 재미있는 모델을 발표했습니다.<br/>\n","이 모델은 텍스트(문장)를 입력받아 상응하는 이미지를 생성합니다.\n","\n","아래는 DALL-E 가 _\"an armchair in the shape of an avocado\"_ 라는 문장을 입력받은 뒤 출력한 이미지입니다.\n","\n"],"metadata":{"id":"TE2_dQTCfKTY"}},{"cell_type":"markdown","source":["<img width=\"700\" alt=\"dall-e\" src=\"https://user-images.githubusercontent.com/45377884/113083201-b9425500-9216-11eb-989a-3e5f28a794e5.png\">"],"metadata":{"id":"THzQX4OvfLD6"}},{"cell_type":"markdown","source":["이렇게 자연어를 넘어 다양한 매체로 기계와 소통하는 태스크를 **멀티모달(Multi-Modal)**이라고 합니다.<br/>\n","트랜스포머가 자연어처리 뿐만 아니라 컴퓨터 비전에 대해서도 좋은 성능을 보이기 때문에 트랜스포머를 활용한 멀티모달 연구도 활발하게 진행되고 있습니다."],"metadata":{"id":"NVSs3C4KfLzm"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"pZE6itUDeydk"},"outputs":[],"source":[]}]}