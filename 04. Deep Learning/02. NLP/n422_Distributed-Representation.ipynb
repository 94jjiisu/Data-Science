{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNlyeJIx4RWPiND4TSrm9tc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# 단어를 분산 표현(Distributed Representation)으로 나타내기\n","\n","## 학습목표\n","\n","- **단어의 분산 표현(Distributed Representation)**\n","    - 원-핫 인코딩의 개념과 단점에 대해서 이해할 수 있습니다.\n","    - 분포 기반의 표현, 임베딩이 무엇인지 설명할 수 있습니다. \n","\n","- **Word2Vec**\n","    - CBoW와 Skip-gram의 차이에 대해서 설명할 수 있습니다.\n","    - Word2Vec의 임베딩 벡터를 시각화한 결과가 어떤 특징을 가지는지 설명할 수 있습니다.\n","\n","- **fastText**\n","    - OOV 문제가 무엇인지에 대해 설명할 수 있습니다.\n","    - 철자(Character) 단위 임베딩 방법의 장점에 대해 설명할 수 있습니다.\n","\n","영상참조\n"," - [Word2Vec](https://youtu.be/sY4YyacSsLc)"],"metadata":{"id":"DzUKKHG9-ocv"}},{"cell_type":"markdown","source":["## 1. Distributed Representation\n","\n","이전 시간에 문서 내에 단어가 등장하는 횟수를 기반으로 문서를 벡터화하는 **등장 횟수 기반 표현(Count-based Representation)**에 대해서 알아보았습니다.\n","\n","\n","이번 시간에는 단어 자체를 벡터화하는 방법에 대해서 알아보고자 합니다.<br/>\n","이번 시간에 배우게 될 Word2Vec, fastText는 우리가 벡터로 표현하고자 하는 타겟 단어(Target word)가 해당 단어 주변 단어에 의해 결정됩니다.\n","\n","단어 벡터를 이렇게 정하는 이유는 [분포 가설(Distribution hypothesis)](https://en.wikipedia.org/wiki/Distributional_semantics) 때문입니다.<br/>\n","분포 가설은 다음과 같습니다.\n","\n","> **'비슷한 위치에서 등장하는 단어들은 비슷한 의미를 가진다'**"],"metadata":{"id":"miyyMpnD-v2_"}},{"cell_type":"markdown","source":["예를 들어, 두 문장\n","\n","- I found **good** stores.\n","- I found **beautiful** stores.\n","\n","에서 _\"**good** 과 **beautiful** 은 해당 단어 주변에 분포한 단어가 유사하기 때문에 비슷한 의미를 지닐 것이다\"_ 라고 가정하는 것이 분포 가설입니다.\n","\n","바로 이 분포 가설에 기반하여 주변 단어 분포를 기준으로 단어의 벡터 표현이 결정되기 때문에 **<font color=\"ff6f61\">분산 표현(Distributed representation)</font>**이라고 부르게 됩니다."],"metadata":{"id":"QO0grcp_-2A9"}},{"cell_type":"markdown","source":["### 1) 원-핫 인코딩(One-hot Encoding)\n","\n","원-핫 인코딩은 단어를 벡터화하고자 할 때 선택할 수 있는 가장 쉬운 방법입니다. <br/>\n","표 형태로 된 데이터를 다룰 때 범주형 변수를 요소마다 [0 0 0 1 ...] 의 형태로 나타냈던 것과 동일한 방법을 사용합니다.\n","\n","> \"I am a student\"\n","\n","라는 문장에서 각 단어를 원-핫 인코딩으로 나타내면 다음과 같습니다.\n","\n","> I : [1 0 0 0] <br/>\n","> am : [0 1 0 0] <br/>\n","> a : [0 0 1 0] <br/>\n","> student : [0 0 0 1]"],"metadata":{"id":"Wt1h5X6O-3Rw"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4PVFGwlFzFKP","executionInfo":{"status":"ok","timestamp":1679784907505,"user_tz":-540,"elapsed":3,"user":{"displayName":"박지수","userId":"15151265950850399854"}},"outputId":"a797b317-168b-4f75-b502-7664ef653d2a"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'I': [1, 0, 0, 0], 'am': [0, 1, 0, 0], 'a': [0, 0, 1, 0], 'student': [0, 0, 0, 1]}\n"]}],"source":["sent = \"I am a student\"\n","word_lst = sent.split()\n","\n","word_dict = {}\n","\n","for idx, word in enumerate(word_lst):\n","    vec = [0 for _ in range(len(word_lst))]\n","    vec[idx] = 1\n","\n","    word_dict[word] = vec\n","\n","print(word_dict)"]},{"cell_type":"markdown","source":["쉽게 이해할 수 있는 직관적인 방법이지만 원-핫 인코딩에는 치명적인 단점이 있습니다.<br/>\n","바로 단어 간 유사도를 구할 수 없다는 점입니다.\n","\n","단어 간 유사도를 구할 때에는 코사인 유사도(cosine similarity)가 자주 사용됩니다.<br/>\n","코사인 유사도를 구하기 위한 식은 다음과 같습니다.\n","\n","$$\n","\\large \\text{Cosine similarity} = \\frac{\\vec{a} \\cdot \\vec{b} }{\\vert \\vec{a} \\vert \\vert \\vec{b} \\vert }\n","$$"],"metadata":{"id":"8t-QI2Fz-nyq"}},{"cell_type":"markdown","source":["원-핫 인코딩을 사용한 두 벡터의 내적은 항상 0이므로 어떤 두 단어를 골라 코사인 유사도를 구하더라도 그 값은 0이 됩니다.<br/>\n","이렇게 두 단어 사이의 관계를 전혀 알 수 없다는 것이 원-핫 인코딩의 최대 단점입니다."],"metadata":{"id":"fsvQsL0w-7AA"}},{"cell_type":"code","source":["import numpy as np\n","\n","def cos_sim(a, b):\n","    arr_a = np.array(a)\n","    arr_b = np.array(b)\n","\n","    result = np.dot(arr_a, arr_b)/(np.linalg.norm(arr_a)*np.linalg.norm(arr_b))\n","    return result\n","\n","print(f\"I 와 am 의 코사인 유사도 : {cos_sim(word_dict['I'], word_dict['am'])}\")\n","print(f\"I 와 student 의 코사인 유사도 : {cos_sim(word_dict['I'], word_dict['student'])}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qnGDvldL-6Oj","executionInfo":{"status":"ok","timestamp":1679784919256,"user_tz":-540,"elapsed":503,"user":{"displayName":"박지수","userId":"15151265950850399854"}},"outputId":"622d807a-2b01-4d98-f2a5-b3948cb86a32"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["I 와 am 의 코사인 유사도 : 0.0\n","I 와 student 의 코사인 유사도 : 0.0\n"]}]},{"cell_type":"markdown","source":["### 2) 임베딩(Embedding)\n","\n","원-핫 인코딩의 단점을 해결하기 위해 등장한 것이 **<font color=\"ff6f61\">임베딩(Embedding)</font>**입니다. <br/>\n","단어를 고정 길이의 벡터, 즉 차원이 일정한 벡터로 나타내기 때문에 '임베딩'이라는 이름이 붙었습니다.<br/>\n","임베딩을 거친 단어 벡터는 원-핫 인코딩과는 다른 형태의 값을 가집니다.\n","\n","예를 들어,\n","\n","> [0.04227, -0.0033, 0.1607, -0.0236, ...]\n","\n","위와 같이 벡터 내의 각 요소가 연속적인 값을 가지게 됩니다.<br/>\n","어떻게 이런 벡터가 만들어지는지, 가장 널리 알려진 임베딩 방법인 **`Word2Vec`** 를 통해 알아보도록 하겠습니다.\n","\n","> ❗️ ***임베딩은 자연어처리 이외에 다른 딥러닝 분야(추천 시스템, GNN 등)에서도 사용되니 잘 기억해두도록 합시다. :)***"],"metadata":{"id":"z2blYqBo-9Lp"}},{"cell_type":"markdown","source":["## 2. Word2Vec\n","\n","2013년에 고안된 **<font color=\"ff6f61\">Word2Vec</font>** 은 말 그대로 **단어를 벡터로(Word to Vector) 나타내는 방법**으로 가장 널리 사용되는 임베딩 방법 중 하나입니다.<br>\n","**`Word2Vec`**은 특정 단어 양 옆에 있는 두 단어(window size = 2)의 관계를 활용하기 때문에 **분포 가설을 잘 반영**하고 있습니다.\n","\n","`Word2Vec` 에는 `CBoW`와 `Skip-gram`의 2가지 방법이 있습니다.<br/>\n","두 가지 방법이 어떻게 다른지에 대해서 알아보도록 하겠습니다.\n","\n","### 1) CBoW 와 Skip-gram\n","\n","`CBoW`와 `Skip-gram`의 차이는 \n","\n","1. 주변 단어에 대한 정보를 기반으로 중심 단어의 정보를 예측하는 모델인지 ▶️ **<font color=\"ff6f61\">CBoW(Continuous Bag-of-Words)</font>**\n","2. 중심 단어의 정보를 기반으로 주변 단어의 정보를 예측하는 모델인지 ▶️ **<font color=\"ff6f61\">Skip-gram</font>**\n","\n","에 따라서 달라집니다.\n","\n","아래 그림을 통해 두 방식의 차이를 좀 더 잘 이해해보도록 하겠습니다."],"metadata":{"id":"GQyEWnSi--_3"}},{"cell_type":"markdown","source":["<img src=\"https://www.researchgate.net/profile/Nailah_Al-Madi/publication/319954363/figure/fig1/AS:552189871353858@1508663732919/CBOW-and-Skip-gram-models-architecture-1.png\" width=\"800\" />\n","\n","\n","예시를 통해서도 둘의 차이를 알아보겠습니다.<br/>\n","<별 헤는 밤> 의 일부분에 형태소 분석기를 적용하여 토큰화한 것입니다.\n","\n","> “… 어머님 나 는 별 하나 에 아름다운 말 한마디 씩 불러 봅니다 …”\n","\n","**CBoW** 를 사용하면 표시된 단어 정보를 바탕으로 아래의 [ ---- ] 에 들어갈 단어를 예측하는 과정으로 학습이 진행됩니다.\n","\n","> “… 나 는 [ -- ] 하나 에 … “ <br/>\n","> “… 는 별 [ ---- ] 에 아름다운 …”<br/>\n","> “… 별 하나 [ -- ] 아름다운 말 …”<br/>\n","> “… 하나 에 [ -------- ] 말 한마디 …”\n","\n","**Skip-gram** 을 사용하면 표시된 단어 정보를 바탕으로 다음의 [ ---- ] 에 들어갈 단어를 예측하는 과정으로 학습이 진행됩니다.\n","\n","> “… [ -- ] [ -- ] 별 [ ---- ] [ -- ] …” <br/>\n","> “… [ -- ] [ -- ] 하나 [ -- ] [ -------- ] …” <br/>\n","> “… [ -- ] [ ---- ] 에 [ -------- ] [ -- ] …” <br/>\n","> “… [ ---- ] [ -- ] 아름다운 [ -- ] [ ------ ] …”\n","\n","더 많은 정보를 바탕으로 특정 단어를 예측하기 때문에 CBoW의 성능이 더 좋을 것으로 생각하기 쉽지만,<br/>\n","역전파 관점에서 보면 Skip-gram에서 훨씬 더 많은 학습이 일어나기 때문에 **Skip-gram의 성능이 조금 더 좋게 나타납니다.**<br/>\n","물론 계산량이 많기 때문에 Skip-gram에 드는 리소스가 더 큰 것도 사실입니다.\n"],"metadata":{"id":"duxfan0D_Eyp"}},{"cell_type":"markdown","source":["### Word2Vec 모델의 구조\n","\n","\n","성능 덕분에 조금 더 자주 사용되는 Skip-gram을 기준으로 Word2Vec의 구조에 대해 알아보겠습니다.\n","\n","- 입력 : Word2Vec의 입력은 원-핫 인코딩된 단어 벡터입니다.\n","- 은닉층 : 임베딩 벡터의 차원수 만큼의 노드로 구성된 은닉층이 1개인 신경망입니다.\n","- 출력층 : 단어 개수 만큼의 노드로 이루어져 있으며 활성화 함수로 소프트맥스를 사용합니다.\n","\n","아래 그림을 통해서 Word2Vec 모델의 개략적인 구조에 대해 알아보겠습니다.\n","\n","논문에서는 총 10,000개의 단어에 대해서 300차원의 임베딩 벡터를 구했기 때문에<br/>\n","신경망 구조가 아래와 같아졌습니다.\n"],"metadata":{"id":"st38NZWt_IIt"}},{"cell_type":"markdown","source":["<img src=\"http://mccormickml.com/assets/word2vec/skip_gram_net_arch.png\" width=\"800\" />\n"],"metadata":{"id":"PgqGhUYM_Lqw"}},{"cell_type":"markdown","source":["### Word2Vec 학습을 위한 학습 데이터 디자인\n","\n","효율적인 Word2Vec 학습을 위해서는 학습 데이터를 잘 구성해야 합니다.<br/>\n","Window 사이즈가 2인 Word2Vec 이므로 중심 단어 옆에 있는 2개 단어에 대해 단어쌍을 구성합니다.\n","\n","예를 들어, **\"The tortoise jumped into the lake\"** 라는 문장에 대해 단어쌍을 구성해보겠습니다.<br/>\n","윈도우 크기가 2인 경우 다음과 같이 Skip-gram을 학습하기 위한 데이터 쌍을 구축할 수 있습니다.\n","\n","- 중심 단어 : **The**, 주변 문맥 단어 : tortoise, jumped\n","    - 학습 샘플: (the, tortoise), (the, jumped)\n","- 중심 단어 : **tortoise**, 주변 문맥 단어 : the, jumped, into\n","    - 학습 샘플: (tortoise, the), (tortoise, jumped), (tortoise, into)\n","- 중심 단어 : **jumped**, 주변 문맥 단어 : the, tortoise, into, the\n","    - 학습 샘플: (jumped, the), (jumped, tortoise), (jumped, into), (jumped, the)\n","- 중심 단어 : **into**, 주변 문맥 단어 : tortoise, jumped, the, lake\n","    - 학습 샘플: (into, tortoise), (into, jumped), (into, the), (into, lake)\n","\n","이런 방법으로 학습 데이터를 만들면 다음과 같은 데이터쌍이 만들어 집니다. \n","\n","|중심단어|문맥단어|\n","|---------|---------|\n","|the|tortoise|\n","|the|jumped|\n","|tortoise|the|\n","|tortoise|jumped|\n","|tortoise|into|\n","|jumped|the|\n","|jumped|tortoise|\n","|jumped|into|\n","|jumped|the|\n","|into|tortoise|\n","|into|jumped|\n","|into|the|\n","|into|lake|\n","|...|...|"],"metadata":{"id":"Q_OT3TtX_M8O"}},{"cell_type":"markdown","source":["Skip-gram 에서는 중심단어를 입력으로, 문맥단어를 레이블로 하는 분류(Classification)를 통해 학습한다고 생각하시면 되겠습니다.\n","\n","### Word2Vec의 결과\n","\n","학습이 모두 끝나면 10000개의 단어에 대해 300차원의 임베딩 벡터가 생성됩니다. <br/>\n","만약에 임베딩 벡터의 차원을 조절하고 싶다면 은닉층의 노드 수를 줄이거나 늘릴 수 있습니다.<br/>\n","\n","아래 그림은 신경망 내부에 있는 $10000 \\times 300$ 크기의 가중치 행렬에 의해서<br/>\n","10000개 단어에 대한 300차원의 벡터가 생성되는 모습을 나타낸 이미지입니다.\n","\n","<img src=\"https://i.imgur.com/1ETMljf.png\" width=\"600\" />\n"],"metadata":{"id":"-fkEsh-S_OlD"}},{"cell_type":"markdown","source":["학습과정에서 효율을 높이기 위해 사용하는 기법들이 있지만 아직은 너무 깊게 들어가지 않겠습니다.<br/>\n","추후 아래 키워드를 통해 Word2Vec을 조금 더 적은 계산으로 하는 방법에 대해 조사해보시면 좋겠습니다.\n","\n","- 더 알아보기\n","    - Sub-sampling\n","    - Negative-sampling\n","\n","\n","결과적으로 Skip-gram 모델을 통해 10000개 단어에 대한 임베딩 벡터를 얻을 수 있습니다.<br/>\n","이렇게 얻은 임베딩 벡터는 문장 간의 관련도 계산, 문서 분류같은 작업에 사용할 수 있습니다."],"metadata":{"id":"2GEuMGRG_SAX"}},{"cell_type":"markdown","source":["### Word2Vec으로 임베딩한 벡터 시각화\n","\n","Word2Vec을 통해 얻은 임베딩 벡터는 **<font color=\"ff6f61\">단어 간의 의미적, 문법적 관계를 잘 나타냅니다.</font>**<br/>\n","이를 대표적으로 잘 보여주는 것이 아래 그림입니다.<br/>\n","\n","1. **`man - woman`** 사이의 관계와 **`king - queen`** 사이의 관계가 매우 유사하게 나타납니다.<br/>\n","생성된 임베딩 벡터가 단어의 **의미적(Semantic) 관계를 잘 표현**하는 것을 확인할 수 있습니다.\n","\n","2. **`walking - walked`** 사이의 관계와 **`swimming - swam`** 사이의 관계가 매우 유사하게 나타납니다.<br/>\n","생성된 임베딩 벡터가 단어의 **문법적(혹은 구조적, Syntactic)인 관계도 잘 표현**하는 것을 확인할 수 있습니다.\n","\n","3. 고유명사에 대해서도 나라 - 수도 와 같은 관계를 잘 나타내고 있는 것을 확인할 수 있습니다."],"metadata":{"id":"i30S2j0W_YWw"}},{"cell_type":"markdown","source":["<img src=\"https://miro.medium.com/max/3010/1*OEmWDt4eztOcm5pr2QbxfA.png\"/>"],"metadata":{"id":"e9UHLgWi_aKC"}},{"cell_type":"markdown","source":["### gensim 패키지로 word2Vec 실습하기\n","\n","**`gensim`** 은 **`Word2Vec`** 으로 사전 학습된 임베딩 벡터를 쉽게 사용해볼 수 있는 패키지입니다.<br/>\n","gensim을 사용하여 word2vec의 결과가 어떻게 도출되는지 알아보겠습니다.\n","\n","0. (시작하기 전에) **`gensim` 패키지를 최신 버전으로 업그레이드** 합니다.\n","\n","아래 `--upgrade` 셀을 실행하여 패키지를 업그레이드 한 후,<br/>\n","메뉴 탭에서 '런타임' > '런타임 다시 시작'을 클릭하여 런타임을 다시 시작합니다.\n","\n","이후 아래 `.__version__` 이 있는 셀을 활용하여 최신 버전인지 확인합니다."],"metadata":{"id":"Kqt0vNsw_bHq"}},{"cell_type":"code","source":["!pip install gensim --upgrade"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KsOvK-HH-7wt","executionInfo":{"status":"ok","timestamp":1679785064442,"user_tz":-540,"elapsed":6228,"user":{"displayName":"박지수","userId":"15151265950850399854"}},"outputId":"a7a3c6df-6137-4c1a-ccfe-b75e1b27b72d"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: gensim in /usr/local/lib/python3.9/dist-packages (4.3.1)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from gensim) (6.3.0)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.9/dist-packages (from gensim) (1.22.4)\n","Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.9/dist-packages (from gensim) (1.10.1)\n"]}]},{"cell_type":"code","source":["import gensim\n","\n","gensim.__version__"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"vsrLx6wZ_dhm","executionInfo":{"status":"ok","timestamp":1679785066774,"user_tz":-540,"elapsed":1819,"user":{"displayName":"박지수","userId":"15151265950850399854"}},"outputId":"9318f504-a999-4540-8072-34e8972b3209"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'4.3.1'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["1. **구글 뉴스 말뭉치로 학습된 `word2vec` 벡터를 다운받습니다.** (시간이 오래 걸립니다.)"],"metadata":{"id":"_pe_qYnu_gfX"}},{"cell_type":"code","source":["import gensim.downloader as api\n","\n","wv = api.load('word2vec-google-news-300')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5iAwpAVY_fZ0","executionInfo":{"status":"ok","timestamp":1679785568781,"user_tz":-540,"elapsed":490276,"user":{"displayName":"박지수","userId":"15151265950850399854"}},"outputId":"73e4cecb-e985-4d16-c1c3-4ef1f75c6631"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"]}]},{"cell_type":"code","source":["for idx, word in enumerate(wv.index_to_key):\n","    if idx == 10:\n","        break\n","\n","    print(f\"word #{idx}/{len(wv.index_to_key)} is '{word}'\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k7z3mDzk_ixJ","executionInfo":{"status":"ok","timestamp":1679785568782,"user_tz":-540,"elapsed":6,"user":{"displayName":"박지수","userId":"15151265950850399854"}},"outputId":"b4b1677f-183d-4137-cacb-77a64968e2fb"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["word #0/3000000 is '</s>'\n","word #1/3000000 is 'in'\n","word #2/3000000 is 'for'\n","word #3/3000000 is 'that'\n","word #4/3000000 is 'is'\n","word #5/3000000 is 'on'\n","word #6/3000000 is '##'\n","word #7/3000000 is 'The'\n","word #8/3000000 is 'with'\n","word #9/3000000 is 'said'\n"]}]},{"cell_type":"markdown","source":["3. **임베딩 벡터의 차원과 값을 눈으로 확인해봅시다.**\n","\n","**king** 이라는 단어의 벡터의 shape을 출력하여 임베딩 벡터의 차원을 확인해봅시다.<br/>\n","결과를 통해 **`Word2Vec`** 을 통해 학습된 임베딩 벡터는 300차원이며, 벡터의 요소가 원-핫 인코딩과는 다르다는 것을 확인할 수 있습니다."],"metadata":{"id":"kKeXkX0p_kuo"}},{"cell_type":"code","source":["vec_king = wv['king']\n","\n","print(f\"Embedding dimesion is : {vec_king.shape}\\n\")\n","print(f\"Embedding vector of 'king' is \\n\\n {vec_king}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ld-nzBVr_jdK","executionInfo":{"status":"ok","timestamp":1679785569272,"user_tz":-540,"elapsed":8,"user":{"displayName":"박지수","userId":"15151265950850399854"}},"outputId":"20cc0ec1-3d1f-4197-c80e-c8e684c8ed25"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Embedding dimesion is : (300,)\n","\n","Embedding vector of 'king' is \n","\n"," [ 1.25976562e-01  2.97851562e-02  8.60595703e-03  1.39648438e-01\n"," -2.56347656e-02 -3.61328125e-02  1.11816406e-01 -1.98242188e-01\n","  5.12695312e-02  3.63281250e-01 -2.42187500e-01 -3.02734375e-01\n"," -1.77734375e-01 -2.49023438e-02 -1.67968750e-01 -1.69921875e-01\n","  3.46679688e-02  5.21850586e-03  4.63867188e-02  1.28906250e-01\n","  1.36718750e-01  1.12792969e-01  5.95703125e-02  1.36718750e-01\n","  1.01074219e-01 -1.76757812e-01 -2.51953125e-01  5.98144531e-02\n","  3.41796875e-01 -3.11279297e-02  1.04492188e-01  6.17675781e-02\n","  1.24511719e-01  4.00390625e-01 -3.22265625e-01  8.39843750e-02\n","  3.90625000e-02  5.85937500e-03  7.03125000e-02  1.72851562e-01\n","  1.38671875e-01 -2.31445312e-01  2.83203125e-01  1.42578125e-01\n","  3.41796875e-01 -2.39257812e-02 -1.09863281e-01  3.32031250e-02\n"," -5.46875000e-02  1.53198242e-02 -1.62109375e-01  1.58203125e-01\n"," -2.59765625e-01  2.01416016e-02 -1.63085938e-01  1.35803223e-03\n"," -1.44531250e-01 -5.68847656e-02  4.29687500e-02 -2.46582031e-02\n","  1.85546875e-01  4.47265625e-01  9.58251953e-03  1.31835938e-01\n","  9.86328125e-02 -1.85546875e-01 -1.00097656e-01 -1.33789062e-01\n"," -1.25000000e-01  2.83203125e-01  1.23046875e-01  5.32226562e-02\n"," -1.77734375e-01  8.59375000e-02 -2.18505859e-02  2.05078125e-02\n"," -1.39648438e-01  2.51464844e-02  1.38671875e-01 -1.05468750e-01\n","  1.38671875e-01  8.88671875e-02 -7.51953125e-02 -2.13623047e-02\n","  1.72851562e-01  4.63867188e-02 -2.65625000e-01  8.91113281e-03\n","  1.49414062e-01  3.78417969e-02  2.38281250e-01 -1.24511719e-01\n"," -2.17773438e-01 -1.81640625e-01  2.97851562e-02  5.71289062e-02\n"," -2.89306641e-02  1.24511719e-02  9.66796875e-02 -2.31445312e-01\n","  5.81054688e-02  6.68945312e-02  7.08007812e-02 -3.08593750e-01\n"," -2.14843750e-01  1.45507812e-01 -4.27734375e-01 -9.39941406e-03\n","  1.54296875e-01 -7.66601562e-02  2.89062500e-01  2.77343750e-01\n"," -4.86373901e-04 -1.36718750e-01  3.24218750e-01 -2.46093750e-01\n"," -3.03649902e-03 -2.11914062e-01  1.25000000e-01  2.69531250e-01\n","  2.04101562e-01  8.25195312e-02 -2.01171875e-01 -1.60156250e-01\n"," -3.78417969e-02 -1.20117188e-01  1.15234375e-01 -4.10156250e-02\n"," -3.95507812e-02 -8.98437500e-02  6.34765625e-03  2.03125000e-01\n","  1.86523438e-01  2.73437500e-01  6.29882812e-02  1.41601562e-01\n"," -9.81445312e-02  1.38671875e-01  1.82617188e-01  1.73828125e-01\n","  1.73828125e-01 -2.37304688e-01  1.78710938e-01  6.34765625e-02\n","  2.36328125e-01 -2.08984375e-01  8.74023438e-02 -1.66015625e-01\n"," -7.91015625e-02  2.43164062e-01 -8.88671875e-02  1.26953125e-01\n"," -2.16796875e-01 -1.73828125e-01 -3.59375000e-01 -8.25195312e-02\n"," -6.49414062e-02  5.07812500e-02  1.35742188e-01 -7.47070312e-02\n"," -1.64062500e-01  1.15356445e-02  4.45312500e-01 -2.15820312e-01\n"," -1.11328125e-01 -1.92382812e-01  1.70898438e-01 -1.25000000e-01\n","  2.65502930e-03  1.92382812e-01 -1.74804688e-01  1.39648438e-01\n","  2.92968750e-01  1.13281250e-01  5.95703125e-02 -6.39648438e-02\n","  9.96093750e-02 -2.72216797e-02  1.96533203e-02  4.27246094e-02\n"," -2.46093750e-01  6.39648438e-02 -2.25585938e-01 -1.68945312e-01\n","  2.89916992e-03  8.20312500e-02  3.41796875e-01  4.32128906e-02\n","  1.32812500e-01  1.42578125e-01  7.61718750e-02  5.98144531e-02\n"," -1.19140625e-01  2.74658203e-03 -6.29882812e-02 -2.72216797e-02\n"," -4.82177734e-03 -8.20312500e-02 -2.49023438e-02 -4.00390625e-01\n"," -1.06933594e-01  4.24804688e-02  7.76367188e-02 -1.16699219e-01\n","  7.37304688e-02 -9.22851562e-02  1.07910156e-01  1.58203125e-01\n","  4.24804688e-02  1.26953125e-01  3.61328125e-02  2.67578125e-01\n"," -1.01074219e-01 -3.02734375e-01 -5.76171875e-02  5.05371094e-02\n","  5.26428223e-04 -2.07031250e-01 -1.38671875e-01 -8.97216797e-03\n"," -2.78320312e-02 -1.41601562e-01  2.07031250e-01 -1.58203125e-01\n","  1.27929688e-01  1.49414062e-01 -2.24609375e-02 -8.44726562e-02\n","  1.22558594e-01  2.15820312e-01 -2.13867188e-01 -3.12500000e-01\n"," -3.73046875e-01  4.08935547e-03  1.07421875e-01  1.06933594e-01\n","  7.32421875e-02  8.97216797e-03 -3.88183594e-02 -1.29882812e-01\n","  1.49414062e-01 -2.14843750e-01 -1.83868408e-03  9.91210938e-02\n","  1.57226562e-01 -1.14257812e-01 -2.05078125e-01  9.91210938e-02\n","  3.69140625e-01 -1.97265625e-01  3.54003906e-02  1.09375000e-01\n","  1.31835938e-01  1.66992188e-01  2.35351562e-01  1.04980469e-01\n"," -4.96093750e-01 -1.64062500e-01 -1.56250000e-01 -5.22460938e-02\n","  1.03027344e-01  2.43164062e-01 -1.88476562e-01  5.07812500e-02\n"," -9.37500000e-02 -6.68945312e-02  2.27050781e-02  7.61718750e-02\n","  2.89062500e-01  3.10546875e-01 -5.37109375e-02  2.28515625e-01\n","  2.51464844e-02  6.78710938e-02 -1.21093750e-01 -2.15820312e-01\n"," -2.73437500e-01 -3.07617188e-02 -3.37890625e-01  1.53320312e-01\n","  2.33398438e-01 -2.08007812e-01  3.73046875e-01  8.20312500e-02\n","  2.51953125e-01 -7.61718750e-02 -4.66308594e-02 -2.23388672e-02\n","  2.99072266e-02 -5.93261719e-02 -4.66918945e-03 -2.44140625e-01\n"," -2.09960938e-01 -2.87109375e-01 -4.54101562e-02 -1.77734375e-01\n"," -2.79296875e-01 -8.59375000e-02  9.13085938e-02  2.51953125e-01]\n"]}]},{"cell_type":"markdown","source":["4. **말뭉치에 등장하지 않는 단어의 임베딩 벡터를 확인해봅시다.**\n","\n","**cameroon** 이라는 단어는 구글 뉴스 말뭉치에 등장하지 않는 단어(Unknown token)입니다.<br/>\n","이 단어를 위와 같이 임베딩 벡터화 해보겠습니다.<br/>\n","아래 결과에서는 **`KeyError`** 가 발생합니다.<br/>\n","이처럼 **`Word2Vec`**은 말뭉치에 등장하지 않는 단어는 벡터화 할 수 없다는 단점이 있습니다."],"metadata":{"id":"3-g7ZvrD_nOR"}},{"cell_type":"code","source":["unk = 'cameroon'\n","\n","try:\n","    vec_unk = wv[unk]\n","except KeyError:\n","    print(f\"The word #{unk} does not appear in this model\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LuzCnrgD_ls-","executionInfo":{"status":"ok","timestamp":1679785569272,"user_tz":-540,"elapsed":7,"user":{"displayName":"박지수","userId":"15151265950850399854"}},"outputId":"e27f5acb-18cb-405c-d662-0e8d37769aae"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["The word #cameroon does not appear in this model\n"]}]},{"cell_type":"markdown","source":["5. **단어 간 유사도를 파악해봅시다.**\n","\n","**`gensim`** 패키지가 제공하는 **`.similarity`** 를 활용하면 단어 간 유사도를 파악할 수 있습니다.<br/>\n","원-핫 인코딩과 다르게 임베딩 벡터는 단어 간 유사도가 0이 아닌 값으로 나오게 됩니다.<br/>\n","아래는 **'car'** 와 몇몇 단어의 유사도를 비교한 결과입니다."],"metadata":{"id":"k0RIG2qJ_oo6"}},{"cell_type":"code","source":["pairs = [\n","    ('car', 'minivan'),   \n","    ('car', 'bicycle'),  \n","    ('car', 'airplane'),\n","    ('car', 'cereal'),    \n","    ('car', 'democracy')\n","]\n","\n","for w1, w2 in pairs:\n","    print(f'{w1} ======= {w2}\\t  {wv.similarity(w1, w2):.2f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_ZmkwWCd_oAs","executionInfo":{"status":"ok","timestamp":1679785569272,"user_tz":-540,"elapsed":5,"user":{"displayName":"박지수","userId":"15151265950850399854"}},"outputId":"1ef9acd9-629e-4d9d-f739-5d97ff942b85"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["car ======= minivan\t  0.69\n","car ======= bicycle\t  0.54\n","car ======= airplane\t  0.42\n","car ======= cereal\t  0.14\n","car ======= democracy\t  0.08\n"]}]},{"cell_type":"markdown","source":["`.most_similar` 메서드를 사용하여\n","\n","`'car'`벡터에 `'minivan'` 벡터를 더한 벡터와 가장 유사한 5개의 단어를 뽑아보겠습니다."],"metadata":{"id":"riCVFMaE_qg2"}},{"cell_type":"code","source":["for i, (word, similarity) in enumerate(wv.most_similar(positive=['car', 'minivan'], topn=5)):\n","    print(f\"Top {i+1} : {word}, {similarity}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a551p7xw_pht","executionInfo":{"status":"ok","timestamp":1679785574701,"user_tz":-540,"elapsed":5432,"user":{"displayName":"박지수","userId":"15151265950850399854"}},"outputId":"2e70944c-5963-4421-ff76-6f75a97d1da0"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Top 1 : SUV, 0.8532192707061768\n","Top 2 : vehicle, 0.8175783753395081\n","Top 3 : pickup_truck, 0.7763688564300537\n","Top 4 : Jeep, 0.7567334175109863\n","Top 5 : Ford_Explorer, 0.7565720081329346\n"]}]},{"cell_type":"markdown","source":["시각화에서 확인한 것처럼 `king` 벡터에 `women` 벡터를 더한 뒤 `men` 벡터를 빼주면 `queen` 이 나오는 것과<br/>\n","`walking` 벡터에 `swam` 벡터를 더한 뒤 `walked` 벡터를 빼주면 `swimming` 이 나오는 것을 확인할 수 있습니다."],"metadata":{"id":"ag4LmTLK_r5S"}},{"cell_type":"code","source":["print(wv.most_similar(positive=['king', 'women'], negative=['men'], topn=1))\n","print(wv.most_similar(positive=['walking', 'swam'], negative=['walked'], topn=1))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WiJmrVks_rQF","executionInfo":{"status":"ok","timestamp":1679785575516,"user_tz":-540,"elapsed":817,"user":{"displayName":"박지수","userId":"15151265950850399854"}},"outputId":"63b05af6-811f-4da4-97dc-8a91abbad1db"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["[('queen', 0.6525818109512329)]\n","[('swimming', 0.7448815703392029)]\n"]}]},{"cell_type":"markdown","source":["`.doesnt_match` 메서드를 사용하여\n","\n","['fire', 'water', 'land', 'sea', 'air', 'car'] 중에서 가장 관계 없는 단어를 뽑아봅시다."],"metadata":{"id":"qZAM6oHv_t0v"}},{"cell_type":"code","source":["print(wv.doesnt_match(['fire', 'water', 'land', 'sea', 'air', 'car']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FHja2bwn_tNd","executionInfo":{"status":"ok","timestamp":1679785575516,"user_tz":-540,"elapsed":5,"user":{"displayName":"박지수","userId":"15151265950850399854"}},"outputId":"1d215eed-bd0f-4f39-bf35-11e059e13838"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["car\n"]}]},{"cell_type":"markdown","source":["## 3. fastText\n","\n","**`fastText`** 는 **`Word2Vec`** 방식에 철자(Character) 기반의 임베딩 방식을 더해준 새로운 임베딩 방식입니다. <br/>\n","**fastText** 가 고안된 이유는 무엇일까요?\n","\n"],"metadata":{"id":"HzO2_hkW_vay"}},{"cell_type":"markdown","source":["### 1) OOV(Out of Vocabulary) 문제\n","\n","데이터 수집에 제법 노력을 쏟더라도 세상 모든 단어가 들어있는 말뭉치를 구하는 것은 불가능합니다.<br/>\n","같은 어근을 지닌 단어라도 \"go, went, gone, goes...\" 등 수많은 변형이 있습니다.<br/>\n","게다가 이전에는 쓰지 않았던 신조어가 등장하기도 합니다.\n","\n","하지만 **`Word2Vec`** 은 말뭉치에 등장하지 않은 단어에 대해서는 임베딩 벡터를 만들지 못한다는 단점이 있습니다.<br/>\n","위에서 살펴본 것처럼 말뭉치에 등장하지 않은 단어인 Cameroon의 임베딩 벡터를 출력하려다 보니 에러가 발생했습니다.<br/>\n","이렇게 기존 말뭉치에 등장하지 않는 단어가 등장하는 문제를 **<font color=\"ff6f61\">OOV(Out of Vocabulary) 문제</font>**라고 합니다.<br/>\n","Word2Vec은 등장하지 않는 단어에 대해서는 학습하지 않기 때문에 예측(혹은 추론) 단계에서<br/>\n","Cameroom과 같은 새로운 단어가 등장하면 에러를 발생시킵니다.\n","\n","또한 적게 등장하는 단어(Rare words)에 대해서는 학습이 적게 일어나기 때문에 적절한 임베딩 벡터를 생성해내지 못한다는 것도 Word2Vec 의 단점입니다."],"metadata":{"id":"gkvp1PxY_wwO"}},{"cell_type":"markdown","source":["### 2) 철자 단위 임베딩(Character level Embedding)\n","\n","- **철자 단위 임베딩이란?**\n","\n","**`fastText`** 는 철자(Character) 수준의 임베딩을 보조 정보로 사용함으로써 OOV 문제를  해결해냈습니다.\n","\n","예시를 생각해 보겠습니다.<br/>\n","\"맞벌이\"라는 단어를 모른다고 하더라도 아래 단어를 알면 대략적인 의미를 유추해 볼 수 있습니다.\n","\n","> 1. _\"맞선, 맞절, 맞대다, 맞들다, 맞바꾸다, 맞서다, 맞잡다, 맞장구치다\"_ <br/>\n","> 2. _\"벌다, 벌어, 벌고\"_<br/>\n","> 3. _\"먹이, 깊이, 넓이\"_\n","\n","첫 번째 줄 단어를 통해서 **\"맞-\"**이라는 접두사의 의미를 유추해 보고,<br/>\n","두 번째 줄 단어를 통해서 **\"-벌-\"**이라는 어근의 의미를 유추해 보고,<br/>\n","세 번째 줄 단어를 통해서 **\"-이\"**라는 접미사의 의미를 유추해 볼 수 있습니다.<br/>\n","세 가지를 잘 조합하면 \"맞벌이\" 라는 단어의 뜻을 알 수 있습니다.\n","\n","**`fastText`** 가 철자 단위 임베딩을 사용하고자 하는 이유도 동일합니다.<br/>\n","모델이 학습하지 못한 단어더라도 잘 쪼개고 보면 말뭉치에서 등장했던 단어를 통해 유추해 볼 수 있다는 아이디어에서 출발하였습니다."],"metadata":{"id":"2ish1jCH_ykG"}},{"cell_type":"markdown","source":["- **fastText가 Character-level(철자 단위) 임베딩을 적용하는 법 : <font color=\"ff6f61\">Character n-gram</font>**\n","\n","**`fastText`** 는 3-6개로 묶은 Character 정보(3-6 grams) 단위를 사용합니다.<br/>\n","3-6개 단위로 묶기 이전에 모델이 접두사와 접미사를 인식할 수 있도록 해당 단어 앞뒤로 \"<\", \">\" 를 붙여줍니다.<br/>\n","그리고 나서 해당 단어를 3-6개 Character-level로 잘라서 임베딩을 적용합니다.\n","\n","> 만약 eating 이라는 단어에 Character-level 임베딩을 적용한다면 3-gram은 다음과 같이 될 것입니다."],"metadata":{"id":"mYGTWMG3_1lz"}},{"cell_type":"markdown","source":["<img src=\"https://amitness.com/images/fasttext-3-gram-sliding.gif\" width=\"300\" />\n","\n","이와 같은 방식을 3개 부터 6개까지 진행한 뒤 임베딩 벡터를 생성하고 원래 eating 의 임베딩 벡터와 함께 사용합니다.\n","\n","| word   | Length(n) | Character n-grams            |\n","|--------|-----------|------------------------------|\n","| eating | 3         | <ea, eat, ati, tin, ing, ng> |\n","| eating | 4         | <eat, eati, atin, ting, ing> |\n","| eating | 5         | <eati, eatin, ating, ting>   |\n","| eating | 6         | <eatin, eating, ating>       |"],"metadata":{"id":"kj0tou8T_3Xw"}},{"cell_type":"markdown","source":["총 18개의 Character-level n-gram을 얻을 수 있습니다.<br/>\n","fastText에서는 이렇게 얻어진 n-gram 들의 임베딩 벡터를 모두 구하게 됩니다.\n","\n","꽤 많은 경우의 수를 다루기 때문에 \"Word2Vec에 비해 엄청나게 많은 시간이 걸리는 것 아닌가?\"라고 생각해볼 수도 있지만,<br/>\n","fastText의 알고리즘이 매우 효율적으로 구성되어 있기 때문에 시간상으로 엄청난 차이가 나지는 않습니다.\n","\n","<br>\n","\n","- **철자 단위 임베딩 적용하기**\n","\n","**eating** 이라는 단어가 말뭉치 내에 있다면 skip-gram으로부터 학습한 임베딩 벡터에 위에서 얻은 18개 Character-level n-gram 들의 벡터를 더해줍니다.<br/>\n","반대로, **eating** 이라는 단어가 말뭉치에 없다면 18개 Character-level n-gram 들의 벡터만으로 구성합니다. \n"],"metadata":{"id":"E6jzbRBy_6K6"}},{"cell_type":"markdown","source":["### 3) 철자 단위 임베딩 시각화\n","\n","fastText의 철자 단위 임베딩이 어떤 관계를 맺고 있는지에 대해 이미지를 통해 알아보겠습니다.\n","\n","아래에 있는 그림은 X,Y축에 있는 단어 내 character n-gram 에 대하여 서로의 연관관계를 나타낸 그래프입니다.<br/>\n","빨간색을 나타낼 수록 두 단어 부분 사이에 유사한 관계가 있음을 나타냅니다.\n","\n","\n","![fasttext1](https://i.imgur.com/nltvwmg.png)"],"metadata":{"id":"L1c5UXwnAAFQ"}},{"cell_type":"markdown","source":["위 그래프에서는 **\"ity>\"** 와 **\"ness>\"** 가 상당히 유사한 관계를 보이는 것을 확인할 수 있습니다.<br/>\n","실제로 둘은 모두 명사를 나타내기 위한 접미사입니다.<br/>\n","fastText를 이러한 단어의 문법적 구조를 잘 나타낸다는 특징을 가지고 있습니다."],"metadata":{"id":"YkSnBNvpACwe"}},{"cell_type":"markdown","source":["![fasttext2](https://i.imgur.com/YJCm6yP.png)"],"metadata":{"id":"c61oW_SwADjh"}},{"cell_type":"markdown","source":["위 그래프에서는 **\"link\"** 와 **\"nnect, onnect, connec\"** 등이 상당히 유사한 관계에 있음을 확인할 수 있습니다.<br/>\n","3-6개 까지의 연속된 character를 다루고 있기 때문에 connect라는 단어 자체가 포함되지는 않았지만<br/>\n","connect와 link가 가지고 있는 \"연결하다\"라는 의미를 n-gram 임베딩 벡터도 유사하게 가지고 있음을 확인할 수 있습니다.\n","\n","예를 들어, `\"connectivity\"` 와 `\"linkage\"`라는 단어의 유사도를 구한다고 해보겠습니다.<br/>\n","**`Word2Vec`** 은 두 단어 중 하나라도 말뭉치 내에 없다면 에러를 발생시키지만,<br/>\n","**`fastText`** 는 꽤 높은 정확도로 두 단어의 임베딩 벡터를 구하고 유사도를 나타낼 수 있다는 장점이 있습니다.\n","\n","이제 코드를 통해 Word2Vec과 fastText의 차이점을 알아보도록 하겠습니다."],"metadata":{"id":"08liY7OBAErL"}},{"cell_type":"markdown","source":["### 4) gensim 패키지로 fastText 실습하기\n","\n","**`gensim`** 를 사용하면 **`fastText`** 도 유추해 볼 수 있습니다.<br/>\n","gensim을 사용하여 fastText의 결과가 어떻게 도출되는지 알아보겠습니다."],"metadata":{"id":"iEULbAooAHNR"}},{"cell_type":"code","source":["from pprint import pprint as print\n","from gensim.models.fasttext import FastText\n","from gensim.test.utils import datapath\n","\n","# Set file names for train and test data\n","corpus_file = datapath('lee_background.cor')\n","\n","model = FastText(vector_size=100)\n","\n","# build the vocabulary\n","model.build_vocab(corpus_file=corpus_file)\n","\n","# train the model\n","model.train(\n","    corpus_file=corpus_file, epochs=model.epochs,\n","    total_examples=model.corpus_count, total_words=model.corpus_total_words,\n",")\n","\n","print(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"azAPWaDj_uoK","executionInfo":{"status":"ok","timestamp":1679785579604,"user_tz":-540,"elapsed":4091,"user":{"displayName":"박지수","userId":"15151265950850399854"}},"outputId":"2aa3176e-d9c4-4673-c908-395614d56fbc"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["<gensim.models.fasttext.FastText object at 0x7f60c647e6d0>\n"]}]},{"cell_type":"markdown","source":["1. **'night' 라는 단어와 'nights'라는 단어가 각각 사전에 있는 지를 확인**해봅시다."],"metadata":{"id":"t1DufuIWAJi5"}},{"cell_type":"code","source":["ft = model.wv\n","print(ft)\n","\n","#\n","# FastText models support vector lookups for out-of-vocabulary words by summing up character ngrams belonging to the word.\n","#\n","print(f\"night => {'night' in ft.key_to_index}\")\n","print(f\"nights => {'nights' in ft.key_to_index}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZROn5rSfAIuQ","executionInfo":{"status":"ok","timestamp":1679785579604,"user_tz":-540,"elapsed":18,"user":{"displayName":"박지수","userId":"15151265950850399854"}},"outputId":"1fa3274b-d4a3-45e4-a631-c37380473631"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["<gensim.models.fasttext.FastTextKeyedVectors object at 0x7f60c647e700>\n","'night => True'\n","'nights => False'\n"]}]},{"cell_type":"markdown","source":["'night' 는 말뭉치에 있지만 'nights'는 말뭉치에 없음을 확인할 수 있습니다.\n","\n","2. **'night'와 'nights'의 임베딩 벡터**를 확인해봅시다."],"metadata":{"id":"2dgNNrQAALli"}},{"cell_type":"code","source":["print(ft['night'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xht1l-HuAK3A","executionInfo":{"status":"ok","timestamp":1679785579604,"user_tz":-540,"elapsed":16,"user":{"displayName":"박지수","userId":"15151265950850399854"}},"outputId":"541e918c-9ea5-49dc-eacc-428cb29224bb"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["array([-1.13794833e-01, -8.92864726e-03, -2.39144430e-01, -6.44885376e-02,\n","        5.87789156e-02,  2.88324058e-01,  4.54784095e-01,  6.34073436e-01,\n","        1.92545593e-01, -3.70680243e-01,  1.08520664e-01, -1.18878156e-01,\n","       -3.32196087e-01,  6.34262860e-01, -2.80676544e-01, -4.72724617e-01,\n","        1.24485366e-01, -1.76719204e-01, -4.10400361e-01, -5.47061145e-01,\n","       -4.07823205e-01,  1.32181555e-01, -6.38784289e-01, -6.64589554e-02,\n","       -2.07975462e-01, -3.20153207e-01, -5.47242701e-01, -7.48493150e-02,\n","       -1.34971142e-01,  9.72354859e-02, -2.55248338e-01,  2.59407014e-01,\n","        8.45553815e-01, -1.56474754e-01,  1.96449712e-01,  2.61272460e-01,\n","        5.10459542e-01, -4.11559083e-02, -3.52287740e-01, -2.71420896e-01,\n","        5.77617526e-01, -4.53099191e-01,  1.53351977e-01, -2.48228595e-01,\n","       -5.53458333e-01, -4.31939036e-01,  8.48252326e-02,  2.32955173e-01,\n","        1.45638674e-01, -6.88780397e-02,  3.21851879e-01, -6.06477499e-01,\n","        2.85179704e-01, -4.11382645e-01, -2.98927963e-01, -2.90719062e-01,\n","       -2.57429957e-01, -8.16196427e-02,  7.66606024e-03, -3.43235344e-01,\n","       -2.80863255e-01, -3.70588899e-01, -3.67634475e-01,  4.31924522e-01,\n","       -5.96914515e-02,  7.53540754e-01,  7.54895210e-02,  3.78477387e-02,\n","        3.56686324e-01,  3.97468537e-01, -2.89750755e-01,  4.43656504e-01,\n","        6.03290975e-01, -5.99096537e-01,  2.86413848e-01,  4.41889241e-02,\n","        2.12787837e-01, -4.56965454e-02,  4.10452252e-03,  2.52076089e-01,\n","        1.40437752e-01, -3.77424210e-01, -6.96392894e-01, -8.95462781e-02,\n","       -1.43285990e-01, -6.37077749e-01,  4.76597220e-01,  2.88130403e-01,\n","        2.67959200e-04, -3.16680700e-01, -7.32371509e-02,  4.54594165e-01,\n","       -2.71693528e-01,  1.48573101e-01, -2.80764639e-01,  5.33989549e-01,\n","       -1.19956590e-01, -2.00914517e-01,  4.92536910e-02, -2.40159005e-01],\n","      dtype=float32)\n"]}]},{"cell_type":"code","source":["print(ft['nights'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y2rbn4PgANyl","executionInfo":{"status":"ok","timestamp":1679785579605,"user_tz":-540,"elapsed":16,"user":{"displayName":"박지수","userId":"15151265950850399854"}},"outputId":"d1af16a6-1c84-476b-a04d-2238efdbddeb"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["array([-0.09963008, -0.00681915, -0.20790829, -0.05587766,  0.04980196,\n","        0.24934843,  0.39655024,  0.55294186,  0.1678485 , -0.3237871 ,\n","        0.09601454, -0.10184667, -0.2898054 ,  0.5491548 , -0.2452727 ,\n","       -0.41146037,  0.10745221, -0.15308401, -0.35531676, -0.47612816,\n","       -0.35156205,  0.11374456, -0.5550227 , -0.05921311, -0.17941168,\n","       -0.2768169 , -0.47429192, -0.06281845, -0.11708865,  0.08606482,\n","       -0.21987465,  0.22493564,  0.7330991 , -0.1355852 ,  0.17081112,\n","        0.22651151,  0.44501257, -0.03575965, -0.30637693, -0.23645507,\n","        0.5010865 , -0.3928168 ,  0.1327444 , -0.215228  , -0.481772  ,\n","       -0.37413475,  0.07663217,  0.20305501,  0.12795682, -0.05877033,\n","        0.28125882, -0.5272557 ,  0.24849473, -0.3574021 , -0.25931025,\n","       -0.25135508, -0.22564183, -0.06918258,  0.00797227, -0.29560056,\n","       -0.24301611, -0.3223733 , -0.31909278,  0.37482557, -0.05127306,\n","        0.6558297 ,  0.06571201,  0.0302433 ,  0.30969715,  0.34650788,\n","       -0.2522982 ,  0.38360196,  0.52536803, -0.5204371 ,  0.2503703 ,\n","        0.03944843,  0.18428653, -0.04052896,  0.00332366,  0.21910249,\n","        0.12253366, -0.3283504 , -0.6045202 , -0.07887454, -0.12347916,\n","       -0.55474555,  0.4140862 ,  0.25053465,  0.00226099, -0.27581832,\n","       -0.06357499,  0.39376217, -0.23659444,  0.12977354, -0.2443737 ,\n","        0.4641224 , -0.10558776, -0.1709649 ,  0.04313251, -0.20950967],\n","      dtype=float32)\n"]}]},{"cell_type":"markdown","source":["3. **두 단어의 유사도**를 확인해봅시다."],"metadata":{"id":"VtuAqwEWAQRK"}},{"cell_type":"code","source":["print(ft.similarity(\"night\", \"nights\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-U1aujG1APbS","executionInfo":{"status":"ok","timestamp":1679785579605,"user_tz":-540,"elapsed":15,"user":{"displayName":"박지수","userId":"15151265950850399854"}},"outputId":"bf3034e9-6814-4a9f-a6a1-164a0b631d2b"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["0.9999919\n"]}]},{"cell_type":"markdown","source":["4. 사전에 없는 단어인 **`'nights'` 와 가장 비슷한 단어**는 어떤 것이 있는지 알아봅시다."],"metadata":{"id":"hGD4xAqVAShD"}},{"cell_type":"code","source":["print(ft.most_similar(\"nights\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GpLOTN8UARXq","executionInfo":{"status":"ok","timestamp":1679785579605,"user_tz":-540,"elapsed":13,"user":{"displayName":"박지수","userId":"15151265950850399854"}},"outputId":"cff68b2e-0380-42e4-eb2b-06b257dbcd0e"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["[('night', 0.999991774559021),\n"," ('rights', 0.9999879598617554),\n"," ('flights', 0.9999876618385315),\n"," ('overnight', 0.999987006187439),\n"," ('fighting', 0.9999858140945435),\n"," ('fighters', 0.99998539686203),\n"," ('fight', 0.9999851584434509),\n"," ('fighter', 0.9999847412109375),\n"," ('eight', 0.9999844431877136),\n"," ('treated', 0.9999842643737793)]\n"]}]},{"cell_type":"markdown","source":["주로 비슷하게 '생긴', 즉 비슷한 character n-gram이 포함된 단어가 많이 속해있는 것을 볼 수 있습니다.\n","\n","5. **`Word2Vec`** 에서 했던 것과 같이 가장 관련 없는 단어를 뽑아봅시다."],"metadata":{"id":"wZ5nw1a0AUN_"}},{"cell_type":"code","source":["print(ft.doesnt_match(\"night noon fight morning\".split()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vQUqSMNmATqQ","executionInfo":{"status":"ok","timestamp":1679785579605,"user_tz":-540,"elapsed":12,"user":{"displayName":"박지수","userId":"15151265950850399854"}},"outputId":"1d0d1db4-fb6b-4865-dcc1-df946dd23f4f"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["'noon'\n"]}]},{"cell_type":"markdown","source":["단어의 뜻만 살펴보면 fight이 나와야 할 것 같지만, 뜬금없게도 noon이 등장했습니다.\n","\n","위 결과들에서 확인할 수 있는 것처럼,<br/>\n","`fastText` 임베딩 벡터는 단어의 의미보다는 결과 쪽에 조금 더 비중을 두고 있음을 확인할 수 있습니다."],"metadata":{"id":"OnxzXAfTAYAV"}},{"cell_type":"markdown","source":["## 4. 임베딩 벡터를 사용하여 문장 분류 수행하기\n","\n","임베딩 벡터를 사용하여 문장 분류를 수행해 봅시다.\n","\n","문장 분류를 사용하는 방법 중 가장 간단한 것은 문장에 있는 **단어 벡터를 모두 더한 뒤에 평균내어 구하는 방법**입니다.<br/>\n","이게 되나 싶을 정도로 간단하지만, 간단한 문제에 대해서는 꽤 좋은 성능을 보여서 baseline 모델로 많이 사용됩니다."],"metadata":{"id":"h4GjclMlAZSk"}},{"cell_type":"markdown","source":["- **`keras`** 및 **`Word2Vec`**을 사용하여 단어 평균으로 문서 분류하기\n","\n","1. 필요한 모듈을 `import` 해줍니다."],"metadata":{"id":"oeFmHsZ9Abch"}},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.datasets import imdb"],"metadata":{"id":"oxSJnDQmAWth","executionInfo":{"status":"ok","timestamp":1679785601062,"user_tz":-540,"elapsed":5055,"user":{"displayName":"박지수","userId":"15151265950850399854"}}},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":["2. Seed를 정해줍니다."],"metadata":{"id":"Y1QID5jrAeam"}},{"cell_type":"code","source":["tf.random.set_seed(42)"],"metadata":{"id":"qXHeVaDaAdgf","executionInfo":{"status":"ok","timestamp":1679785601062,"user_tz":-540,"elapsed":11,"user":{"displayName":"박지수","userId":"15151265950850399854"}}},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":["3. 데이터셋을 split 해줍니다."],"metadata":{"id":"n2OlcD01Af2H"}},{"cell_type":"code","source":["(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=20000)"],"metadata":{"id":"md0FGPgMAfO3","executionInfo":{"status":"ok","timestamp":1679785898052,"user_tz":-540,"elapsed":4353,"user":{"displayName":"박지수","userId":"15151265950850399854"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["print(f\"Train set shape : {X_train.shape}\")\n","print(f\"Test set shape : {X_test.shape}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tjXxPIAAAgg_","executionInfo":{"status":"ok","timestamp":1679785898053,"user_tz":-540,"elapsed":13,"user":{"displayName":"박지수","userId":"15151265950850399854"}},"outputId":"7287a4ca-7e05-4110-fa0a-450555fea3f5"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["'Train set shape : (25000,)'\n","'Test set shape : (25000,)'\n"]}]},{"cell_type":"markdown","source":["4. 데이터셋이 어떻게 생겼는지 눈으로 확인해봅니다."],"metadata":{"id":"R6B-fmzDAhw_"}},{"cell_type":"code","source":["X_train[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2AkibJ0cAhPB","executionInfo":{"status":"ok","timestamp":1679785898053,"user_tz":-540,"elapsed":12,"user":{"displayName":"박지수","userId":"15151265950850399854"}},"outputId":"c5fedd8f-84e2-4303-f866-c18f62bee9ac"},"execution_count":41,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[1,\n"," 14,\n"," 22,\n"," 16,\n"," 43,\n"," 530,\n"," 973,\n"," 1622,\n"," 1385,\n"," 65,\n"," 458,\n"," 4468,\n"," 66,\n"," 3941,\n"," 4,\n"," 173,\n"," 36,\n"," 256,\n"," 5,\n"," 25,\n"," 100,\n"," 43,\n"," 838,\n"," 112,\n"," 50,\n"," 670,\n"," 2,\n"," 9,\n"," 35,\n"," 480,\n"," 284,\n"," 5,\n"," 150,\n"," 4,\n"," 172,\n"," 112,\n"," 167,\n"," 2,\n"," 336,\n"," 385,\n"," 39,\n"," 4,\n"," 172,\n"," 4536,\n"," 1111,\n"," 17,\n"," 546,\n"," 38,\n"," 13,\n"," 447,\n"," 4,\n"," 192,\n"," 50,\n"," 16,\n"," 6,\n"," 147,\n"," 2025,\n"," 19,\n"," 14,\n"," 22,\n"," 4,\n"," 1920,\n"," 4613,\n"," 469,\n"," 4,\n"," 22,\n"," 71,\n"," 87,\n"," 12,\n"," 16,\n"," 43,\n"," 530,\n"," 38,\n"," 76,\n"," 15,\n"," 13,\n"," 1247,\n"," 4,\n"," 22,\n"," 17,\n"," 515,\n"," 17,\n"," 12,\n"," 16,\n"," 626,\n"," 18,\n"," 19193,\n"," 5,\n"," 62,\n"," 386,\n"," 12,\n"," 8,\n"," 316,\n"," 8,\n"," 106,\n"," 5,\n"," 4,\n"," 2223,\n"," 5244,\n"," 16,\n"," 480,\n"," 66,\n"," 3785,\n"," 33,\n"," 4,\n"," 130,\n"," 12,\n"," 16,\n"," 38,\n"," 619,\n"," 5,\n"," 25,\n"," 124,\n"," 51,\n"," 36,\n"," 135,\n"," 48,\n"," 25,\n"," 1415,\n"," 33,\n"," 6,\n"," 22,\n"," 12,\n"," 215,\n"," 28,\n"," 77,\n"," 52,\n"," 5,\n"," 14,\n"," 407,\n"," 16,\n"," 82,\n"," 10311,\n"," 8,\n"," 4,\n"," 107,\n"," 117,\n"," 5952,\n"," 15,\n"," 256,\n"," 4,\n"," 2,\n"," 7,\n"," 3766,\n"," 5,\n"," 723,\n"," 36,\n"," 71,\n"," 43,\n"," 530,\n"," 476,\n"," 26,\n"," 400,\n"," 317,\n"," 46,\n"," 7,\n"," 4,\n"," 12118,\n"," 1029,\n"," 13,\n"," 104,\n"," 88,\n"," 4,\n"," 381,\n"," 15,\n"," 297,\n"," 98,\n"," 32,\n"," 2071,\n"," 56,\n"," 26,\n"," 141,\n"," 6,\n"," 194,\n"," 7486,\n"," 18,\n"," 4,\n"," 226,\n"," 22,\n"," 21,\n"," 134,\n"," 476,\n"," 26,\n"," 480,\n"," 5,\n"," 144,\n"," 30,\n"," 5535,\n"," 18,\n"," 51,\n"," 36,\n"," 28,\n"," 224,\n"," 92,\n"," 25,\n"," 104,\n"," 4,\n"," 226,\n"," 65,\n"," 16,\n"," 38,\n"," 1334,\n"," 88,\n"," 12,\n"," 16,\n"," 283,\n"," 5,\n"," 16,\n"," 4472,\n"," 113,\n"," 103,\n"," 32,\n"," 15,\n"," 16,\n"," 5345,\n"," 19,\n"," 178,\n"," 32]"]},"metadata":{},"execution_count":41}]},{"cell_type":"markdown","source":["5. **인덱스로 된 데이터를 텍스트로 변경하는 함수를 구현합니다.**<br/>\n","첫 번째 데이터를 텍스트로 변경하고 확인해봅시다."],"metadata":{"id":"-L9YJFcFAjTv"}},{"cell_type":"code","source":["word_index = imdb.get_word_index()\n","reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n","\n","def decode_review(text):\n","    \"\"\"\n","    word_index를 받아 text를 sequence 형태로 반환하는 함수입니다.\n","    \"\"\"\n","    return ' '.join([reverse_word_index.get(i, '?') for i in text])"],"metadata":{"id":"VweglxMrAifQ","executionInfo":{"status":"ok","timestamp":1679785898053,"user_tz":-540,"elapsed":10,"user":{"displayName":"박지수","userId":"15151265950850399854"}}},"execution_count":42,"outputs":[]},{"cell_type":"code","source":["decode_review(X_train[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":146},"id":"o1lPQjzwAkBi","executionInfo":{"status":"ok","timestamp":1679785898053,"user_tz":-540,"elapsed":10,"user":{"displayName":"박지수","userId":"15151265950850399854"}},"outputId":"e7b0d778-7c37-46d6-854d-b4359cae2c6d"},"execution_count":43,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"the as you with out themselves powerful lets loves their becomes reaching had journalist of lot from anyone to have after out atmosphere never more room and it so heart shows to years of every never going and help moments or of every chest visual movie except her was several of enough more with is now current film as you of mine potentially unfortunately of you than him that with out themselves her get for was camp of you movie sometimes movie that with scary but pratfalls to story wonderful that in seeing in character to of 70s musicians with heart had shadows they of here that with her serious to have does when from why what have critics they is you that isn't one will very to as itself with other tricky in of seen over landed for anyone of and br show's to whether from than out themselves history he name half some br of 'n odd was two most of mean for 1 any an boat she he should is thought frog but of script you not while history he heart to real at barrel but when from one bit then have two of script their with her nobody most that with wasn't to with armed acting watch an for with heartfelt film want an\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":43}]},{"cell_type":"markdown","source":["6. **`keras` 의 `tokenizer` 에 텍스트를 학습**시킵니다."],"metadata":{"id":"ql7UQXb5Alkm"}},{"cell_type":"code","source":["sentences = [decode_review(idx) for idx in X_train]\n","\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(sentences)"],"metadata":{"id":"5TXmHfYXAkkm","executionInfo":{"status":"ok","timestamp":1679785907009,"user_tz":-540,"elapsed":4944,"user":{"displayName":"박지수","userId":"15151265950850399854"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["vocab_size = len(tokenizer.word_index) + 1\n","print(vocab_size)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RnYTt87qAmV5","executionInfo":{"status":"ok","timestamp":1679785907010,"user_tz":-540,"elapsed":10,"user":{"displayName":"박지수","userId":"15151265950850399854"}},"outputId":"eb4815b4-7fae-4670-a1cd-a42ad5b97ea7"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["19999\n"]}]},{"cell_type":"markdown","source":["7. **`pad_sequence` 를 통해 패딩 처리해줍니다.**<br/>\n","`pad_squence`가 무엇이며 이를 해주는 이유에 대해서는 구글링을 통해 스스로 알아보도록 합니다.<br/>\n","`maxlen` 을 평균보다 조금 더 긴 400 으로 설정합니다."],"metadata":{"id":"lfd52U8qAnhZ"}},{"cell_type":"code","source":["X_encoded = tokenizer.texts_to_sequences(sentences)\n","\n","#\n","max_len = max(len(sent) for sent in X_encoded)\n","print(max_len)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xJxg7SjoAm5l","executionInfo":{"status":"ok","timestamp":1679785910631,"user_tz":-540,"elapsed":3630,"user":{"displayName":"박지수","userId":"15151265950850399854"}},"outputId":"29e3669a-660e-4fb4-d7e5-6e2f7ce867a8"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["2494\n"]}]},{"cell_type":"code","source":["print(f'Mean length of train set: {np.mean([len(sent) for sent in X_train], dtype=int)}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S641reiDAoPf","executionInfo":{"status":"ok","timestamp":1679785910632,"user_tz":-540,"elapsed":7,"user":{"displayName":"박지수","userId":"15151265950850399854"}},"outputId":"1c498479-287a-4c34-f99f-b2844d35b70f"},"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["'Mean length of train set: 238'\n"]}]},{"cell_type":"code","source":["X_train=pad_sequences(X_encoded, maxlen=max_len, padding='post')\n","y_train=np.array(y_train)"],"metadata":{"id":"tCGpi-TqApFn","executionInfo":{"status":"ok","timestamp":1679786140126,"user_tz":-540,"elapsed":1,"user":{"displayName":"박지수","userId":"15151265950850399854"}}},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":["8. **`word2vec`의 임베딩 가중치 행렬을 만들어줍니다.**<br/>\n","미리 학습된 모든 단어(300만개)에 대해 만들 경우 너무 행렬이 커지므로<br/>\n","vocab에 속하는 단어에 대해서만 만들어지도록 합니다. "],"metadata":{"id":"LMlbH5uPAqug"}},{"cell_type":"code","source":["embedding_matrix = np.zeros((vocab_size, 300))\n","\n","print(np.shape(embedding_matrix))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RHBTarWQAqFN","executionInfo":{"status":"ok","timestamp":1679786141349,"user_tz":-540,"elapsed":3,"user":{"displayName":"박지수","userId":"15151265950850399854"}},"outputId":"9cb380dd-9cc0-4cb0-c295-f1c8a5a4bcd4"},"execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["(19999, 300)\n"]}]},{"cell_type":"code","source":["def get_vector(word):\n","    \"\"\"\n","    해당 word가 word2vec에 있는 단어일 경우 임베딩 벡터를 반환\n","    \"\"\"\n","    if word in wv:\n","        return wv[word]\n","    else:\n","        return None\n"," \n","for word, i in tokenizer.word_index.items():\n","    temp = get_vector(word)\n","    if temp is not None:\n","        embedding_matrix[i] = temp"],"metadata":{"id":"6HSbUV3gAriL","executionInfo":{"status":"ok","timestamp":1679786141863,"user_tz":-540,"elapsed":4,"user":{"displayName":"박지수","userId":"15151265950850399854"}}},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":["9. 신경망을 구성하기 위한 keras 모듈을 불러온 후<br/>\n","학습을 수행해줍니다."],"metadata":{"id":"MqBhBvK-As6O"}},{"cell_type":"code","source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Embedding, Flatten"],"metadata":{"id":"MG1oCo5WAsHw","executionInfo":{"status":"ok","timestamp":1679786141863,"user_tz":-540,"elapsed":4,"user":{"displayName":"박지수","userId":"15151265950850399854"}}},"execution_count":55,"outputs":[]},{"cell_type":"code","source":["model = Sequential()\n","model.add(Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_len, trainable=False))\n","model.add(GlobalAveragePooling1D()) # 입력되는 단어 벡터의 평균을 구하는 함수입니다.\n","model.add(Dense(1, activation='sigmoid'))"],"metadata":{"id":"hfye7feTAvM0","executionInfo":{"status":"ok","timestamp":1679786141863,"user_tz":-540,"elapsed":4,"user":{"displayName":"박지수","userId":"15151265950850399854"}}},"execution_count":56,"outputs":[]},{"cell_type":"code","source":["model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n","model.fit(X_train, y_train, batch_size=64, epochs=20, validation_split=0.2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"axio5-z9AwJz","executionInfo":{"status":"ok","timestamp":1679786182973,"user_tz":-540,"elapsed":41113,"user":{"displayName":"박지수","userId":"15151265950850399854"}},"outputId":"37a592f8-3295-498b-a6ca-ac60537e50af"},"execution_count":57,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","313/313 [==============================] - 6s 4ms/step - loss: 0.6915 - acc: 0.5341 - val_loss: 0.6887 - val_acc: 0.5902\n","Epoch 2/20\n","313/313 [==============================] - 1s 3ms/step - loss: 0.6878 - acc: 0.5807 - val_loss: 0.6848 - val_acc: 0.6042\n","Epoch 3/20\n","313/313 [==============================] - 1s 3ms/step - loss: 0.6845 - acc: 0.5986 - val_loss: 0.6814 - val_acc: 0.6068\n","Epoch 4/20\n","313/313 [==============================] - 1s 3ms/step - loss: 0.6816 - acc: 0.6046 - val_loss: 0.6785 - val_acc: 0.5942\n","Epoch 5/20\n","313/313 [==============================] - 1s 4ms/step - loss: 0.6790 - acc: 0.6070 - val_loss: 0.6761 - val_acc: 0.6174\n","Epoch 6/20\n","313/313 [==============================] - 1s 5ms/step - loss: 0.6764 - acc: 0.6117 - val_loss: 0.6734 - val_acc: 0.6020\n","Epoch 7/20\n","313/313 [==============================] - 1s 3ms/step - loss: 0.6742 - acc: 0.6139 - val_loss: 0.6711 - val_acc: 0.6198\n","Epoch 8/20\n","313/313 [==============================] - 1s 3ms/step - loss: 0.6720 - acc: 0.6137 - val_loss: 0.6695 - val_acc: 0.6220\n","Epoch 9/20\n","313/313 [==============================] - 1s 3ms/step - loss: 0.6700 - acc: 0.6187 - val_loss: 0.6670 - val_acc: 0.6256\n","Epoch 10/20\n","313/313 [==============================] - 1s 5ms/step - loss: 0.6682 - acc: 0.6230 - val_loss: 0.6651 - val_acc: 0.6292\n","Epoch 11/20\n","313/313 [==============================] - 1s 3ms/step - loss: 0.6663 - acc: 0.6229 - val_loss: 0.6634 - val_acc: 0.6312\n","Epoch 12/20\n","313/313 [==============================] - 1s 4ms/step - loss: 0.6646 - acc: 0.6253 - val_loss: 0.6618 - val_acc: 0.6322\n","Epoch 13/20\n","313/313 [==============================] - 1s 3ms/step - loss: 0.6629 - acc: 0.6258 - val_loss: 0.6609 - val_acc: 0.6290\n","Epoch 14/20\n","313/313 [==============================] - 1s 3ms/step - loss: 0.6613 - acc: 0.6290 - val_loss: 0.6585 - val_acc: 0.6336\n","Epoch 15/20\n","313/313 [==============================] - 1s 3ms/step - loss: 0.6597 - acc: 0.6300 - val_loss: 0.6571 - val_acc: 0.6382\n","Epoch 16/20\n","313/313 [==============================] - 1s 4ms/step - loss: 0.6584 - acc: 0.6337 - val_loss: 0.6560 - val_acc: 0.6384\n","Epoch 17/20\n","313/313 [==============================] - 1s 4ms/step - loss: 0.6569 - acc: 0.6329 - val_loss: 0.6557 - val_acc: 0.6348\n","Epoch 18/20\n","313/313 [==============================] - 1s 3ms/step - loss: 0.6556 - acc: 0.6344 - val_loss: 0.6532 - val_acc: 0.6406\n","Epoch 19/20\n","313/313 [==============================] - 1s 3ms/step - loss: 0.6543 - acc: 0.6358 - val_loss: 0.6521 - val_acc: 0.6426\n","Epoch 20/20\n","313/313 [==============================] - 1s 3ms/step - loss: 0.6530 - acc: 0.6381 - val_loss: 0.6514 - val_acc: 0.6398\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f601b59d220>"]},"metadata":{},"execution_count":57}]},{"cell_type":"code","source":["test_sentences = [decode_review(idx) for idx in X_test]\n","\n","X_test_encoded = tokenizer.texts_to_sequences(test_sentences)\n","\n","X_test=pad_sequences(X_test_encoded, maxlen=400, padding='post')\n","y_test=np.array(y_test)"],"metadata":{"id":"cwkJ-KhbAxfy","executionInfo":{"status":"ok","timestamp":1679786188620,"user_tz":-540,"elapsed":5650,"user":{"displayName":"박지수","userId":"15151265950850399854"}}},"execution_count":58,"outputs":[]},{"cell_type":"code","source":["model.evaluate(X_test, y_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PGMcv_tKAyQr","executionInfo":{"status":"ok","timestamp":1679786193446,"user_tz":-540,"elapsed":4834,"user":{"displayName":"박지수","userId":"15151265950850399854"}},"outputId":"7aaedf33-a552-4514-996f-f4531ac126c7"},"execution_count":59,"outputs":[{"output_type":"stream","name":"stdout","text":["782/782 [==============================] - 3s 4ms/step - loss: 0.6732 - acc: 0.5701\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.673154354095459, 0.5701199769973755]"]},"metadata":{},"execution_count":59}]},{"cell_type":"markdown","source":["# 복습\n","\n","- 단어의 분산 표현(Distributed Representation)\n","    - 원-핫 인코딩(One-hot Encoding)\n","    - 임베딩(Embedding)\n","\n","- Word2Vec\n","    - CBoW와 Skip-gram\n","    - Word2Vec의 구조\n","    - Word2Vec의 임베딩 벡터를 시각화한 결과\n","\n","- fastText\n","    - OOV(Out of Vocabulary) 문제\n","    - 철자(Character) 단위 임베딩"],"metadata":{"id":"tRz8ygPtAuRy"}},{"cell_type":"markdown","source":["References\n","\n","- [n-gram](https://www.youtube.com/watch?v=4f9XC8HHluE) 이란 무엇일까요?\n","- [Bag of Tricks for Efficient Text Classification](https://arxiv.org/abs/1607.01759)"],"metadata":{"id":"WNJB_7BoA57T"}},{"cell_type":"markdown","source":["# Exercise\n","\n","## 캐글의 [SMS Spam dataset](https://www.kaggle.com/uciml/sms-spam-collection-dataset) 에 사전 학습된 Word2Vec 임베딩 벡터를 적용하여 분류해봅시다.<br/>\n","세션 노트에 있었던 단어 임베딩 벡터를 평균내어 분류하는 방법을 적용해봅시다."],"metadata":{"id":"Yxs9ikwBBFe2"}},{"cell_type":"code","source":["import gensim\n","\n","gensim.__version__"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"hl7U5hxmAtuN","executionInfo":{"status":"ok","timestamp":1679786193446,"user_tz":-540,"elapsed":9,"user":{"displayName":"박지수","userId":"15151265950850399854"}},"outputId":"e6356b2a-c91b-46ea-feac-6b7f1cd7860b"},"execution_count":60,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'4.3.1'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":60}]},{"cell_type":"markdown","source":["## 1. Word2Vec과 코사인 유사도\n","\n","word2vec을 이용해 구한 'data'와 'science'임베딩 값의 코사인 유사도를 구해보겠습니다. sklearn의 cosine_similarity를 이용하겠습니다."],"metadata":{"id":"roYs9is2BOJ4"}},{"cell_type":"code","source":["from sklearn.metrics.pairwise import cosine_similarity\n","\n","cosine_similarity((wv['data'], wv['science']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Aj0R0xZEBK6C","executionInfo":{"status":"ok","timestamp":1679786194533,"user_tz":-540,"elapsed":1095,"user":{"displayName":"박지수","userId":"15151265950850399854"}},"outputId":"379d5517-b1a9-4704-c940-827de6bf47be"},"execution_count":61,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.9999999 , 0.15759146],\n","       [0.15759146, 1.        ]], dtype=float32)"]},"metadata":{},"execution_count":61}]},{"cell_type":"markdown","source":["문항 1) 위에서 구한 코사인 유사도를 소수점 3째자리까지 입력하세요"],"metadata":{"id":"Ggi4RjukBQom"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","\n","from keras.preprocessing import sequence\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n","from tensorflow.keras.preprocessing.text import Tokenizer"],"metadata":{"id":"txWnzUKxBP1Q","executionInfo":{"status":"ok","timestamp":1679786194533,"user_tz":-540,"elapsed":8,"user":{"displayName":"박지수","userId":"15151265950850399854"}}},"execution_count":62,"outputs":[]},{"cell_type":"markdown","source":["## 2. 텍스트 분류\n","\n","### 1) 데이터 전처리\n","    \n","- 데이터셋을 데이터프레임으로 읽어옵니다 `encoding = 'latin-1'` 을 사용합니다.\n","- 필요없는 열(column)을 삭제합니다.\n","- LabelEncoder를 사용하여 label 전처리를 해줍니다. "],"metadata":{"id":"qndjjdWwBTQg"}},{"cell_type":"code","source":["from google.colab import files\n","\n","file = files.upload()\n","df = pd.read_csv('spam.csv', encoding='latin-1')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":77},"id":"bl7Zs7vKBRwH","executionInfo":{"status":"ok","timestamp":1679786251040,"user_tz":-540,"elapsed":56515,"user":{"displayName":"박지수","userId":"15151265950850399854"}},"outputId":"aff58615-d43a-461b-b1f7-6b94fca0da45"},"execution_count":63,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-c7b5670b-fc17-408c-a726-d3e8be853113\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-c7b5670b-fc17-408c-a726-d3e8be853113\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving spam.csv to spam.csv\n"]}]},{"cell_type":"code","source":["df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"-3us2MOfBZOi","executionInfo":{"status":"ok","timestamp":1679786251040,"user_tz":-540,"elapsed":21,"user":{"displayName":"박지수","userId":"15151265950850399854"}},"outputId":"6f5564d5-9aaf-4fc9-f5b4-71bc3f622f81"},"execution_count":64,"outputs":[{"output_type":"execute_result","data":{"text/plain":["        v1                                                 v2 Unnamed: 2  \\\n","0      ham  Go until jurong point, crazy.. Available only ...        NaN   \n","1      ham                      Ok lar... Joking wif u oni...        NaN   \n","2     spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n","3      ham  U dun say so early hor... U c already then say...        NaN   \n","4      ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n","...    ...                                                ...        ...   \n","5567  spam  This is the 2nd time we have tried 2 contact u...        NaN   \n","5568   ham              Will Ì_ b going to esplanade fr home?        NaN   \n","5569   ham  Pity, * was in mood for that. So...any other s...        NaN   \n","5570   ham  The guy did some bitching but I acted like i'd...        NaN   \n","5571   ham                         Rofl. Its true to its name        NaN   \n","\n","     Unnamed: 3 Unnamed: 4  \n","0           NaN        NaN  \n","1           NaN        NaN  \n","2           NaN        NaN  \n","3           NaN        NaN  \n","4           NaN        NaN  \n","...         ...        ...  \n","5567        NaN        NaN  \n","5568        NaN        NaN  \n","5569        NaN        NaN  \n","5570        NaN        NaN  \n","5571        NaN        NaN  \n","\n","[5572 rows x 5 columns]"],"text/html":["\n","  <div id=\"df-c17d4b0d-8a57-475e-bf66-892fc5973888\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>v1</th>\n","      <th>v2</th>\n","      <th>Unnamed: 2</th>\n","      <th>Unnamed: 3</th>\n","      <th>Unnamed: 4</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>ham</td>\n","      <td>Go until jurong point, crazy.. Available only ...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>ham</td>\n","      <td>Ok lar... Joking wif u oni...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>spam</td>\n","      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>ham</td>\n","      <td>U dun say so early hor... U c already then say...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>ham</td>\n","      <td>Nah I don't think he goes to usf, he lives aro...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>5567</th>\n","      <td>spam</td>\n","      <td>This is the 2nd time we have tried 2 contact u...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>5568</th>\n","      <td>ham</td>\n","      <td>Will Ì_ b going to esplanade fr home?</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>5569</th>\n","      <td>ham</td>\n","      <td>Pity, * was in mood for that. So...any other s...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>5570</th>\n","      <td>ham</td>\n","      <td>The guy did some bitching but I acted like i'd...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>5571</th>\n","      <td>ham</td>\n","      <td>Rofl. Its true to its name</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5572 rows × 5 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c17d4b0d-8a57-475e-bf66-892fc5973888')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-c17d4b0d-8a57-475e-bf66-892fc5973888 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-c17d4b0d-8a57-475e-bf66-892fc5973888');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":64}]},{"cell_type":"code","source":["df = df[['v1', 'v2']]"],"metadata":{"id":"HCMrvvPtBaIW","executionInfo":{"status":"ok","timestamp":1679786251040,"user_tz":-540,"elapsed":19,"user":{"displayName":"박지수","userId":"15151265950850399854"}}},"execution_count":65,"outputs":[]},{"cell_type":"code","source":["df['v1'].value_counts()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0IlV4ZbkBa1Q","executionInfo":{"status":"ok","timestamp":1679786251041,"user_tz":-540,"elapsed":20,"user":{"displayName":"박지수","userId":"15151265950850399854"}},"outputId":"8044df7d-cec3-40df-f48d-ebfd74dbfe03"},"execution_count":66,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ham     4825\n","spam     747\n","Name: v1, dtype: int64"]},"metadata":{},"execution_count":66}]},{"cell_type":"code","source":["enc = LabelEncoder()\n","df['v1'] = enc.fit_transform(df['v1'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CKnRQS0uBbgA","executionInfo":{"status":"ok","timestamp":1679786251041,"user_tz":-540,"elapsed":18,"user":{"displayName":"박지수","userId":"15151265950850399854"}},"outputId":"4e359832-58c7-4707-cfb1-8c01b7247b8e"},"execution_count":67,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-67-ff39a1a970bc>:2: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df['v1'] = enc.fit_transform(df['v1'])\n"]}]},{"cell_type":"markdown","source":["### 2) 텍스트 분류를 수행해주세요.\n","\n","- 데이터셋 split시 test_size의 비율은 15%로, `random_state = 42` 로 설정합니다. \n","- Tokenizer의 `num_words = 1000` 으로 설정합니다.\n","- pad_sequence의 `maxlen=150` 으로 설정합니다.\n","- 학습 시, 파라미터는 `batch_size=64, epochs=10, validation_split=0.2` 로 설정합니다.\n","- evaluate 했을 때의 loss와 accuarcy를 [loss, acc] 형태로 입력해주세요. Ex) [0.4321, 0.8765]"],"metadata":{"id":"ne9JXm2zBdCy"}},{"cell_type":"code","source":["np.random.seed(42)\n","tf.random.set_seed(42)"],"metadata":{"id":"h2KwXNxHBcDd","executionInfo":{"status":"ok","timestamp":1679786251041,"user_tz":-540,"elapsed":16,"user":{"displayName":"박지수","userId":"15151265950850399854"}}},"execution_count":68,"outputs":[]},{"cell_type":"code","source":["sentences = df['v2']\n","label = df['v1']\n","\n","X_train, X_test, y_train, y_test = train_test_split(sentences, label, test_size = 0.15, random_state =42)\n","\n","tokenizer = Tokenizer(num_words = 1000)\n","tokenizer.fit_on_texts(X_train)\n","\n","vocab_size = len(tokenizer.word_index) + 1\n","print(vocab_size)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vv6T1T5LDp14","executionInfo":{"status":"ok","timestamp":1679786818685,"user_tz":-540,"elapsed":1064,"user":{"displayName":"박지수","userId":"15151265950850399854"}},"outputId":"b7e49713-9dc9-4916-b9f1-612e0bb71a5a"},"execution_count":76,"outputs":[{"output_type":"stream","name":"stdout","text":["8210\n"]}]},{"cell_type":"code","source":["X_train.shape, X_test.shape, y_train.shape, y_test.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-PItOryGDtyT","executionInfo":{"status":"ok","timestamp":1679786819131,"user_tz":-540,"elapsed":2,"user":{"displayName":"박지수","userId":"15151265950850399854"}},"outputId":"dacc2f4a-175e-4b6c-f30a-f1f580926631"},"execution_count":77,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((4736,), (836,), (4736,), (836,))"]},"metadata":{},"execution_count":77}]},{"cell_type":"code","source":["max_len=150\n","\n","\n","X_encoded = tokenizer.texts_to_sequences(X_train)\n","\n","X_train_enc=pad_sequences(X_encoded, maxlen=max_len, padding='post')\n","y_train=np.array(label)"],"metadata":{"id":"6vKfSgDbDuoU","executionInfo":{"status":"ok","timestamp":1679786820048,"user_tz":-540,"elapsed":1,"user":{"displayName":"박지수","userId":"15151265950850399854"}}},"execution_count":78,"outputs":[]},{"cell_type":"code","source":["embedding_matrix = np.zeros((vocab_size, 300))\n","\n","print(np.shape(embedding_matrix))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8DYU0SCJDvb6","executionInfo":{"status":"ok","timestamp":1679786820625,"user_tz":-540,"elapsed":3,"user":{"displayName":"박지수","userId":"15151265950850399854"}},"outputId":"7c3b65c8-3fa7-4a2e-d0a2-0b4f8cd4e7f4"},"execution_count":79,"outputs":[{"output_type":"stream","name":"stdout","text":["(8210, 300)\n"]}]},{"cell_type":"code","source":["def get_vector(word):\n","    \"\"\"\n","    해당 word가 word2vec에 있는 단어일 경우 임베딩 벡터를 반환\n","    \"\"\"\n","    if word in wv:\n","        return wv[word]\n","    else:\n","        return None\n"," \n","for word, i in tokenizer.word_index.items():\n","    temp = get_vector(word)\n","    if temp is not None:\n","        embedding_matrix[i] = temp"],"metadata":{"id":"KXYrld7eDv-O","executionInfo":{"status":"ok","timestamp":1679786826579,"user_tz":-540,"elapsed":6,"user":{"displayName":"박지수","userId":"15151265950850399854"}}},"execution_count":80,"outputs":[]},{"cell_type":"code","source":["model = Sequential()\n","model.add(Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_len, trainable=False))\n","model.add(GlobalAveragePooling1D())\n","model.add(Dense(1, activation='sigmoid'))"],"metadata":{"id":"Rc4FB9kwDwtK","executionInfo":{"status":"ok","timestamp":1679786826579,"user_tz":-540,"elapsed":5,"user":{"displayName":"박지수","userId":"15151265950850399854"}}},"execution_count":81,"outputs":[]},{"cell_type":"code","source":["model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n","model.fit(X_train_enc, y_train, batch_size=64, epochs=10, validation_split=0.2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gfCZ6-3nDxki","executionInfo":{"status":"ok","timestamp":1679786830962,"user_tz":-540,"elapsed":4388,"user":{"displayName":"박지수","userId":"15151265950850399854"}},"outputId":"928d129c-df11-4c25-c70a-539869c4a3ba"},"execution_count":82,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","60/60 [==============================] - 1s 10ms/step - loss: 0.6721 - acc: 0.8609 - val_loss: 0.6515 - val_acc: 0.8597\n","Epoch 2/10\n","60/60 [==============================] - 0s 6ms/step - loss: 0.6346 - acc: 0.8669 - val_loss: 0.6164 - val_acc: 0.8597\n","Epoch 3/10\n","60/60 [==============================] - 0s 6ms/step - loss: 0.6023 - acc: 0.8669 - val_loss: 0.5863 - val_acc: 0.8597\n","Epoch 4/10\n","60/60 [==============================] - 0s 5ms/step - loss: 0.5745 - acc: 0.8669 - val_loss: 0.5602 - val_acc: 0.8597\n","Epoch 5/10\n","60/60 [==============================] - 0s 4ms/step - loss: 0.5506 - acc: 0.8669 - val_loss: 0.5379 - val_acc: 0.8597\n","Epoch 6/10\n","60/60 [==============================] - 0s 4ms/step - loss: 0.5303 - acc: 0.8669 - val_loss: 0.5192 - val_acc: 0.8597\n","Epoch 7/10\n","60/60 [==============================] - 0s 4ms/step - loss: 0.5129 - acc: 0.8669 - val_loss: 0.5029 - val_acc: 0.8597\n","Epoch 8/10\n","60/60 [==============================] - 0s 4ms/step - loss: 0.4982 - acc: 0.8669 - val_loss: 0.4893 - val_acc: 0.8597\n","Epoch 9/10\n","60/60 [==============================] - 0s 4ms/step - loss: 0.4855 - acc: 0.8669 - val_loss: 0.4774 - val_acc: 0.8597\n","Epoch 10/10\n","60/60 [==============================] - 0s 4ms/step - loss: 0.4746 - acc: 0.8669 - val_loss: 0.4673 - val_acc: 0.8597\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f6023626820>"]},"metadata":{},"execution_count":82}]},{"cell_type":"code","source":["X_test_encoded = tokenizer.texts_to_sequences(X_test)\n","X_test_pad = pad_sequences(X_test_encoded, maxlen=150, padding='post')"],"metadata":{"id":"wGcZcto8DyJQ","executionInfo":{"status":"ok","timestamp":1679786830962,"user_tz":-540,"elapsed":10,"user":{"displayName":"박지수","userId":"15151265950850399854"}}},"execution_count":83,"outputs":[]},{"cell_type":"code","source":["model.evaluate(X_test_pad,y_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IHikUODzDy2S","executionInfo":{"status":"ok","timestamp":1679786832209,"user_tz":-540,"elapsed":1257,"user":{"displayName":"박지수","userId":"15151265950850399854"}},"outputId":"1d761730-e0a4-4c12-8ced-b6a547cf21b6"},"execution_count":84,"outputs":[{"output_type":"stream","name":"stdout","text":["27/27 [==============================] - 0s 4ms/step - loss: 0.4787 - acc: 0.8708\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.47874677181243896, 0.8708133697509766]"]},"metadata":{},"execution_count":84}]},{"cell_type":"markdown","source":["### 3)Word2Vec에서의 OOV 문제\n","\n","def get_vector(word):\n","    \"\"\"\n","    해당 word가 word2vec에 있는 단어일 경우 임베딩 벡터를 반환\n","    \"\"\"\n","    if word in wv:\n","        return wv[word]\n","    else:\n","        return None\n","\n","for word, i in tokenizer.word_index.items():  \n","    temp = get_vector(word)  \n","    if temp is not None:  \n","        embedding_matrix[i] = temp    \n","\n","Lecture Note에 있는 위의 코드를 변형하여, OOV 개수를 확인해주세요.\n","\n","- tokenizer는 위에서 활용한 tokenizer를 그대로 사용하겠습니다.\n","- Tip : dictionary를 활용하거나, Counter를 활용해보세요."],"metadata":{"id":"W_UGInQMD0VL"}},{"cell_type":"code","source":["oov = 0 \n","for word, i in tokenizer.word_index.items():\n","    temp = get_vector(word)\n","    if temp is None:\n","        oov += 1\n","\n","oov"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pYSy7GXADzej","executionInfo":{"status":"ok","timestamp":1679786844892,"user_tz":-540,"elapsed":455,"user":{"displayName":"박지수","userId":"15151265950850399854"}},"outputId":"1fec5d87-d8c5-47f5-edee-e52924eca517"},"execution_count":85,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2419"]},"metadata":{},"execution_count":85}]},{"cell_type":"code","source":[],"metadata":{"id":"Om39VsUUD2Jm","executionInfo":{"status":"ok","timestamp":1679786845365,"user_tz":-540,"elapsed":1,"user":{"displayName":"박지수","userId":"15151265950850399854"}}},"execution_count":85,"outputs":[]}]}