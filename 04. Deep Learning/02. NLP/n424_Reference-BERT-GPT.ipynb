{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMyNjwQwEpBxPlPQuNRF+Ls"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Transformer & BERT, GPT"],"metadata":{"id":"xQXLz1M2dnre"}},{"cell_type":"markdown","source":["## GPT, BERT"],"metadata":{"id":"6S14eFZ_dpRQ"}},{"cell_type":"markdown","source":["다음으로 GPT와 BERT에 대해서 알아보겠습니다.<br/>\n","\n","GPT와 BERT는 트랜스포머 구조를 변형하여 만들어진 언어 모델입니다.<br/>\n","두 모델은 **<font color=\"ff6f61\">사전 학습된 언어 모델(Pre-trained Language Model)</font>** 이라는 공통점을 가지고 있습니다.<br/>\n","사전 학습이란 대량의 데이터를 사용하여 미리 학습하는 과정입니다. 여기에 필요한 데이터를 추가 학습시켜 모델의 성능을 최적화합니다.<br/>\n","이런 학습 방법을 **전이 학습(Transfer Learning)**이라고도 합니다.<br/>\n","최근 발표되고 있는 언어 모델은 모두 전이 학습을 적용하고 있습니다."],"metadata":{"id":"O5OIS9LhdqoS"}},{"cell_type":"markdown","source":["### GPT (2018.6)"],"metadata":{"id":"6JLW9llgdr6L"}},{"cell_type":"markdown","source":["**GPT**는 **G**enerative **P**re-trained **T**ransformer 의 약자로 2018년 6월에 OpenAI를 통해 발표되었습니다.<br/>\n","연이어 발표한 GPT-2(2019.2), GPT-3(2020.6)가 좋은 성능을 보이면서 세간의 주목을 받았습니다.<br/>\n","\n","[discriminative vs generative](https://ratsgo.github.io/generative%20model/2017/12/17/compare/)<br/>\n","\n","> - **GPT2 기사**\n","    - [The AI that was too dangerous to release](https://blog.floydhub.com/gpt2/)\n","    - [OpenAI, 공유하기에는 너무 위험한 ‘텍스트 생성 AI’의 진실](http://www.aitimes.com/news/articleView.html?idxno=121589)<br/>\n","> - **GPT3 기사**\n","    - [A GPT-3 bot posted comments on Reddit for a week and no one noticed](https://www.technologyreview.com/2020/10/08/1009845/a-gpt-3-bot-posted-comments-on-reddit-for-a-week-and-no-one-noticed/)\n","    - [GPT3가 쓴 뉴스가 랭킹 1위, 사람을 이겼다](http://www.aitimes.com/news/articleView.html?idxno=131593)\n","\n"],"metadata":{"id":"Hqkhsz-Rdsn6"}},{"cell_type":"markdown","source":["GPT-1, GPT-2, GPT-3 의 구조가 전부 동일하지는 않지만 기본적인 뼈대는 동일합니다.\n","\n","GPT의 구조를 본격적으로 알아보기에 앞서 기본이 되는 아이디어인 **<font color=\"ff6f61\">사전 학습(Pre-training)</font>**에 대해서 알아보겠습니다."],"metadata":{"id":"qQOiIGwDdyNs"}},{"cell_type":"markdown","source":["- **사전 학습된 언어 모델 (Pre-trained LM)**\n","\n","<img width=\"700\" alt=\"Pre-training\" src=\"https://user-images.githubusercontent.com/45377884/112943247-35cc2980-916c-11eb-99be-2fa7657507d2.png\">"],"metadata":{"id":"B1wHMB9DdzH5"}},{"cell_type":"markdown","source":["사전 학습 언어 모델은 크게 2가지 과정을 통해 완성됩니다. 첫 번째가 **<font color=\"ff6f61\">사전 학습(Pre-training)</font>**입니다. \n","\n","존재하는 자연어 중에는 책(Book corpus)이나 위키피디아(Wiki corpus) 등 레이블링 되지 않은 데이터가 더 많습니다.<br/>\n","여러분이 지금 읽고 있는 강의 노트 역시 **레이블링 되지 않은 데이터** 입니다.<br/>\n","책을 많이 읽는 것처럼 레이블링 되지 않은 데이터를 모델이 학습하도록 하는 과정을 **사전 학습** 이라고 합니다.\n","\n","사전 학습이 끝난 모델에 우리가 하고자하는 태스크에 특화된(Task specific) 데이터를 학습합니다.<br/>\n","이를 **<font color=\"ff6f61\">Fine-tuning</font>** 이라고 합니다.<br/> Fine-tuning에서는 학습 시 **레이블링 된 데이터** [Ex) 감성 분석, 자연어 추론(NLI), 질의 응답(QA)] 를 사용합니다."],"metadata":{"id":"8oJhlI2cd4I3"}},{"cell_type":"markdown","source":["- **모델 구조**\n","\n","아래는 GPT의 모델 구조를 나타낸 그림입니다.<br/>\n","트랜스포머의 **<font color=\"ff6f61\">디코더(Decoder) 블록</font>**을 쌓아서 모델을 구성합니다.<br/>\n","(그림에서는 6개의 디코더 블록만을 나타냈만 GPT에서는 12개의 디코더 블록을 사용하였습니다.)"],"metadata":{"id":"KEh6x_x8d5M8"}},{"cell_type":"markdown","source":["<img width=\"700\" alt=\"Pre-training\" src=\"http://jalammar.github.io/images/xlnet/transformer-decoder-intro.png\">"],"metadata":{"id":"pXSALFoAd57f"}},{"cell_type":"markdown","source":["GPT에서는 인코더를 사용하지 않기 때문에 디코더 블록내에 **2개의 Sub-layer**만 있습니다.<br/>\n","트랜스포머의 디코더 블록에는 Masked Self-Attention, Encoder-Decoder Attention, Feed-Forward 층이 있었습니다.<br/>\n","하지만 GPT는 인코더를 사용하지 않기 때문에 Encoder-Decoder Attention 층이 빠지게 됩니다."],"metadata":{"id":"FzMHHMZOd6ty"}},{"cell_type":"markdown","source":["- **사전 학습(Pre-training)**\n","\n","레이블링 되지 않은 대량의 말뭉치 $U = (u_1, \\cdots , u_n)$ 에 대해 로그 우도 $L_1$ 을 최대화하는 방향으로 학습됩니다.<br/>\n","지난 노트에서 배웠던 언어 모델처럼 다음에 올 단어를 계속해서 맞추는 방식으로 학습합니다.\n","\n","$$\n","L_1(U) = \\sum_i \\log P(u_i \\vert u_{i-k}, \\cdots, u_{i-1}; \\Theta)\n","$$"],"metadata":{"id":"B2BHYVV0d7ZW"}},{"cell_type":"markdown","source":["- **Fine-tuning**\n","\n","기존 모델에서는 태스크에 맞춰 모델 구조를 변경해주고 학습을 진행시켰습니다.\n","\n","하지만 GPT와 같은 사전 학습 언어 모델은 Fine-tuning 과정에서 데이터의 입력 방식만을 변형시키고 모델 구조는 일정하도록 설계되었습니다."],"metadata":{"id":"VNc6NSypd8Wp"}},{"cell_type":"markdown","source":["<img width=\"600\" alt=\"fine-tune_structure\" src=\"https://user-images.githubusercontent.com/45377884/112949500-408abc80-9174-11eb-8090-4f0be34db572.png\">"],"metadata":{"id":"dQSy-dhdd9LV"}},{"cell_type":"markdown","source":["\n","Fine-tuning은 레이블링 된 말뭉치 $C = (x_1, \\cdots , x_m)$ 에 대하여 로그 우도 $L_2$ 를 최대화하는 방향으로 학습합니다.\n","\n","$$\n","L_2(C) = \\sum_{(x,y)} \\log P(y \\vert x_1, \\cdots , x_m)\n","$$\n","\n","GPT의 경우 Fine-tuning에서 학습하는 데이터셋이 클 때는 보조 목적함수로 $L_1$ 을 추가하여 $L_3$로 학습하면 학습이 더 잘 진행됩니다.\n","\n","$$\n","L_3(C) = L_2(C) + \\lambda \\cdot L_1(C)\n","$$"],"metadata":{"id":"ePXf486Cd-Hg"}},{"cell_type":"markdown","source":["- **결과 & 결론**\n","\n","LSTM, GRU를 사용한 기존 모델보다 자연어 추론(NLI), 질의 응답(QA), 분류(Classification) 등의 태스크에서 높은 성능을 달성하였습니다.<br/>GPT는 사전 학습된 언어 모델을 바탕으로 좋은 성능을 확보할 수 있다는 점과 사전 학습 모델에 Transformer 구조가 더 좋은 성능을 보임을 알 수 있었습니다."],"metadata":{"id":"4J51N-2Zd-_L"}},{"cell_type":"markdown","source":["### BERT (2018.10)"],"metadata":{"id":"YvO1egcSeAR_"}},{"cell_type":"markdown","source":["**<font color=\"ff6f61\">BERT</font>**(**B**idirectional **E**ncoder **R**epresentation by **T**ransformer)는 2018년 10월 구글에서 발표한 모델입니다.\n","\n","모델 이름에서 알 수 있듯 BERT는 트랜스포머의 인코더만을 사용하여 문맥을 양방향(Bidirectional)으로 읽어냅니다."],"metadata":{"id":"R2PsdhKBeHyt"}},{"cell_type":"markdown","source":["- **BERT의 구조**\n","\n","GPT가 트랜스포머의 디코더 블록을 12개 쌓아올린 모델이었다면 **BERT는 트랜스포머의 <font color=\"ff6f61\">인코더 블록</font>**을 12개 쌓아올린 모델입니다."],"metadata":{"id":"n10lSU8keI6M"}},{"cell_type":"markdown","source":["<img width=\"500\" alt=\"model_name\" src=\"http://jalammar.github.io/images/bert-base-bert-large-encoders.png\">"],"metadata":{"id":"aR7iF2mYeJkf"}},{"cell_type":"markdown","source":["- **BERT의 Special Token ([CLS], [SEP])과 입력 벡터를 알아봅시다**\n","\n","이번에는 BERT에만 있는 특별한 토큰과 이에 따라 입력되는 벡터가 어떻게 구성되는 지에 대해 알아보겠습니다."],"metadata":{"id":"jF8BvlUheKvO"}},{"cell_type":"markdown","source":["<img width=\"600\" alt=\"bert_input\" src=\"https://imgur.com/iW77E5Q.png\">"],"metadata":{"id":"s6G3ZDk8eLbA"}},{"cell_type":"markdown","source":["1. Special Token : **[CLS]**, **[SEP]**\n","\n"],"metadata":{"id":"h2bl4mHmeMLd"}},{"cell_type":"markdown","source":["BERT에는 **[CLS]**와 **[SEP]**이라는 두 가지 Special Token이 있습니다.<br/>\n","\n","> **[CLS]** : Classification <br/>\n","\n","[CLS] 토큰은 입력의 맨 앞에 위치하는 토큰입니다.<br/>\n","아래에서 등장할 NSP(Next Sentence Prediction)이라는 학습을 위해 존재합니다.\n","\n","> **[SEP]** : Separation <br/>\n","\n","BERT는 사전 학습 시에 텍스트를 2부분으로 나누어 넣게 됩니다.<br/>\n","**[SEP]** 토큰은 첫 번째 부분의 끝자리와 두 번째 부분의 끝자리에 위치합니다."],"metadata":{"id":"-axt7g3geNRm"}},{"cell_type":"markdown","source":["<img width=\"600\" alt=\"bert_input\" src=\"https://imgur.com/iW77E5Q.png\">"],"metadata":{"id":"_XCAgxR-eN_M"}},{"cell_type":"markdown","source":["2. Input Vector : Token Embeddings, Segment Embeddings, Position Embeddings"],"metadata":{"id":"e7fSL1mneOzW"}},{"cell_type":"markdown","source":["BERT는 3종류의 임베딩 벡터를 모두 더하여 모델에 입력합니다.\n","\n","- Token Embeddings : 단어를 나타내는 임베딩입니다. Word2Vec, GloVe, FastText 등으로 사전 학습된 임베딩 벡터를 사용합니다.<br/>\n","- Segment Embeddings : 첫 번째 부분과 두 번째 부분을 구분하기 위한 임베딩입니다.<br/>**`[CLS]`** 부터 첫 번째 **`[SEP]`** 까지 동일한 벡터를 적용하고, 다음 토큰부터 두 번째 **`[SEP]`** 까지 동일한 벡터를 적용합니다.\n","- Positional Embeddings : 단어의 위치를 나타내기 위한 임베딩입니다.\n"],"metadata":{"id":"jNowADjpePdi"}},{"cell_type":"markdown","source":["- **BERT의 사전 학습(Pre-training) 방법들**\n"],"metadata":{"id":"qX2iI-3PeQPe"}},{"cell_type":"markdown","source":["BERT 역시 Pre-trained Language Model이기 때문에 사전 학습 이후에 Fine-tuning을 진행하게 됩니다.<br/>\n","아래에서는 BERT의 사전 학습 방식에 대해 알아보겠습니다.<br/>\n","BERT는 GPT와 다른 방식의 사전 학습이 적용되었습니다.<br/>\n","BERT의 사전 학습에서 사용되는 **2가지 방법(MLM, NSP)**을 알아보겠습니다."],"metadata":{"id":"IC4h7Y6peQ2v"}},{"cell_type":"markdown","source":["> **<font color=\"ff6f61\">MLM(Masked Language Model)</font>**"],"metadata":{"id":"Xrkv_tbHeRux"}},{"cell_type":"markdown","source":["첫 번째는 MLM(Masked Language Model) 입니다. 혹시 아래와 같은 문제를 풀어보신 경험이 있으신가요?\n","\n","<img width=\"300\" alt=\"mlm\" src=\"https://thumb.mt.co.kr/06/2013/11/2013110718224659109_1.jpg/dims/optimize/\">"],"metadata":{"id":"550PySVNeSgX"}},{"cell_type":"markdown","source":["영어 시험을 한 번쯤 준비해보신 분이라면 빈칸 채우기 유형 문제를 풀어보신 적이 있을 것입니다.<br/>\n","위와 같은 문제에서 보통은 빈칸에 문법적/의미적으로 올바른 단어를 채우게 됩니다.\n","\n","BERT도 이처럼 **빈칸 채우기**를 하면서 단어를 학습합니다.<br/>\n","BERT는 사전 학습 과정에서 레이블링 되지 않은 말뭉치 중에서 랜덤으로 15\\%가량의 단어를 마스킹합니다.<br/>\n","그리고 마스킹된 위치에 원래 있던 단어를 예측하는 방식으로 학습을 진행합니다."],"metadata":{"id":"RV5QlqCreaFZ"}},{"cell_type":"markdown","source":["<img width=\"600\" alt=\"mlm_example\" src=\"http://jalammar.github.io/images/BERT-language-modeling-masked-lm.png\">"],"metadata":{"id":"rn6LaqPOeaxY"}},{"cell_type":"markdown","source":["MLM은 양쪽의 문맥을 동시에 볼 수 있다는 장점이 있습니다.\n","\n","아래 그림은 GPT와 BERT의 학습 방향을 비교하여 나타낸 그림입니다."],"metadata":{"id":"6ZvOxjRFecE4"}},{"cell_type":"markdown","source":["<img width=\"300\" alt=\"gpt_vs_bert\" src=\"https://user-images.githubusercontent.com/45377884/113259927-a445ee80-9308-11eb-8fbd-95d5f553480a.png\">"],"metadata":{"id":"q1XyROicedRI"}},{"cell_type":"markdown","source":["GPT는 ***'거기'*** 라는 단어를 예측할 때 '어제 카페 갔었어'의 정보만 볼 수 있습니다.<br/>\n","하지만 BERT는 빈칸에 들어갈 ***'거기'*** 라는 단어를 예측 할 때 '어제 카페 갔었어'뿐만 아니라 '사람 많더라'의 정보도 참고할 수 있습니다.<br/>\n","이렇게 양방향으로 학습할 경우 단어가 문맥 사이에서 가진 의미를 최대로 학습할 수 있습니다.\n","\n","MLM은 다소 간단한 아이디어이지만 단어의 문맥적 의미를 최대로 학습할 수 있도록 함으로써 BERT가 높은 성능을 달성하는 데에 커다란 역할을 하였습니다.<br/>다음으로 2번째 방법인 NSP에 대해서 알아보겠습니다."],"metadata":{"id":"XAQpdRjaefh3"}},{"cell_type":"markdown","source":["> **<font color=\"ff6f61\">NSP(Next Sentence Prediction)</font>**"],"metadata":{"id":"X9u4pnYoegj6"}},{"cell_type":"markdown","source":["BERT는 NSP(Next Sentence Prediction) 방식으로도 학습합니다. 동문서답이라는 사자성어 알고 계신가요?"],"metadata":{"id":"bFB1w58zeh2w"}},{"cell_type":"markdown","source":["NSP는 모델이 문맥에 맞는 이야기를 하는지 아니면 동문서답을 하는지를 판단하며 학습하는 방식입니다.<br/>"],"metadata":{"id":"3Yjmw3g5ejKu"}},{"cell_type":"markdown","source":["위에서 알아본 **`[SEP]`** 토큰 왼쪽의 문장과 오른쪽의 문장이 바로 이어지는 문장일 경우 **`[CLS]`** 토큰의 출력이 **`IsNext`** 로 되도록 학습합니다.<br/> \n","두 문장이 이어지지 않는 쌍일 경우에는 출력 벡터가 **`NotNext`** 로 나오도록 학습합니다."],"metadata":{"id":"dmdB-jYbej07"}},{"cell_type":"markdown","source":["<img width=\"500\" alt=\"nsp_1\" src=\"http://jalammar.github.io/images/bert-next-sentence-prediction.png\">"],"metadata":{"id":"KIWQmw3FekgL"}},{"cell_type":"markdown","source":["아래는 드라마 대본을 예시로 NSP가 어떻게 작동하는 지를 나타낸 그림입니다."],"metadata":{"id":"RZx2PNIcelVl"}},{"cell_type":"markdown","source":["<img width=\"500\" alt=\"nsp_2\" src=\"https://user-images.githubusercontent.com/45377884/86514846-d0067780-be4f-11ea-9809-c3e43b8ad3f9.png\">     \n","\n","<img width=\"500\" alt=\"nsp_3\" src=\"https://user-images.githubusercontent.com/45377884/86514847-d137a480-be4f-11ea-82be-d229bf75fbf8.png\">"],"metadata":{"id":"B_3lES61el5z"}},{"cell_type":"markdown","source":["NSP 역시 간단한 아이디어입니다.<br/>\n","모델이 문장과 문장 사이의 관계를 학습할 수 있도록 함으로써 질의응답(QA), 자연어 추론(NLI) 등,<br/>\n","문장 관계를 이해해야만 하는 복잡한 태스크에서 좋은 성능을 나타내는 역할을 하였습니다."],"metadata":{"id":"kXuSn1odemo6"}},{"cell_type":"markdown","source":["- **Fine-tuning**\n","\n","BERT 역시 모델의 구조는 그대로 유지한 채 데이터를 입력하는 형태만 바꾸어서 Fine-tuning을 실시합니다."],"metadata":{"id":"cSk6Xgwlenfg"}},{"cell_type":"markdown","source":["<img width=\"700\" alt=\"nsp_2\" src=\"http://jalammar.github.io/images/bert-tasks.png\">"],"metadata":{"id":"S3Pn8GmveoGe"}},{"cell_type":"markdown","source":["- (a)는 “Sentence” 쌍을 분류하는 태스크입니다. `[SEP]`으로 나눠진 “Sentence” 쌍을 입력받아 `[CLS]`가 출력하는 클래스를 반환합니다.\n","- (b)는 감성분석 등 하나의 문장을 입력하여 `[CLS]`로 해당 문장을 분류하는 태스크입니다.\n","- (c)는 질의 응답 태스크입니다. 질문과 본문에 해당하는 단락을 `[SEP]` 토큰으로 나누어 입력하면 질문에 대한 답을 출력하도록 합니다.\n","- (d)는 품사 태깅(POS tagging)이나 개체명 인식(Named Entity Recognition, NER) 등의 태스크입니다. 입력받은 각 토큰마다 답을 출력합니다."],"metadata":{"id":"qZ0O8cNxeo8S"}},{"cell_type":"markdown","source":["- **결과 & 결론**\n","\n","BERT는 간단한 사전 학습 아이디어로 많은 태스크에서 SOTA를 달성하였습니다.<br/>\n","단순한 아이디어를 통해 엄청난 성능을 달성하였기에 당시 많은 충격을 주었습니다.<br/>\n","이후로도 BERT를 개선하기 위한 연구가 많이 진행되었습니다.\n","\n","특히 MLM을 통해 BERT가 좋은 성능을 달성한 뒤로 텍스트에 노이즈를 준 후에 이를 다시 맞추는(Denoising) 방법에 대해 많은 연구가 진행되었습니다."],"metadata":{"id":"hrrEA2Ouepnm"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"JXwKyEQfdjeE"},"outputs":[],"source":[]}]}