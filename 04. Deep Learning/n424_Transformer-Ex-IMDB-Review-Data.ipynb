{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO/U7FfOKPzZZr0/XUOohKF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# Transformer_and_BERT_GPT"],"metadata":{"id":"1l-7O6A_ZC01"}},{"cell_type":"markdown","source":["# Transformer로 감정 분류(Classification task) 수행하기\n","\n","Transformer는 분명, seq2seq 형태의 task에서 강점을 보입니다. 하지만, Attention과 Transformer를 classificatoin task에 적용할 수 있습니다.\n","\n","Transformer의 Encoder 부분만을 활용하여 분류 태스크를 적용할 수 있습니다. 전체 모델 그림은 다음과 같습니다."],"metadata":{"id":"nu8-mqiJZEgv"}},{"cell_type":"markdown","source":["<img src=https://s3.ap-northeast-2.amazonaws.com/urclass-images/aoJFaVFMvRAsntP5k16yq-1640699853834.png>"],"metadata":{"id":"bPia8vv9ZFmu"}},{"cell_type":"markdown","source":["## Code"],"metadata":{"id":"XwZtcWYVZGgI"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"6hj7s-PtYss6","executionInfo":{"status":"ok","timestamp":1679808673786,"user_tz":-540,"elapsed":3239,"user":{"displayName":"박지수","userId":"15151265950850399854"}}},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np"]},{"cell_type":"code","source":["def create_padding_mask(seq):\n","    # 패딩 처리된 부분을 마스킹합니다.\n","    # 모델이 패딩을 입력으로 취급하지 않도록 \n","    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n","\n","    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n","\n","x = tf.constant([[7, 6, 5, 4, 0], [1, 2, 3, 0, 0], [1, 8, 0, 0, 0]])\n","create_padding_mask(x)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z8JQvT88ZHQS","executionInfo":{"status":"ok","timestamp":1679808677239,"user_tz":-540,"elapsed":3455,"user":{"displayName":"박지수","userId":"15151265950850399854"}},"outputId":"a31201a0-34b8-4c4b-bd6a-b158b6e352e7"},"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(3, 1, 1, 5), dtype=float32, numpy=\n","array([[[[0., 0., 0., 0., 1.]]],\n","\n","\n","       [[[0., 0., 0., 1., 1.]]],\n","\n","\n","       [[[0., 0., 1., 1., 1.]]]], dtype=float32)>"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["def get_angles(pos, i, d_model):\n","    \"\"\"\n","    sin, cos 안에 들어갈 수치를 구하는 함수입니다.\n","    \"\"\"\n","    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n","    return pos * angle_rates\n","\n","def positional_encoding(position, d_model):\n","    \"\"\"\n","    위치 인코딩(Positional Encoding)을 구하는 함수입니다.\n","    \"\"\"\n","    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n","                          np.arange(d_model)[np.newaxis, :],\n","                          d_model)\n","\n","    # apply sin to even indices in the array; 2i\n","    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n","\n","    # apply cos to odd indices in the array; 2i+1\n","    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n","\n","    pos_encoding = angle_rads[np.newaxis, ...]\n","\n","    return tf.cast(pos_encoding, dtype=tf.float32)"],"metadata":{"id":"KR7ohRg3Zgz6","executionInfo":{"status":"ok","timestamp":1679808677240,"user_tz":-540,"elapsed":3,"user":{"displayName":"박지수","userId":"15151265950850399854"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["### Dot Scaled Attention 구현"],"metadata":{"id":"tF07K9OLZjyF"}},{"cell_type":"code","source":["def scaled_dot_product_attention(q, k, v, mask):\n","    \n","    \"\"\"Calculate the attention weights.\n","    q, k, v must have matching leading dimensions.\n","    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n","    The mask has different shapes depending on its type(padding or look ahead) \n","    but it must be broadcastable for addition.\n","  \n","    Args:\n","        q: query shape == (..., seq_len_q, depth)\n","        k: key shape == (..., seq_len_k, depth)\n","        v: value shape == (..., seq_len_v, depth_v)\n","        mask: Float tensor with shape broadcastable \n","            to (..., seq_len_q, seq_len_k). Defaults to None.\n","    \n","    Returns:\n","        output, attention_weights\n","    \"\"\"\n","    \n","    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n","    \n","    # scale matmul_qk\n","    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n","    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n","\n","    \"\"\"\n","    mask가 있을 경우 masking된 자리(mask=1)에는 (-inf)에 해당하는 절댓값이 큰 음수 -1e9(=-10억)을 더해줍니다.\n","    그 값에 softmax를 취해주면 거의 0에 가까운 값이 나옵니다. 그 다음 value 계산시에 반영되지 않습니다.\n","    \"\"\"\n","    \n","    # add the mask to the scaled tensor.\n","    if mask is not None:\n","        scaled_attention_logits += (mask * -1e9)  \n","        \n","    # softmax is normalized on the last axis (seq_len_k) so that the scores\n","    # add up to 1.\n","    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n","    \n","    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n","    \n","    return output, attention_weights"],"metadata":{"id":"DUNvCwpGZkG8","executionInfo":{"status":"ok","timestamp":1679808684249,"user_tz":-540,"elapsed":883,"user":{"displayName":"박지수","userId":"15151265950850399854"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["### MultiHead Attention"],"metadata":{"id":"ONDD-AOLZmWO"}},{"cell_type":"markdown","source":["Lecture Note에서는 keras에서 제공하는 MultiHead Attention을 사용했습니다. 여기서는, Multi Head Attention을 직접 구현해보겠습니다."],"metadata":{"id":"hq01Vj6pZnLP"}},{"cell_type":"code","source":["def point_wise_feed_forward_network(**kargs):\n","    \"\"\"\n","    FFNN을 구현한 코드입니다.\n","\n","    Args:\n","        d_model : 모델의 차원입니다.\n","        dff : 은닉층의 차원 수입니다. 논문에서는 2048을 사용하였습니다.\n","    \"\"\"\n","    return tf.keras.Sequential([\n","      tf.keras.layers.Dense(kargs['dff'], activation='relu'),  # (batch_size, seq_len, dff)\n","      tf.keras.layers.Dense(kargs['d_model'])  # (batch_size, seq_len, d_model)\n","    ])"],"metadata":{"id":"JPfJfFUDZlf0","executionInfo":{"status":"ok","timestamp":1679808694151,"user_tz":-540,"elapsed":1442,"user":{"displayName":"박지수","userId":"15151265950850399854"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["class MultiHeadAttention(tf.keras.layers.Layer):\n","    def __init__(self, **kargs):\n","        super(MultiHeadAttention, self).__init__()\n","        self.num_heads = kargs['num_heads']\n","        self.d_model = kargs['d_model']\n","\n","        assert self.d_model % self.num_heads == 0\n","\n","        self.depth = self.d_model // self.num_heads\n","\n","        self.wq = tf.keras.layers.Dense(self.d_model)\n","        self.wk = tf.keras.layers.Dense(self.d_model)\n","        self.wv = tf.keras.layers.Dense(self.d_model)\n","\n","        self.dense = tf.keras.layers.Dense(kargs['d_model'])\n","\n","    def split_heads(self, x, batch_size):\n","        \"\"\"\n","        Split the last dimension into (num_heads, depth).\n","        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n","        \"\"\"\n","        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n","        return tf.transpose(x, perm=[0, 2, 1, 3])\n","\n","    def call(self, v, k, q, mask):\n","        batch_size = tf.shape(q)[0]\n","\n","        q = self.wq(q)  # (batch_size, seq_len, d_model)\n","        k = self.wk(k)  # (batch_size, seq_len, d_model)\n","        v = self.wv(v)  # (batch_size, seq_len, d_model)\n","\n","        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n","        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n","        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n","\n","        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n","        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n","        scaled_attention, attention_weights = scaled_dot_product_attention(\n","            q, k, v, mask)\n","\n","        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n","\n","        concat_attention = tf.reshape(scaled_attention, \n","                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n","\n","        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n","\n","        return output, attention_weights"],"metadata":{"id":"yy_3vy1mZn8p","executionInfo":{"status":"ok","timestamp":1679808696812,"user_tz":-540,"elapsed":1,"user":{"displayName":"박지수","userId":"15151265950850399854"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["### Encoder Layer"],"metadata":{"id":"2PAvAxIjZpll"}},{"cell_type":"code","source":["class EncoderLayer(tf.keras.layers.Layer):\n","    def __init__(self, **kargs):\n","        super(EncoderLayer, self).__init__()\n","\n","        self.mha = MultiHeadAttention(**kargs)\n","        self.ffn = point_wise_feed_forward_network(**kargs)\n","\n","        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","\n","        self.dropout1 = tf.keras.layers.Dropout(kargs['rate'])\n","        self.dropout2 = tf.keras.layers.Dropout(kargs['rate'])\n","\n","    def call(self, x, mask):\n","        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n","        attn_output = self.dropout1(attn_output)\n","        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n","\n","        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n","        ffn_output = self.dropout2(ffn_output)\n","        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n","\n","        return out2"],"metadata":{"id":"un1HDtSwZo0X","executionInfo":{"status":"ok","timestamp":1679808702250,"user_tz":-540,"elapsed":1,"user":{"displayName":"박지수","userId":"15151265950850399854"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["### Encoder"],"metadata":{"id":"QXAbv0njZq3z"}},{"cell_type":"code","source":["class Encoder(tf.keras.layers.Layer):\n","    def __init__(self, **kwargs):\n","        super(Encoder, self).__init__()\n","\n","        self.d_model = kwargs['d_model']\n","        self.num_layers = kwargs['num_layers']\n","\n","        self.embedding = tf.keras.layers.Embedding(kwargs['vocab_size'], self.d_model)\n","        self.pos_encoding = positional_encoding(kwargs['maximum_position_encoding'], \n","                                                self.d_model)\n","\n","\n","        self.enc_layers = [EncoderLayer(**kwargs) \n","                           for _ in range(self.num_layers)]\n","\n","        self.dropout = tf.keras.layers.Dropout(kwargs['rate'])\n","\n","    def call(self, x, mask):\n","\n","        seq_len = tf.shape(x)[1]\n","\n","        # adding embedding and position encoding.\n","        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n","        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","        x += self.pos_encoding[:, :seq_len, :]\n","\n","        x = self.dropout(x)\n","\n","        for i in range(self.num_layers):\n","            x = self.enc_layers[i](x, mask)\n","\n","        return x  # (batch_size, input_seq_len, d_model)"],"metadata":{"id":"nlDL1GuPZqJO","executionInfo":{"status":"ok","timestamp":1679808707749,"user_tz":-540,"elapsed":2,"user":{"displayName":"박지수","userId":"15151265950850399854"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["### Transformer"],"metadata":{"id":"7OnmyIq_ZsZg"}},{"cell_type":"markdown","source":["여기서는 가장 첫 번째 타임 스텝의 은닉 값을 가져와서 classification_layer에 통과시키겠습니다.\n","\n","(batch_size, 1, d_model) 크기의 텐서가 바로 이진 분류를 수행하는 Dense Layer로 들어가게끔 구현했습니다.\n","- 이 부분은 설계에 따라 변경될 수 있습니다. 다음과 같이 2개의 Dense Layer를 쌓아서 이진 분류를 수행할 수도 있습니다.\n","  - Dense(64, activation='relu')\n","  - Dense(1, activation='sigmoid')\n"],"metadata":{"id":"28EZ5akHZtG3"}},{"cell_type":"code","source":["class Transformer(tf.keras.Model):\n","    def __init__(self, **kargs):\n","        super(Transformer, self).__init__()\n","        \n","        self.encoder = Encoder(**kargs)\n","        self.classification_layer = tf.keras.layers.Dense(1, activation='sigmoid')\n","\n","    def call(self, inputs, training):\n","        # Keras models prefer if you pass all your inputs in the first argument\n","\n","        enc_padding_mask = self.create_masks(inputs)\n","\n","        enc_output = self.encoder(inputs, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n","\n","        final_output = self.classification_layer(enc_output[:, 0, :]) # (bs, 1, d_model)\n","\n","        return final_output\n","\n","    def create_masks(self, inputs):\n","        # Encoder padding mask\n","        enc_padding_mask = create_padding_mask(inputs)\n","\n","        return enc_padding_mask"],"metadata":{"id":"ARHyoPxxZrgN","executionInfo":{"status":"ok","timestamp":1679808717765,"user_tz":-540,"elapsed":1,"user":{"displayName":"박지수","userId":"15151265950850399854"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["### Hyper-Parameter 설정"],"metadata":{"id":"xaSCjdoAZukh"}},{"cell_type":"code","source":["BATCH_SIZE = 16\n","MAX_SEQUENCE = 80\n","EPOCHS = 3\n","VALID_SPLIT = 0.1\n","MAX_FEATURES = 20000\n","MAXLEN = 80\n","\n","kargs = {'num_layers': 2,\n","         'd_model': 512,\n","         'num_heads': 8,\n","         'dff': 2048,\n","         'vocab_size': MAX_FEATURES,\n","         'maximum_position_encoding': MAXLEN,\n","         'rate': 0.1\n","        }"],"metadata":{"id":"2Hwp5ZQkZt0u","executionInfo":{"status":"ok","timestamp":1679808723109,"user_tz":-540,"elapsed":494,"user":{"displayName":"박지수","userId":"15151265950850399854"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["IMDB Review Data 불러오기"],"metadata":{"id":"4S_-lcgoZwAN"}},{"cell_type":"code","source":["from tensorflow.keras.datasets import imdb\n","\n","# 데이터를 불러옵니다.\n","print('Loading data...')\n","(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=MAX_FEATURES)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0kNM93RAZvOW","executionInfo":{"status":"ok","timestamp":1679808736250,"user_tz":-540,"elapsed":5995,"user":{"displayName":"박지수","userId":"15151265950850399854"}},"outputId":"9acd7db9-3a2a-4a84-8f7c-1e0d9891c7b6"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading data...\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n","17464789/17464789 [==============================] - 2s 0us/step\n"]}]},{"cell_type":"code","source":["x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=MAXLEN)\n","x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test, maxlen=MAXLEN)"],"metadata":{"id":"akZhTgAFZwsB","executionInfo":{"status":"ok","timestamp":1679808737301,"user_tz":-540,"elapsed":1052,"user":{"displayName":"박지수","userId":"15151265950850399854"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["### 모델 선언"],"metadata":{"id":"2wYpKbgoZyMp"}},{"cell_type":"code","source":["model = Transformer(**kargs)\n","model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n","              loss='binary_crossentropy',\n","              metrics='accuracy')"],"metadata":{"id":"bhOTs4SmZxYy","executionInfo":{"status":"ok","timestamp":1679808738307,"user_tz":-540,"elapsed":1,"user":{"displayName":"박지수","userId":"15151265950850399854"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","\n","earlystop_callback = EarlyStopping(monitor='val_accuracy', min_delta=0.0001, patience=10)\n","\n","checkpoint_path = 'weights.h5'\n","\n","cp_callback = ModelCheckpoint(\n","    checkpoint_path, monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=True)"],"metadata":{"id":"HXGeVSerZy1d","executionInfo":{"status":"ok","timestamp":1679808740696,"user_tz":-540,"elapsed":2,"user":{"displayName":"박지수","userId":"15151265950850399854"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["### 모델 학습"],"metadata":{"id":"4UjPjKJBZ0CF"}},{"cell_type":"code","source":["history = model.fit(x_train, y_train, \n","                    batch_size=BATCH_SIZE, epochs=EPOCHS,\n","                    validation_split=VALID_SPLIT, callbacks=[earlystop_callback, cp_callback])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6vQUsv5tZzU-","executionInfo":{"status":"ok","timestamp":1679808898572,"user_tz":-540,"elapsed":151797,"user":{"displayName":"박지수","userId":"15151265950850399854"}},"outputId":"d8c29788-471e-42c1-996e-7d7259d9f55c"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","1407/1407 [==============================] - ETA: 0s - loss: 0.5178 - accuracy: 0.7309\n","Epoch 1: val_accuracy improved from -inf to 0.83800, saving model to weights.h5\n","1407/1407 [==============================] - 72s 41ms/step - loss: 0.5178 - accuracy: 0.7309 - val_loss: 0.3690 - val_accuracy: 0.8380\n","Epoch 2/3\n","1407/1407 [==============================] - ETA: 0s - loss: 0.2871 - accuracy: 0.8809\n","Epoch 2: val_accuracy did not improve from 0.83800\n","1407/1407 [==============================] - 41s 29ms/step - loss: 0.2871 - accuracy: 0.8809 - val_loss: 0.4104 - val_accuracy: 0.8324\n","Epoch 3/3\n","1407/1407 [==============================] - ETA: 0s - loss: 0.1585 - accuracy: 0.9406\n","Epoch 3: val_accuracy did not improve from 0.83800\n","1407/1407 [==============================] - 40s 28ms/step - loss: 0.1585 - accuracy: 0.9406 - val_loss: 0.4868 - val_accuracy: 0.8216\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"wHIKf9tCZ0zR"},"execution_count":null,"outputs":[]}]}