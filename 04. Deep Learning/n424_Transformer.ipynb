{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN5DQkELn+UBWGZ4FknYeFX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# Transformer & BERT, GPT"],"metadata":{"id":"L2I8Xhqh5-7k"}},{"cell_type":"markdown","source":["## 학습목표\n","\n","- **Transformer의 장점과 주요 프로세스인 Self-Attention에 대해 이해하고 설명할 수 있다.** \n","    - 트랜스포머를 발표한 논문 제목은 왜 \"Attention is All You Need\"인지 설명할 수 있다.\n","    - Positional Encoding을 적용하는 이유에 대해서 설명할 수 있다.\n","    - Masked Self-Attention가 트랜스포머 구조 중 어디에 적용되며 어떤 역할을 하는지 설명할 수 있다. \n","    - 기존 RNN과 비교하여 Transformer가 가지는 장점에 대해서 설명할 수 있다.\n","\n","- **GPT, BERT 그리고 다른 모델에 대해서 개략적으로 설명할 수 있다.**\n","    - GPT(Generative Pre-Training)\n","        - 사전 학습된 언어 모델(Pre-trained LM)의 Pre-training과 Fine-tuning은 무엇이고 각각 어떤 종류의 데이터셋을 사용하는 지 설명할 수 있다.\n","        - GPT는 Transformer를 어떻게 변형하였는지 설명할 수 있다.\n","    - BERT(Bidirectional Encoder Representation by Transformer)\n","        - BERT는 Transformer를 어떻게 변형하였으며 GPT와의 차이 무엇인지 알 수 있다.\n","        - MLM(Masked Language Model)은 무엇인지 이해할 수 있다.\n","        - NSP(Next Sentence Prediction)은 무엇인지 이해할 수 있다.\n","- **최근 언어 모델의 발전은 어떻게 진행되고 있는지 알 수 있다.**\n","\n","- 주의 : 이후에도 새로운 모델을 공부하게 되면 아래와 같은 순서로 학습하게 될 겁니다.\n","    1. 논문을 통해 해당 모델의 컨셉과 대략적인 구조를 파악한다.\n","    2. 기존에 구현되어 있는 라이브러리를 통해 해당 모델을 사용해본다. (추론 혹은 파인튜닝)\n","    3. 해당 모델에 대한 좀 더 깊은 이해가 필요할 경우 코드로 구현해본다.\n","    - 즉, 오늘은 첫 번째로 아래 개념을 배우는 만큼 모든 코드를 이해하지 못해도 괜찮습니다.\n"],"metadata":{"id":"eTttg9HN6BAT"}},{"cell_type":"markdown","source":["## 1. Transformer : Attention is All You Need"],"metadata":{"id":"ohp10C2o6FP3"}},{"cell_type":"markdown","source":["### 트랜스포머(Transformer)란?"],"metadata":{"id":"snQmCG9v6GZJ"}},{"cell_type":"markdown","source":["**<font color=\"ff6f61\">트랜스포머(Transformer)</font>**는 기계 번역을 위한 새로운 모델로 이전에 등장했던 **Attention 메커니즘을 극대화**하여 뛰어난 번역 성능을 기록했습니다.<br/>\n","최근 자연어처리 모델 SOTA(State-of-the-Art)의 기본 아이디어는 거의 모두 트랜스포머를 기반으로 하고 있습니다.<br/>\n","모델을 소개한 논문 [Attention is All You Need](https://arxiv.org/abs/1706.03762) 는 3년 사이에 18000번 이상 인용되었습니다.<br/>\n","트랜스포머가 자연어처리가 아닌 다른 문제도 잘 풀고있기 때문에 최근에는 컴퓨터 비전 쪽에서도 적용하려는 시도가 있으며, 멀티모달(Multi-Modal) 모델에도 적용되고 있습니다.<br/>\n","\n","RNN 기반 모델이 가진 구조적 단점은 단어가 **순서대로** 들어온다는 점입니다.<br/>\n","그렇기 때문에 처리해야 하는 시퀀스가 길수록 **연산 시간이** 길어집니다.<br/>\n","**트랜스포머는 이런 문제를 해결하기 위해 등장한 모델**입니다.<br/>\n","모든 토큰을 동시에 입력받아 병렬 연산하기 때문에 GPU 연산에 최적화 되어 있습니다.\n","\n","아래는 트랜스포머의 구조를 단순하게 시각화한 그림입니다.<br/>\n","Encoder, Decoder로 표현된 사각형을 각각 인코더 블록과 디코더 블록이라고 합니다.<br/>\n","트랜스포머는 **인코더 블록과 디코더 블록이 6개씩** 모여있는 구조를 하고 있습니다. "],"metadata":{"id":"AW_DeX7Q6HQh"}},{"cell_type":"markdown","source":["<img src=\"http://jalammar.github.io/images/t/The_transformer_encoder_decoder_stack.png\" alt=\"positional_encoding\" width=\"700\" />"],"metadata":{"id":"w7KEob-M6IOp"}},{"cell_type":"markdown","source":["그림 하나를 더 보도록 하겠습니다. 아래는 논문에서 트랜스포머의 구조를 나타낸 그림입니다.<br/>\n","그림을 보면 커다란 회색 블록이 2개 있습니다.<br/>\n","왼쪽은 인코더 블록 하나를 나타내고 오른쪽은 디코더 블록 하나를 나타냅니다.<br/>\n","인코더 블록은 크게 **2개의 sub-layer ▶️ [`Multi-Head (Self) Attention`, `Feed Forward`]** 로 나눌 수 있습니다.<br/>\n","디코더 블록은 **3개의 sub-layer ▶️ [`Masked Multi-Head (Self) Attention`, `Multi-Head (Encoder-Decoder) Attention`, `Feed Forward`]** 로 나눌 수 있습니다.\n","\n"],"metadata":{"id":"jdYN3gDn6J8J"}},{"cell_type":"markdown","source":["<img src=\"https://miro.medium.com/max/1400/1*BHzGVskWGS_3jEcYYi6miQ.png\" alt=\"positional_encoding\" width=\"550\" />"],"metadata":{"id":"54QOjx3H6M2u"}},{"cell_type":"markdown","source":["트랜스포머에서는 병렬화를 위해 모든 단어 벡터를 동시에 입력받습니다.<br/>\n","컴퓨터는 어떤 단어가 어디에 위치하는지 알 수 없게 됩니다.<br/>\n","그래서 컴퓨터가 이해할 수 있도록 단어의 위치 정보를 제공하기 위한 벡터를 따로 제공해주어야 합니다.<br/>\n","단어의 상대적인 위치 정보를 제공하기 위한 벡터를 만드는 과정을 **<font color=\"ff6f61\">Positional Encoding</font>** 이라고 합니다.<br/>\n","\n","Positional Encoding 은 아래와 같은 수식으로 이루어집니다.<br/>\n"],"metadata":{"id":"XtrGCL606OEl"}},{"cell_type":"markdown","source":["$$\n","\\begin{aligned}\n","\\text{PE}_{\\text{pos},2i} &= \\sin \\bigg(\\frac{\\text{pos}}{10000^{2i/d_{\\text{model}}}}\\bigg) \\\\\n","\\text{PE}_{\\text{pos},2i+1} &= \\cos \\bigg(\\frac{\\text{pos}}{10000^{2i/d_{\\text{model}}}}\\bigg)\n","\\end{aligned}\n","$$\n"],"metadata":{"id":"T-Ee9uaG6QDS"}},{"cell_type":"markdown","source":["아래는 Positional Encoding을 통해 만들어진 벡터를 시각화한 자료입니다.<br/>\n","일정한 패턴이 있는 벡터가 만들어지는 것을 볼 수 있습니다.<br/>\n","컴퓨터는 이를 통해 단어의 상대적인 위치를 파악하게 됩니다.<br/>"],"metadata":{"id":"vYwDccSf6Ro0"}},{"cell_type":"markdown","source":["<img src=\"http://jalammar.github.io/images/t/transformer_positional_encoding_large_example.png\" alt=\"positional_encoding\" width=\"500\" />"],"metadata":{"id":"5b2BvBK-6ScF"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"sjYuKp8P51Jg","executionInfo":{"status":"ok","timestamp":1679806143629,"user_tz":-540,"elapsed":2,"user":{"displayName":"박지수","userId":"15151265950850399854"}}},"outputs":[],"source":["def get_angles(pos, i, d_model):\n","    \"\"\"\n","    sin, cos 안에 들어갈 수치를 구하는 함수입니다.\n","    \"\"\"\n","    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n","    return pos * angle_rates"]},{"cell_type":"code","source":["def positional_encoding(position, d_model):\n","    \"\"\"\n","    위치 인코딩(Positional Encoding)을 구하는 함수입니다.\n","    \n","    \"\"\"\n","    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n","                          np.arange(d_model)[np.newaxis, :],\n","                          d_model)\n","\n","    # apply sin to even indices in the array; 2i\n","    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n","\n","    # apply cos to odd indices in the array; 2i+1\n","    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n","\n","    pos_encoding = angle_rads[np.newaxis, ...]\n","\n","    return tf.cast(pos_encoding, dtype=tf.float32)"],"metadata":{"id":"2ymI820EP4Eo","executionInfo":{"status":"ok","timestamp":1679806143630,"user_tz":-540,"elapsed":2,"user":{"displayName":"박지수","userId":"15151265950850399854"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["### Self-Attention (셀프-어텐션)\n","\n","\n","\n"],"metadata":{"id":"T1wZiKgsP6U1"}},{"cell_type":"markdown","source":["<img width=\"300\" alt=\"self-Attn\" src=\"https://user-images.githubusercontent.com/45377884/112809266-ca735080-90b4-11eb-9a25-7f34f37880c7.png\">"],"metadata":{"id":"8XL88qw4P7X_"}},{"cell_type":"markdown","source":["트랜스포머의 주요 메커니즘인 **<font color=\"ff6f61\">Self-Attention</font>** 에 대해 알아보도록 하겠습니다.\n","\n","아래와 같은 문장이 있다고 해보겠습니다.\n","\n","> *The animal didn't cross the street because <ins>it</ins> was too tired* \n","\n","위 문장을 제대로 번역하려면 **_\"it\"_** 과 같은 지시대명사가 어떤 대상을 가리키는지 알아야 합니다.<br/>\n","그렇기 때문에 트랜스포머에서는 번역하려는 문장 내부 요소의 관계를 잘 파악하기 위해서, 문장 자신에 대해 어텐션 메커니즘을 적용합니다.<br/>\n","이를 **Self-Attention** 이라고 합니다.\n","\n","아래는 **_\"it\"_** 이 어떤 단어와 가장 연관되어 있는 지를 시각화한 그림입니다."],"metadata":{"id":"QR13HDEzP8XA"}},{"cell_type":"markdown","source":["<img src=\"http://jalammar.github.io/images/t/transformer_self-attention_visualization.png\" alt=\"self_attention_visualization\" width=\"350\" />"],"metadata":{"id":"uQhAc_uKP9Xw"}},{"cell_type":"markdown","source":["**Self-Attention**은 어떤 과정이길래 단어 사이의 관계를 알아낼 수 있을까요?\n","\n","Self-Attention에서도 쿼리(Query)-키(Key)-밸류(Value)의 아이디어가 동일하게 등장합니다.\n"],"metadata":{"id":"xQUE_g_qP-MK"}},{"cell_type":"markdown","source":["- **지난 시간에 배운 내용을 복습해봅시다 - 검색 시스템에서 흘러나온 쿼리, 키, 밸류**\n","\n","Attention을 다룰 때 등장했던 검색 시스템의 아이디어를 다시 복습해봅시다.\n","\n","아래는 구글에서 _\"what is attention in nlp\"_ 라는 검색어를 구글에 입력했을 때의 검색 결과를 나타낸 이미지입니다."],"metadata":{"id":"LvojIGW0P-5i"}},{"cell_type":"markdown","source":["<img src=\"https://i.imgur.com/JdCQr1l.png\" alt=\"retrieval_system\" width=\"600\" />"],"metadata":{"id":"n24fZNeBQABL"}},{"cell_type":"markdown","source":["그림에서 볼 수 있듯이 검색 시스템은 아래와 같은 3단계를 거쳐 작동합니다.\n","\n","1. 찾고자 하는 정보에 대한 검색어(Query)를 입력합니다.\n","2. 검색 엔진은 검색어와 가장 비슷한 키워드(Key)를 찾습니다.\n","3. 그리고 해당 키워드(Key)와 연결된 페이지(Value)를 보여줍니다."],"metadata":{"id":"QBb3k73ZQBI0"}},{"cell_type":"markdown","source":["**기존 Attention과의 차이는 각 벡터가 모두 가중치 벡터라는 점**입니다.<br/>\n","\n","각각의 벡터가 어떤 역할을 하는지 알아보겠습니다.\n","\n","- **쿼리(q)**는 분석하고자 하는 단어에 대한 가중치 벡터입니다.\n","\n","- **키(k)**는 각 단어가 쿼리에 해당하는 단어와 얼마나 연관있는 지를 비교하기 위한 가중치 벡터입니다.\n","\n","- **밸류(v)**는 각 단어의 의미를 살려주기 위한 가중치 벡터입니다."],"metadata":{"id":"hlpvT8m8QB88"}},{"cell_type":"markdown","source":["**Self-Attention**은 세 가지 가중치 벡터를 대상으로 어텐션을 적용합니다.<br/>\n","적용하는 방식은 기존 Attention 메커니즘과 거의 동일합니다.\n","\n","1. 먼저, **특정 단어의 쿼리(q) 벡터와 모든 단어의 키(k) 벡터를 내적**합니다. 내적을 통해 나오는 값이 Attention 스코어(Score)가 됩니다.\n","\n","2. 트랜스포머에서는 이 가중치를 q,k,v 벡터 차원 $d_k$ 의 제곱근인 $\\sqrt{d_k}$ 로 나누어줍니다.<br/>계산값을 안정적으로 만들어주기 위한 계산 보정으로 생각해주시면 됩니다.  \n","\n","3. 다음으로 **Softmax**를 취해줍니다.<br/>\n","이를 통해 쿼리에 해당하는 단어와 문장 내 다른 단어가 가지는 관계의 비율을 구할 수 있습니다.\n","\n","4. 마지막으로 **밸류(v) 각 단어의 벡터를 곱해준 후 모두 더하면** Self-Attention 과정이 마무리됩니다. "],"metadata":{"id":"B_5eMpLvQCzC"}},{"cell_type":"markdown","source":["**Self-Attention** 의 과정을 그림으로 다시 보겠습니다."],"metadata":{"id":"1nl2usMdQDwY"}},{"cell_type":"markdown","source":["**1. 가중치 행렬 $W^Q, W^K, W^V$ 로부터 각 단어의 쿼리, 키, 밸류(q, k, v) 벡터를 만들어냅니다.**"],"metadata":{"id":"r0JOt57PQEnG"}},{"cell_type":"markdown","source":["<img src=\"http://jalammar.github.io/images/xlnet/self-attention-1.png\" alt=\"transformer_15\" width=\"600\" />"],"metadata":{"id":"B2TTb8Z0QFaa"}},{"cell_type":"markdown","source":["**2. 분석하고자 하는 단어의 쿼리 벡터(q)와 문장 내 모든 단어(자신 포함)의 키 벡터(k)를 내적하여 각 단어와 얼마나 관련 정도를 구합니다.**\n","\n","(아래 그림에서는 $\\sqrt{d_k}$로 나누어 준 뒤에 Softmax를 취해주는 과정은 생략되었습니다.)\n"],"metadata":{"id":"mxkn0NIcQGPz"}},{"cell_type":"markdown","source":["<img src=\"http://jalammar.github.io/images/xlnet/self-attention-2.png\" alt=\"transformer_15\" width=\"600\" />"],"metadata":{"id":"Z9y0Pmk5QHEX"}},{"cell_type":"markdown","source":["**3.  Softmax의 출력값과밸류 벡터(v)를 곱해준 뒤 더하면 해당 단어에 대한 Self-Attention 출력값을 얻을 수 있습니다.**"],"metadata":{"id":"OQgLCkesQKO_"}},{"cell_type":"markdown","source":["<img src=\"http://jalammar.github.io/images/xlnet/self-attention-3.png\" alt=\"transformer_15\" width=\"600\" />"],"metadata":{"id":"45cvumYcQK13"}},{"cell_type":"markdown","source":["실제로 각 벡터는 **행렬(Q, K, V)**로 한꺼번에 계산됩니다. $W^Q, W^K, W^V$ 는 학습 과정에서 갱신되는 파라미터로 이루어진 행렬입니다.<br/>\n","세 행렬과 단어 행렬을 내적하여 쿼리, 키, 밸류 행렬(Q, K, V)를 만들어냅니다.\n","\n"],"metadata":{"id":"imOf1Ca9QLvb"}},{"cell_type":"markdown","source":["<img src=\"http://jalammar.github.io/images/t/self-attention-matrix-calculation.png\" alt=\"transformer_12\" width=\"400\" />"],"metadata":{"id":"0hTlivIxQMbV"}},{"cell_type":"markdown","source":["위에서 살펴본 바와 같이\n","\n","1. 먼저 쿼리 행렬(Q)과 키 행렬(K)을 **내적**합니다.\n","\n","2. 결과로 나오는 행렬의 요소를 $\\sqrt{d_k}$ 로 **나누어 줍니다.**\n","\n","3. 행렬의 각 요소에 **소프트맥스(Softmax)**를 취해줍니다. \n","\n","4. 마지막으로 **밸류 행렬(V)과 내적**하면 최종 결과 행렬(Z)이 반환됩니다."],"metadata":{"id":"-VWiWqwsQNQg"}},{"cell_type":"markdown","source":["<img src=\"http://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png\" alt=\"transformer_13\" width=\"700\" />"],"metadata":{"id":"fk_rORvhQN9n"}},{"cell_type":"markdown","source":["아래는 Tensorflow 에서 Self-Attention을 구현한 코드입니다. 코드를 통해 Attention이 계산되는 과정을 다시 살펴보도록 합시다."],"metadata":{"id":"lwrcoldUQP1K"}},{"cell_type":"code","source":["def scaled_dot_product_attention(q, k, v, mask):\n","    \"\"\"\n","    Attention 가중치를 구하는 함수입니다.\n","    q, k, v 의 leading dimension은 동일해야 합니다.\n","    k, v의 penultimate dimension이 동일해야 합니다, i.e.: seq_len_k = seq_len_v.\n","\n","    Mask는 타입(padding or look ahead)에 따라 다른 차원을 가질 수 있습니다.\n","    덧셈시에는 브로드캐스팅 될 수 있어야합니다.\n","    \n","    Args:\n","        q: query shape == (..., seq_len_q, depth)\n","        k: key shape == (..., seq_len_k, depth)\n","        v: value shape == (..., seq_len_v, depth_v)\n","        mask: Float tensor with shape broadcastable \n","            to (..., seq_len_q, seq_len_k). Defaults to None.\n","        \n","    Returns:\n","        output, attention_weights\n","    \"\"\"\n","\n","    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n","    \n","    # matmul_qk(쿼리와 키의 내적)을 dk의 제곱근으로 scaling 합니다.\n","    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n","    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n","\n","    # 마스킹을 진행합니다.\n","    if mask is not None:\n","        scaled_attention_logits += (mask * -1e9)  \n","\n","    # 소프트맥스(softmax) 함수를 통해서 attention weight 를 구해봅시다.\n","    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n","\n","    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n","\n","    return output, attention_weights"],"metadata":{"id":"Wj8TJ07AP4zX","executionInfo":{"status":"ok","timestamp":1679806237803,"user_tz":-540,"elapsed":3,"user":{"displayName":"박지수","userId":"15151265950850399854"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["### Multi-Head Attention"],"metadata":{"id":"kA2GflllQRgk"}},{"cell_type":"markdown","source":["다음으로 **<font color=\"ff6f61\">Multi-Head Attention</font>** 에 대해 알아보겠습니다.<br/>\n","Multi-Head Attention 이란 **Self-Attention을 동시에 병렬적으로 실행하는 것**입니다.<br/>\n","각 Head 마다 다른 Attention 결과를 내어주기 때문에 앙상블과 유사한 효과를 얻을 수 있습니다.<br/> \n","논문에서는 8개의 Head를 사용하였습니다.<br/>\n","8번의 Self-Attention을 실행하여 각각의 출력 행렬 $Z_0, Z_1, \\cdots , Z_7$ 을 만들어냅니다."],"metadata":{"id":"joMZst1uQSV3"}},{"cell_type":"markdown","source":["<img src=\"http://jalammar.github.io/images/t/transformer_attention_heads_z.png\" alt=\"transformer_16\" width=\"500\"/>"],"metadata":{"id":"LC9vagBUQSRi"}},{"cell_type":"markdown","source":["출력된 행렬 $Z_n (n=0,\\cdots,7)$ 은 **이어붙여집니다(Concatenate)**.<br/>\n","또 다른 파라미터 행렬인 $W^o$ 와의 내적을 통해 Multi-Head Attention의 최종 결과인 행렬 $Z$를 만들어냅니다.<br/>\n","여기서 행렬 $W^o$의 요소 역시 학습을 통해 갱신됩니다.<br/>\n","최종적으로 생성된 행렬 $Z$는 토큰 벡터로 이루어진 행렬 $X$와 **동일한 크기(Shape)**가 됩니다."],"metadata":{"id":"Cf06jUHjQUL6"}},{"cell_type":"markdown","source":["<img src=\"http://jalammar.github.io/images/t/transformer_attention_heads_weight_matrix_o.png\" alt=\"transformer_17\" width=\"500\" />"],"metadata":{"id":"5mElvbW-QU9O"}},{"cell_type":"markdown","source":["### Layer Normalization & Skip Connection\n","\n"],"metadata":{"id":"37RmBo1CQV_9"}},{"cell_type":"markdown","source":["<img width=\"300\" alt=\"lnorm_resicon\" src=\"https://user-images.githubusercontent.com/45377884/113169444-9056aa00-9280-11eb-8ba0-17c9211ad412.png\">"],"metadata":{"id":"DnIOyrl1QWnG"}},{"cell_type":"markdown","source":["트랜스포머의 모든 sub-layer에서 출력된 벡터는 **Layer normalization**과 **Skip connection**을 거치게 됩니다.<br/>\n","Layer normalization의 효과는 Batch normalization과 유사합니다. 학습이 훨씬 빠르고 잘 되도록 합니다.<br/>\n","Skip connection(혹은 Residual connection)은 역전파 과정에서 정보가 소실되지 않도록 합니다.<br/>"],"metadata":{"id":"dTt80AXdQXZh"}},{"cell_type":"markdown","source":["### Feed Forward Neural Network"],"metadata":{"id":"NB3dOTHNQYLK"}},{"cell_type":"markdown","source":["<img width=\"300\" alt=\"스크린샷 2021-03-29 오후 5 27 32\" src=\"https://user-images.githubusercontent.com/45377884/112808809-58027080-90b4-11eb-8ca7-ffa38e577d3d.png\">\n","\n"],"metadata":{"id":"fUdgVVqYQY9S"}},{"cell_type":"markdown","source":["다음으로 **<font color=\"ff6f61\">FFNN(Feed forward neural network)</font>** 로 들어갑니다.<br/>은닉층의 차원이 늘어났다가 다시 원래 차원으로 줄어드는 단순한 2층 신경망입니다.<br/>활성화 함수(Activation function)으로 ReLU를 사용합니다.\n","\n","$$\n"," \\text{FFNN}(x) = \\max(0, W_1x + b_1) W_2 +b_2\n","$$"],"metadata":{"id":"4kpBccJyQb65"}},{"cell_type":"code","source":["def point_wise_feed_forward_network(d_model, dff):\n","    \"\"\"\n","    FFNN을 구현한 코드입니다.\n","\n","    Args:\n","        d_model : 모델의 차원입니다.\n","        dff : 은닉층의 차원 수입니다. 논문에서는 2048을 사용하였습니다.\n","    \"\"\"\n","    return tf.keras.Sequential([\n","        tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n","        tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n","    ])"],"metadata":{"id":"mmmZHTriQQmx","executionInfo":{"status":"ok","timestamp":1679806289800,"user_tz":-540,"elapsed":3,"user":{"displayName":"박지수","userId":"15151265950850399854"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["### Masked Self-Attention\n"],"metadata":{"id":"tvXi_x8tQeFw"}},{"cell_type":"markdown","source":["<img width=\"300\" alt=\"Masked_Self-Attention_in_structure\" src=\"https://user-images.githubusercontent.com/45377884/112808936-78322f80-90b4-11eb-9315-22cd9caad41d.png\">"],"metadata":{"id":"bVsbNn_2Qe6s"}},{"cell_type":"markdown","source":["**<font color=\"ff6f61\">Masked Self-Attention</font>**은 디코더 블록에서 사용되는 특수한 Self-Attention입니다.<br/>\n","디코더는 Auto-Regressive(왼쪽 단어를 보고 오른쪽 단어를 예측)하게 단어를 생성하기 때문에 타깃 단어 이후 단어를 보지 않고 단어를 예측해야 합니다.<br/>\n","따라서 타깃 단어 뒤에 위치한 단어는 Self-Attention에 영향을 주지 않도록 **마스킹(masking)**을 해주어야 합니다."],"metadata":{"id":"Il-330IzQfoF"}},{"cell_type":"markdown","source":["<img width=\"500\" alt=\"Masked_Self-Attention_ex\" src=\"http://jalammar.github.io/images/xlnet/transformer-decoder-block-self-attention-2.png\">"],"metadata":{"id":"cImpzTyWQgPH"}},{"cell_type":"markdown","source":["***Self-Attention (without Masking) vs Masked Self-Attention***\n","\n","<img width=\"500\" alt=\"Masked_Self-Attention_ex2\" src=\"http://jalammar.github.io/images/gpt2/self-attention-and-masked-self-attention.png\">"],"metadata":{"id":"VvBa3hv9QhP5"}},{"cell_type":"markdown","source":["Self-Attention 메커니즘은 쿼리 행렬(Q)와 키 행렬(K)의 내적합니다.<br/>\n","결과로 나온 행렬을 차원의 제곱근 $\\sqrt{d_k}$ 로 나누어 준 다음,<br/> \n","Softmax를 취해주고 밸류 행렬(V)과 내적하였습니다.\n","\n","**Masked Self-Attention** 에서는 Softmax를 취해주기 전, 가려주고자 하는 요소에만 $-\\infty$ 에 해당하는 매우 작은 수를 더해줍니다.<br/>\n","아래 코드 예시에서는 -10억(=`-1e9`)을 더해주었습니다.<br/>\n","이 과정을 **마스킹(Masking)**이라고 합니다.<br/>\n","마스킹된 값은 Softmax를 취해 주었을 때 0이 나오므로 Value 계산에 반영되지 않습니다."],"metadata":{"id":"elsrJYUoQh_G"}},{"cell_type":"markdown","source":["<img width=\"600\" alt=\"masked_1\" src=\"http://jalammar.github.io/images/gpt2/transformer-attention-mask.png\">\n","\n","<img width=\"600\" alt=\"masked_2\" src=\"http://jalammar.github.io/images/gpt2/transformer-attention-masked-scores-softmax.png\">"],"metadata":{"id":"GR7dmQHWQjG7"}},{"cell_type":"markdown","source":["위에서 등장했던 Self-Attention을 구현 코드에서 `mask` 와 관련된 부분만 다시 보도록 합시다."],"metadata":{"id":"zcN7fjouQkhR"}},{"cell_type":"code","source":["def scaled_dot_product_attention(q, k, v, mask):\n","    \"\"\"\n","    Attention 가중치를 구하는 함수입니다.\n","    q, k, v 의 leading dimension은 동일해야 합니다.\n","    k, v의 penultimate dimension이 동일해야 합니다, i.e.: seq_len_k = seq_len_v.\n","\n","    Mask는 타입(padding or look ahead)에 따라 다른 차원을 가질 수 있습니다.\n","    덧셈시에는 브로드캐스팅 될 수 있어야합니다.\n","    \n","    Args:\n","        q: query shape == (..., seq_len_q, depth)\n","        k: key shape == (..., seq_len_k, depth)\n","        v: value shape == (..., seq_len_v, depth_v)\n","        mask: Float tensor with shape broadcastable \n","            to (..., seq_len_q, seq_len_k). Defaults to None.\n","        \n","    Returns:\n","        output, attention_weights\n","    \"\"\"\n","\n","    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n","    \n","    # matmul_qk(쿼리와 키의 내적)을 dk의 제곱근으로 scaling 합니다.\n","    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n","    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n","\n","    \"\"\"\n","    mask가 있을 경우 masking된 자리(mask=1)에는 (-inf)에 해당하는 절댓값이 큰 음수 -1e9(=-10억)을 더해줍니다.\n","    그 값에 softmax를 취해주면 거의 0에 가까운 값이 나옵니다. 그 다음 value 계산시에 반영되지 않습니다.\n","    \"\"\"\n","\n","    # 마스킹을 진행합니다.\n","    if mask is not None:\n","        scaled_attention_logits += (mask * -1e9)  \n","\n","    # 소프트맥스(softmax) 함수를 통해서 attention weight 를 구해봅시다.\n","    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n","\n","    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n","\n","    return output, attention_weights"],"metadata":{"id":"vQGzgTgmQdS0","executionInfo":{"status":"ok","timestamp":1679806322312,"user_tz":-540,"elapsed":1,"user":{"displayName":"박지수","userId":"15151265950850399854"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["### Encoder-Decoder Attention"],"metadata":{"id":"clZWbhU-QmPC"}},{"cell_type":"markdown","source":["<img width=\"300\" alt=\"Encoder-Decoder_Attention\" src=\"https://user-images.githubusercontent.com/45377884/112809435-f8f12b80-90b4-11eb-96e1-3b0f7c530659.png\">"],"metadata":{"id":"-uBCiAUiQnFi"}},{"cell_type":"markdown","source":["디코더에서 Masked Self-Attention 층을 지난 벡터는 **<font color=\"ff6f61\">Encoder-Decoder Attention</font>** 층으로 들어갑니다.<br/>\n","좋은 번역을 위해서는 **번역할 문장과 번역된 문장 간의 관계** 역시 중요합니다.<br/>\n","번역할 문장과 번역되는 문장의 정보 관계를 엮어주는 부분이 바로 이 부분입니다.\n","\n","이 층에서는 **디코더 블록의** Masked Self-Attention으로부터 출력된 벡터를 **쿼리(Q)** 벡터로 사용합니다.<br/>\n","**키(K)와 밸류(V)** 벡터는 최상위(=6번째) 인코더 블록에서 사용했던 값을 그대로 가져와서 사용합니다.<br/>\n","**Encoder-Decoder Attention** 층의 계산 과정은 Self-Attention 했던 것과 동일합니다.\n","\n","아래는 **Encoder-Decoder Attention** 가 진행되는 순서를 나타낸 이미지입니다."],"metadata":{"id":"SrEb-Zr6Qn65"}},{"cell_type":"markdown","source":["<img width=\"700\" alt=\"Encoder-Decoder_Attention_gif\" src=\"http://jalammar.github.io/images/t/transformer_decoding_1.gif\">"],"metadata":{"id":"Ej_b5-blQo-T"}},{"cell_type":"markdown","source":["### Linear & Softmax Layer\n"],"metadata":{"id":"vNRqOIy_Qpym"}},{"cell_type":"markdown","source":["<img width=\"300\" alt=\"Linear_Softmax\" src=\"https://user-images.githubusercontent.com/45377884/112815762-994a4e80-90bb-11eb-9a57-a8be65c1a30b.png\">\n","\n","디코더의 최상층을 통과한 벡터들은 Linear 층을 지난 후 Softmax를 통해 예측할 단어의 확률을 구하게 됩니다."],"metadata":{"id":"KCN0-jU6QqyM"}},{"cell_type":"markdown","source":["## 코드 실습 1\n","\n","(오래 걸립니다. 원활한 실습을 위해서 epoch 수를 줄이고 학습해도 좋습니다.)"],"metadata":{"id":"-jeV6AKJQsZP"}},{"cell_type":"markdown","source":["필요한 모듈을 import 해줍니다."],"metadata":{"id":"-on62IcRQtmP"}},{"cell_type":"code","source":["#!pip install tensorflow==2.12.0"],"metadata":{"id":"oYDXJ68IQ6mX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pathlib\n","import random\n","import string\n","import re\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.layers import TextVectorization"],"metadata":{"id":"A-hj9eAxQlSg","executionInfo":{"status":"ok","timestamp":1679807366251,"user_tz":-540,"elapsed":350,"user":{"displayName":"박지수","userId":"15151265950850399854"}}},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":["학습팔 말뭉치를 불러옵니다.<br/>\n","아래 코드에서는 스페인어 - 영어 말뭉치를 불러옵니다."],"metadata":{"id":"HVmIAeNMQvDd"}},{"cell_type":"code","source":["text_file = keras.utils.get_file(\n","    fname=\"spa-eng.zip\",\n","    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",\n","    extract=True,\n",")\n","text_file = pathlib.Path(text_file).parent / \"spa-eng\" / \"spa.txt\""],"metadata":{"id":"TNkNk-hHQuQz","executionInfo":{"status":"ok","timestamp":1679807366593,"user_tz":-540,"elapsed":1,"user":{"displayName":"박지수","userId":"15151265950850399854"}}},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":["학습할 수 있도록 적절히 전처리를 해줍니다."],"metadata":{"id":"2CQpyBmYRmke"}},{"cell_type":"code","source":["with open(text_file) as f:\n","    lines = f.read().split(\"\\n\")[:-1]\n","text_pairs = []\n","for line in lines:\n","    eng, spa = line.split(\"\\t\")\n","    spa = \"[start] \" + spa + \" [end]\"\n","    text_pairs.append((eng, spa))"],"metadata":{"id":"UJoDtWGEQvtK","executionInfo":{"status":"ok","timestamp":1679807367212,"user_tz":-540,"elapsed":300,"user":{"displayName":"박지수","userId":"15151265950850399854"}}},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":["전처리가 잘 되었는지 확인합니다. <br/>\n","번역되는 문장의 앞에는 [start] 토큰을 위치시키고 뒤에는 [end] 토큰을 위치시킵니다."],"metadata":{"id":"H4jF-bNoRoFa"}},{"cell_type":"code","source":["for _ in range(5):\n","    print(random.choice(text_pairs))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xg3QQUkKRnOZ","executionInfo":{"status":"ok","timestamp":1679807367212,"user_tz":-540,"elapsed":4,"user":{"displayName":"박지수","userId":"15151265950850399854"}},"outputId":"6090b2ad-fa6a-40af-ca43-c52e97a02d44"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["('He is as poor as can be.', '[start] Él es pobre como pocos. [end]')\n","('It is mine.', '[start] Es mío. [end]')\n","('The extremists refused to negotiate.', '[start] Los extremistas rechazaron negociar. [end]')\n","('I like to dance.', '[start] Me gusta bailar. [end]')\n","('I have a meeting this afternoon.', '[start] Tengo una reunión esta tarde. [end]')\n"]}]},{"cell_type":"markdown","source":["데이터셋을 split 해줍니다."],"metadata":{"id":"7Uqogh-NRrhy"}},{"cell_type":"code","source":["random.shuffle(text_pairs)\n","num_val_samples = int(0.15 * len(text_pairs))\n","num_train_samples = len(text_pairs) - 2 * num_val_samples\n","train_pairs = text_pairs[:num_train_samples]\n","val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n","test_pairs = text_pairs[num_train_samples + num_val_samples :]\n","\n","print(f\"{len(text_pairs)} total pairs\")\n","print(f\"{len(train_pairs)} training pairs\")\n","print(f\"{len(val_pairs)} validation pairs\")\n","print(f\"{len(test_pairs)} test pairs\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZF6H3eMgRo6J","executionInfo":{"status":"ok","timestamp":1679807367212,"user_tz":-540,"elapsed":4,"user":{"displayName":"박지수","userId":"15151265950850399854"}},"outputId":"c7cd677a-2e71-4d46-ca3b-bc69fbf29ed9"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["118964 total pairs\n","83276 training pairs\n","17844 validation pairs\n","17844 test pairs\n"]}]},{"cell_type":"code","source":["strip_chars = string.punctuation + \"¿\"\n","strip_chars = strip_chars.replace(\"[\", \"\")\n","strip_chars = strip_chars.replace(\"]\", \"\")\n","\n","vocab_size = 15000\n","sequence_length = 20\n","batch_size = 64\n","\n","\n","def custom_standardization(input_string):\n","    lowercase = tf.strings.lower(input_string)\n","    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n","\n","\n","eng_vectorization = TextVectorization(\n","    max_tokens=vocab_size, output_mode=\"int\", output_sequence_length=sequence_length,\n",")\n","spa_vectorization = TextVectorization(\n","    max_tokens=vocab_size,\n","    output_mode=\"int\",\n","    output_sequence_length=sequence_length + 1,\n","    standardize=custom_standardization,\n",")\n","train_eng_texts = [pair[0] for pair in train_pairs]\n","train_spa_texts = [pair[1] for pair in train_pairs]\n","eng_vectorization.adapt(train_eng_texts)\n","spa_vectorization.adapt(train_spa_texts)"],"metadata":{"id":"Lf1JQhBDRsJ7","executionInfo":{"status":"ok","timestamp":1679807384080,"user_tz":-540,"elapsed":16558,"user":{"displayName":"박지수","userId":"15151265950850399854"}}},"execution_count":43,"outputs":[]},{"cell_type":"code","source":["def format_dataset(eng, spa):\n","    eng = eng_vectorization(eng)\n","    spa = spa_vectorization(spa)\n","    return ({\"encoder_inputs\": eng, \"decoder_inputs\": spa[:, :-1],}, spa[:, 1:])\n","\n","\n","def make_dataset(pairs):\n","    eng_texts, spa_texts = zip(*pairs)\n","    eng_texts = list(eng_texts)\n","    spa_texts = list(spa_texts)\n","    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n","    dataset = dataset.batch(batch_size)\n","    dataset = dataset.map(format_dataset)\n","    return dataset.shuffle(2048).prefetch(16).cache()\n","\n","\n","train_ds = make_dataset(train_pairs)\n","val_ds = make_dataset(val_pairs)"],"metadata":{"id":"TJjjgnJWRstN","executionInfo":{"status":"ok","timestamp":1679807384800,"user_tz":-540,"elapsed":729,"user":{"displayName":"박지수","userId":"15151265950850399854"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["for inputs, targets in train_ds.take(1):\n","    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n","    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n","    print(f\"targets.shape: {targets.shape}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aiEek6H3RtcN","executionInfo":{"status":"ok","timestamp":1679807385515,"user_tz":-540,"elapsed":716,"user":{"displayName":"박지수","userId":"15151265950850399854"}},"outputId":"f6e637b4-13fb-4db8-c45a-10277a80dab3"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["inputs[\"encoder_inputs\"].shape: (64, 20)\n","inputs[\"decoder_inputs\"].shape: (64, 20)\n","targets.shape: (64, 20)\n"]}]},{"cell_type":"code","source":["class TransformerEncoder(layers.Layer):\n","    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n","        super().__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","        self.dense_dim = dense_dim\n","        self.num_heads = num_heads\n","        self.attention = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim\n","        )\n","        self.dense_proj = keras.Sequential(\n","            [layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n","        )\n","        self.layernorm_1 = layers.LayerNormalization()\n","        self.layernorm_2 = layers.LayerNormalization()\n","        self.supports_masking = True\n","\n","    def call(self, inputs, mask=None):\n","        if mask is not None:\n","            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n","        attention_output = self.attention(\n","            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n","        )\n","        proj_input = self.layernorm_1(inputs + attention_output)\n","        proj_output = self.dense_proj(proj_input)\n","        return self.layernorm_2(proj_input + proj_output)\n","\n","\n","class PositionalEmbedding(layers.Layer):\n","    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n","        super().__init__(**kwargs)\n","        self.token_embeddings = layers.Embedding(\n","            input_dim=vocab_size, output_dim=embed_dim\n","        )\n","        self.position_embeddings = layers.Embedding(\n","            input_dim=sequence_length, output_dim=embed_dim\n","        )\n","        self.sequence_length = sequence_length\n","        self.vocab_size = vocab_size\n","        self.embed_dim = embed_dim\n","\n","    def call(self, inputs):\n","        length = tf.shape(inputs)[-1]\n","        positions = tf.range(start=0, limit=length, delta=1)\n","        embedded_tokens = self.token_embeddings(inputs)\n","        embedded_positions = self.position_embeddings(positions)\n","        return embedded_tokens + embedded_positions\n","\n","    def compute_mask(self, inputs, mask=None):\n","        return tf.math.not_equal(inputs, 0)\n","\n","\n","class TransformerDecoder(layers.Layer):\n","    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n","        super().__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","        self.latent_dim = latent_dim\n","        self.num_heads = num_heads\n","        self.attention_1 = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim\n","        )\n","        self.attention_2 = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim\n","        )\n","        self.dense_proj = keras.Sequential(\n","            [layers.Dense(latent_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n","        )\n","        self.layernorm_1 = layers.LayerNormalization()\n","        self.layernorm_2 = layers.LayerNormalization()\n","        self.layernorm_3 = layers.LayerNormalization()\n","        self.supports_masking = True\n","\n","    def call(self, inputs, encoder_outputs, mask=None):\n","        causal_mask = self.get_causal_attention_mask(inputs)\n","        if mask is not None:\n","            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n","            padding_mask = tf.minimum(padding_mask, causal_mask)\n","\n","        attention_output_1 = self.attention_1(\n","            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n","        )\n","        out_1 = self.layernorm_1(inputs + attention_output_1)\n","\n","        attention_output_2 = self.attention_2(\n","            query=out_1,\n","            value=encoder_outputs,\n","            key=encoder_outputs,\n","            attention_mask=padding_mask,\n","        )\n","        out_2 = self.layernorm_2(out_1 + attention_output_2)\n","\n","        proj_output = self.dense_proj(out_2)\n","        return self.layernorm_3(out_2 + proj_output)\n","\n","    def get_causal_attention_mask(self, inputs):\n","        input_shape = tf.shape(inputs)\n","        batch_size, sequence_length = input_shape[0], input_shape[1]\n","        i = tf.range(sequence_length)[:, tf.newaxis]\n","        j = tf.range(sequence_length)\n","        mask = tf.cast(i >= j, dtype=\"int32\")\n","        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n","        mult = tf.concat(\n","            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n","            axis=0,\n","        )\n","        return tf.tile(mask, mult)"],"metadata":{"id":"D2lsgUv0RuQw","executionInfo":{"status":"ok","timestamp":1679807385515,"user_tz":-540,"elapsed":1,"user":{"displayName":"박지수","userId":"15151265950850399854"}}},"execution_count":46,"outputs":[]},{"cell_type":"code","source":["embed_dim = 256\n","latent_dim = 2048\n","num_heads = 8\n","\n","encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n","x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n","encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n","encoder = keras.Model(encoder_inputs, encoder_outputs)\n","\n","decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n","encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n","x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n","x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n","x = layers.Dropout(0.5)(x)\n","decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n","decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n","\n","decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n","transformer = keras.Model(\n","    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",")"],"metadata":{"id":"hfsNLzi9RwJm","executionInfo":{"status":"ok","timestamp":1679807386140,"user_tz":-540,"elapsed":626,"user":{"displayName":"박지수","userId":"15151265950850399854"}}},"execution_count":47,"outputs":[]},{"cell_type":"code","source":["epochs = 1  # This should be at least 30 for convergence\n","\n","transformer.summary()\n","transformer.compile(\n","    \"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",")\n","transformer.fit(train_ds, epochs=epochs, validation_data=val_ds)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RXRCwUsFRyxk","executionInfo":{"status":"ok","timestamp":1679807532830,"user_tz":-540,"elapsed":146701,"user":{"displayName":"박지수","userId":"15151265950850399854"}},"outputId":"bb72a94d-3fe0-4e17-b865-33f2df51892d"},"execution_count":48,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"transformer\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," encoder_inputs (InputLayer)    [(None, None)]       0           []                               \n","                                                                                                  \n"," positional_embedding_4 (Positi  (None, None, 256)   3845120     ['encoder_inputs[0][0]']         \n"," onalEmbedding)                                                                                   \n","                                                                                                  \n"," decoder_inputs (InputLayer)    [(None, None)]       0           []                               \n","                                                                                                  \n"," transformer_encoder_2 (Transfo  (None, None, 256)   3155456     ['positional_embedding_4[0][0]'] \n"," rmerEncoder)                                                                                     \n","                                                                                                  \n"," model_5 (Functional)           (None, None, 15000)  12959640    ['decoder_inputs[0][0]',         \n","                                                                  'transformer_encoder_2[0][0]']  \n","                                                                                                  \n","==================================================================================================\n","Total params: 19,960,216\n","Trainable params: 19,960,216\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f29d9463ca0> and will run it as-is.\n","Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n","Cause: closure mismatch, requested ('self', 'step_function'), but source function had ('self',)\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"]},{"output_type":"stream","name":"stdout","text":["WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f29d9463ca0> and will run it as-is.\n","Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n","Cause: closure mismatch, requested ('self', 'step_function'), but source function had ('self',)\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","1302/1302 [==============================] - ETA: 0s - loss: 3.8194 - accuracy: 0.4366"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f2a88dbb160> and will run it as-is.\n","Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n","Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"]},{"output_type":"stream","name":"stdout","text":["WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f2a88dbb160> and will run it as-is.\n","Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n","Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","1302/1302 [==============================] - 117s 83ms/step - loss: 3.8194 - accuracy: 0.4366 - val_loss: 2.8750 - val_accuracy: 0.5379\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f2a880fae20>"]},"metadata":{},"execution_count":48}]},{"cell_type":"code","source":["spa_vocab = spa_vectorization.get_vocabulary()\n","spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n","max_decoded_sentence_length = 20\n","\n","\n","def decode_sequence(input_sentence):\n","    tokenized_input_sentence = eng_vectorization([input_sentence])\n","    decoded_sentence = \"[start]\"\n","    for i in range(max_decoded_sentence_length):\n","        tokenized_target_sentence = spa_vectorization([decoded_sentence])[:, :-1]\n","        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n","\n","        sampled_token_index = np.argmax(predictions[0, i, :])\n","        sampled_token = spa_index_lookup[sampled_token_index]\n","        decoded_sentence += \" \" + sampled_token\n","\n","        if sampled_token == \"[end]\":\n","            break\n","    return decoded_sentence\n","\n","\n","test_eng_texts = [pair[0] for pair in test_pairs]\n","for _ in range(30):\n","    input_sentence = random.choice(test_eng_texts)\n","    translated = decode_sequence(input_sentence)"],"metadata":{"id":"ryiPFdLWR2Kc","executionInfo":{"status":"ok","timestamp":1679807545175,"user_tz":-540,"elapsed":12352,"user":{"displayName":"박지수","userId":"15151265950850399854"}}},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":["## 코드 실습 2"],"metadata":{"id":"YN2UcI0aUuRz"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers"],"metadata":{"id":"IKYBUFyCR3q1","executionInfo":{"status":"ok","timestamp":1679807545557,"user_tz":-540,"elapsed":8,"user":{"displayName":"박지수","userId":"15151265950850399854"}}},"execution_count":50,"outputs":[]},{"cell_type":"code","source":["class TransformerBlock(layers.Layer):\n","    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n","        super(TransformerBlock, self).__init__()\n","        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n","        self.ffn = keras.Sequential(\n","            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n","        )\n","        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n","        self.dropout1 = layers.Dropout(rate)\n","        self.dropout2 = layers.Dropout(rate)\n","\n","    def call(self, inputs, training):\n","        attn_output = self.att(inputs, inputs)\n","        attn_output = self.dropout1(attn_output, training=training)\n","        out1 = self.layernorm1(inputs + attn_output)\n","        ffn_output = self.ffn(out1)\n","        ffn_output = self.dropout2(ffn_output, training=training)\n","        return self.layernorm2(out1 + ffn_output)"],"metadata":{"id":"RqzxHHkWUv3p","executionInfo":{"status":"ok","timestamp":1679807545557,"user_tz":-540,"elapsed":8,"user":{"displayName":"박지수","userId":"15151265950850399854"}}},"execution_count":51,"outputs":[]},{"cell_type":"code","source":["class TokenAndPositionEmbedding(layers.Layer):\n","    def __init__(self, maxlen, vocab_size, embed_dim):\n","        super(TokenAndPositionEmbedding, self).__init__()\n","        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n","        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n","\n","    def call(self, x):\n","        maxlen = tf.shape(x)[-1]\n","        positions = tf.range(start=0, limit=maxlen, delta=1)\n","        positions = self.pos_emb(positions)\n","        x = self.token_emb(x)\n","        return x + positions"],"metadata":{"id":"SfEZpEHaUwZa","executionInfo":{"status":"ok","timestamp":1679807545557,"user_tz":-540,"elapsed":8,"user":{"displayName":"박지수","userId":"15151265950850399854"}}},"execution_count":52,"outputs":[]},{"cell_type":"code","source":["vocab_size = 20000  # Only consider the top 20k words\n","maxlen = 200  # Only consider the first 200 words of each movie review\n","(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(num_words=vocab_size)\n","print(len(x_train), \"Training sequences\")\n","print(len(x_val), \"Validation sequences\")\n","x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n","x_val = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i0IGLyZaUxd-","executionInfo":{"status":"ok","timestamp":1679807549573,"user_tz":-540,"elapsed":4023,"user":{"displayName":"박지수","userId":"15151265950850399854"}},"outputId":"b249271a-05cf-406e-c5b4-a9f5d0323f99"},"execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n","17464789/17464789 [==============================] - 0s 0us/step\n","25000 Training sequences\n","25000 Validation sequences\n"]}]},{"cell_type":"code","source":["embed_dim = 32  # Embedding size for each token\n","num_heads = 2  # Number of attention heads\n","ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n","\n","inputs = layers.Input(shape=(maxlen,))\n","embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n","x = embedding_layer(inputs)\n","transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n","x = transformer_block(x)\n","x = layers.GlobalAveragePooling1D()(x)\n","x = layers.Dropout(0.1)(x)\n","x = layers.Dense(20, activation=\"relu\")(x)\n","x = layers.Dropout(0.1)(x)\n","outputs = layers.Dense(2, activation=\"softmax\")(x)\n","\n","model = keras.Model(inputs=inputs, outputs=outputs)"],"metadata":{"id":"-mM1ajFFUyEi","executionInfo":{"status":"ok","timestamp":1679807550087,"user_tz":-540,"elapsed":522,"user":{"displayName":"박지수","userId":"15151265950850399854"}}},"execution_count":54,"outputs":[]},{"cell_type":"code","source":["model.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n","history = model.fit(\n","    x_train, y_train, batch_size=32, epochs=2, validation_data=(x_val, y_val)\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UyqVrhsFUy1U","executionInfo":{"status":"ok","timestamp":1679807694708,"user_tz":-540,"elapsed":144624,"user":{"displayName":"박지수","userId":"15151265950850399854"}},"outputId":"d78e5c38-f886-4a34-d6e9-528451a0511b"},"execution_count":55,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/2\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f29e613d5e0> and will run it as-is.\n","Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n","Cause: closure mismatch, requested ('self', 'step_function'), but source function had ('self',)\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"]},{"output_type":"stream","name":"stdout","text":["WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f29e613d5e0> and will run it as-is.\n","Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n","Cause: closure mismatch, requested ('self', 'step_function'), but source function had ('self',)\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","782/782 [==============================] - ETA: 0s - loss: 0.3895 - accuracy: 0.8113"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f29e6a3d1f0> and will run it as-is.\n","Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n","Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"]},{"output_type":"stream","name":"stdout","text":["WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f29e6a3d1f0> and will run it as-is.\n","Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n","Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","782/782 [==============================] - 70s 82ms/step - loss: 0.3895 - accuracy: 0.8113 - val_loss: 0.3246 - val_accuracy: 0.8668\n","Epoch 2/2\n","782/782 [==============================] - 21s 26ms/step - loss: 0.1987 - accuracy: 0.9241 - val_loss: 0.3072 - val_accuracy: 0.8699\n"]}]},{"cell_type":"markdown","source":["## Review\n","\n","- Attention의 장점에 대해서 생각하고 설명해봅니다.\n","\n","    - RNN 모델의 단점 2가지\n","    - 장기 의존성(Long-term dependency)\n","    - Attention의 장점\n"],"metadata":{"id":"mGRIb82qUz7s"}},{"cell_type":"markdown","source":["- GPT & BERT\n","    - 사전 학습 언어 모델(Pretrained Language Model), 전이 학습(Transfer Learning)\n","        - 사전 학습(Pre-training)\n","        - Fine-tuning\n","    - GPT의 구조\n","    - BERT의 구조\n","        - MLM(Masked Langauge Model)\n","        - NSP(Next Sentence Prediction)\n","\n","        "],"metadata":{"id":"EUk8kwoMU33L"}},{"cell_type":"markdown","source":["## References\n","\n","- 트랜스포머에 대해 조금 더 자세하게 알고 싶다면\n","    - [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)\n","    - [번역](https://nlpinkorean.github.io/illustrated-transformer/)\n","    - [Paper](https://arxiv.org/pdf/1706.03762.pdf) (Attention is All You Need)\n","\n","- GPT에 대해 더 자세하게 알고 싶다면\n","    - [The Illustrated GPT-2](http://jalammar.github.io/illustrated-gpt2/) (Visualizing Transformer Language Models)\n","    - [Paper](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf) (Improving Language Understanding by Generative Pre-Training)\n","\n","- BERT에 대해 더 자세하게 알고 싶다면\n","    - [The Illustrated BERT, ELMo, and co.](http://jalammar.github.io/illustrated-bert/) (How NLP Cracked Transfer Learning)\n","    - [번역](https://nlpinkorean.github.io/illustrated-bert/)\n","    - [Paper](https://arxiv.org/pdf/1810.04805.pdf) (Pre-training of Deep Bidirectional Transformers for\n","Language Understanding)"],"metadata":{"id":"bVe_c_lMU47S"}},{"cell_type":"code","source":[],"metadata":{"id":"0-QgVbgKUzXm"},"execution_count":null,"outputs":[]}]}